<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>[Distributed Training] Horovod_and_Openmpi | Jian&#39;s Blog</title>
<meta name="keywords" content="distributed training">
<meta name="description" content="Horovod ä»‹ç» Horovod æ˜¯ Uber å¼€æºçš„æ·±åº¦å­¦ä¹ å·¥å…·ï¼Œå®ƒçš„å‘å±•å¸å–äº†Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; ä¸ç™¾åº¦ &ldquo;Ring Allreduce&rdquo; çš„ä¼˜ç‚¹ï¼Œåœ¨ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„åŒæ—¶ï¼Œå…¼é¡¾äº†å‰ç«¯çš„ç®€æ´å’Œå¯¹ä¸">
<meta name="author" content="Jian">
<link rel="canonical" href="https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.1ea9c8832138446635789668415e5c75b8a534b191ee749a44f5ab404c9f27c2.css" integrity="sha256-HqnIgyE4RGY1eJZoQV5cdbilNLGR7nSaRPWrQEyfJ8I=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="apple-touch-icon" href="https://jianye0428.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://jianye0428.github.io/favicon/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="baidu-site-verification" content="code-9oLyeix0aK" />
<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4a41bf85d719f0e8c3165fc76904f546";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>



<script defer crossorigin="anonymous" src="/js/katex.min.8f5024e83d2055dd60e021751066111b0057e230db34911dd56242d67f0a4c86.js" integrity="sha256-j1Ak6D0gVd1g4CF1EGYRGwBX4jDbNJEd1WJC1n8KTIY="></script>


<script defer crossorigin="anonymous" src="/js/auto-render.min.b09accad850e4e87b8a2fc8b93fae790def79172b68de72fd777958c52e566ad.js" integrity="sha256-sJrMrYUOToe4ovyLk/rnkN73kXK2jecv13eVjFLlZq0="></script>

<script>
    
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });

    
    window.WebFontConfig = {
        custom: {
            families: ['KaTeX_AMS', 'KaTeX_Caligraphic:n4,n7', 'KaTeX_Fraktur:n4,n7',
            'KaTeX_Main:n4,n7,i4,i7', 'KaTeX_Math:i4,i7', 'KaTeX_Script',
            'KaTeX_SansSerif:n4,n7,i4', 'KaTeX_Size1', 'KaTeX_Size2', 'KaTeX_Size3',
            'KaTeX_Size4', 'KaTeX_Typewriter'],
        },
    };
</script>


<script defer crossorigin="anonymous" src="/js/webfontloader.min.min.d1c6c39d18e2decb5c99dc9efc579098ab37b9654725df3f9c0737bc2dd00760.js" integrity="sha256-0cbDnRji3stcmdye/FeQmKs3uWVHJd8/nAc3vC3QB2A="></script>


 

<script async src="https://www.googletagmanager.com/gtag/js?id=G-C6GDZ56F4S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-C6GDZ56F4S', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="[Distributed Training] Horovod_and_Openmpi" />
<meta property="og:description" content="Horovod ä»‹ç» Horovod æ˜¯ Uber å¼€æºçš„æ·±åº¦å­¦ä¹ å·¥å…·ï¼Œå®ƒçš„å‘å±•å¸å–äº†Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; ä¸ç™¾åº¦ &ldquo;Ring Allreduce&rdquo; çš„ä¼˜ç‚¹ï¼Œåœ¨ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„åŒæ—¶ï¼Œå…¼é¡¾äº†å‰ç«¯çš„ç®€æ´å’Œå¯¹ä¸" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/" /><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-27T17:31:57&#43;08:00" />
<meta property="article:modified_time" content="2022-07-27T17:31:57&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/>

<meta name="twitter:title" content="[Distributed Training] Horovod_and_Openmpi"/>
<meta name="twitter:description" content="Horovod ä»‹ç» Horovod æ˜¯ Uber å¼€æºçš„æ·±åº¦å­¦ä¹ å·¥å…·ï¼Œå®ƒçš„å‘å±•å¸å–äº†Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; ä¸ç™¾åº¦ &ldquo;Ring Allreduce&rdquo; çš„ä¼˜ç‚¹ï¼Œåœ¨ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„åŒæ—¶ï¼Œå…¼é¡¾äº†å‰ç«¯çš„ç®€æ´å’Œå¯¹ä¸"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jianye0428.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "[Distributed Training] Horovod_and_Openmpi",
      "item": "https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[Distributed Training] Horovod_and_Openmpi",
  "name": "[Distributed Training] Horovod_and_Openmpi",
  "description": "Horovod ä»‹ç» Horovod æ˜¯ Uber å¼€æºçš„æ·±åº¦å­¦ä¹ å·¥å…·ï¼Œå®ƒçš„å‘å±•å¸å–äº†Facebook \u0026ldquo;Training ImageNet In 1 Hour\u0026rdquo; ä¸ç™¾åº¦ \u0026ldquo;Ring Allreduce\u0026rdquo; çš„ä¼˜ç‚¹ï¼Œåœ¨ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„åŒæ—¶ï¼Œå…¼é¡¾äº†å‰ç«¯çš„ç®€æ´å’Œå¯¹ä¸",
  "keywords": [
    "distributed training"
  ],
  "articleBody": "Horovod ä»‹ç» Horovod æ˜¯ Uber å¼€æºçš„æ·±åº¦å­¦ä¹ å·¥å…·ï¼Œå®ƒçš„å‘å±•å¸å–äº†Facebook â€œTraining ImageNet In 1 Hourâ€ ä¸ç™¾åº¦ â€œRing Allreduceâ€ çš„ä¼˜ç‚¹ï¼Œåœ¨ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„åŒæ—¶ï¼Œå…¼é¡¾äº†å‰ç«¯çš„ç®€æ´å’Œå¯¹ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æ¶çš„æ”¯æŒï¼Œä½¿ç”¨èµ·æ¥å¯¹å¼€å‘äººå‘˜æ¯”è¾ƒçš„å‹å¥½ï¼Œç®—æ˜¯åˆ†å¸ƒå¼è®­ç»ƒæ–¹å‘çš„æ ‡æ†é¡¹ç›®äº†ã€‚\né›†åˆé€šä¿¡åº“ é›†åˆé€šä¿¡åº“ï¼Œè¿™ä¸ªè¯å¯èƒ½å¬èµ·æ¥ä¼šæ¯”è¾ƒçš„é™Œç”Ÿï¼Œä¸è¿‡å¦‚æœæˆ‘å†æå‡ ä¸ªå…³é”®å­—ï¼Œå¯èƒ½å¤§å®¶å¤šå°‘éƒ½ä¼šæœ‰æ‰€è€³é—»ã€‚èµ„å†æ¯”è¾ƒè€çš„æ˜¯ MPI (Message Passing Interface åŠå…¶å®ç° OpenMPI å’Œ MPICHï¼Œå¹´è½»ä¸€ç‚¹çš„ä¼šæ˜¯ Nvidia é’ˆå¯¹å…¶æ˜¾å¡å¼€æºçš„ NCCLï¼Œæˆ–è€…æ˜¯ facebook å¼€æºçš„ glooï¼Œæˆ–è€…æ˜¯åƒåä¸ºé’ˆå¯¹å…¶é«˜æ€§èƒ½ç¡¬ä»¶æä¾›çš„HCCLï¼Œå¤§ä½“ä¸Šéƒ½å¯ä»¥å½’å…¥åˆ°é›†åˆé€šä¿¡åº“çš„ç±»åˆ«ã€‚ä»–ä»¬ç›¸åŒçš„åœ°æ–¹æ˜¯å¤§ä½“ä¸Šä¼šéµç…§ MPI æä¾›çš„æ¥å£è§„å®šï¼Œå®ç°äº†åŒ…æ‹¬ç‚¹å¯¹ç‚¹é€šä¿¡ï¼ˆSEND,RECVç­‰ï¼‰ï¼Œé›†åˆé€šä¿¡ï¼ˆ REDUCEï¼ŒBROADCASTï¼ŒALLREDUCEç­‰ï¼‰ç­‰ç›¸å…³æ¥å£ï¼Œç„¶åæ ¹æ®è‡ªå·±ç¡¬ä»¶æˆ–è€…æ˜¯ç³»ç»Ÿçš„éœ€è¦ï¼Œåœ¨åº•å±‚å®ç°ä¸Šè¿›è¡Œäº†ç›¸åº”çš„æ”¹åŠ¨ï¼Œä¿è¯æ¥å£çš„ç¨³å®šå’Œæ€§èƒ½ã€‚\nç‚¹å¯¹ç‚¹é€šä¿¡: Point-to-Point Communication Send/Recv:\né›†åˆé€šä¿¡ Scatter/Gather\nreduce/allreduce\nboradcast/all-gather\nè¿™é‡Œåœ¨æœºå™¨å­¦ä¹ è®­ç»ƒä¸­ä½¿ç”¨æ¯”è¾ƒå¤šçš„æ˜¯ all-reduceï¼Œåœºæ™¯ç±»ä¼¼åœ¨ä¸åŒçš„ node ä¸Šè·‘ä¸åŒ batch çš„æ•°æ®ï¼Œç„¶åæ›´æ–°æ¢¯åº¦éœ€è¦ä»å„ä¸ªæ±‡æ€»ä¹‹åå¹³å‡å†å›ä¼ åˆ°å„è‡ªçš„ node ä¸­ã€‚è€Œè¿™éƒ¨åˆ†ï¼Œæœ‰å¾ˆå¤šç§å®ç°çš„æ–¹å¼ï¼Œæ¯”è¾ƒç›´è§‚å’Œç®€å•çš„æ˜¯æŠŠæ‰€æœ‰çš„æ¢¯åº¦éƒ½æ±‡æ€»åˆ°çš„æŸä¸€ä¸ª node ä¸Šï¼ˆå¦‚ä¸‹å›¾ node d æ‰€ç¤ºï¼‰ï¼Œç„¶åå†æŠŠæ±‡æ€»çš„ä¿¡æ¯é‡æ–°åˆ†å‘åˆ°ä¸åŒçš„ node ä¸Š ï¼Œè¿™æ ·å¯ä»¥è®¡ç®—é€šä¿¡é‡ï¼Œå¦‚ä¸‹ï¼šå¯¹äº P ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ¶ˆæ¯å¤§å°ä¸º Mï¼Œnode d èŠ‚ç‚¹çš„é€šä¿¡é‡ä¸º 2*(P-1)Mï¼Œè¿™é‡Œå‡è®¾èŠ‚ç‚¹ä¹‹é—´äº’è”äº’é€šï¼Œå¸¦å®½ä¸ºBã€‚\nä¸è¿‡è¿™ç§æƒ…å†µä¸‹ï¼Œå¾ˆå®¹æ˜“å¯¼è‡´ node d ä¼šæˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œå› ä¸º node d éœ€è¦è·Ÿå…¶ä»–æ‰€æœ‰ node é€šä¿¡æ‰€ä»¥å®ƒçš„é€šä¿¡é‡æ˜¯å…¶ä»–èŠ‚ç‚¹çš„ P å€ã€‚å‡è®¾èŠ‚ç‚¹é—´çš„å¸¦å®½è¿˜æ˜¯ä¸€æ ·ï¼Œnode d å®Œæˆæ‰€æœ‰é€šä¿¡æ‰€éœ€è¦çš„æ—¶é—´æ˜¯ 2(P-1)M/B*ã€‚æ‰€ä»¥ç°åœ¨å¾ˆå¤šçš„é›†åˆé€šä¿¡æ¡†æ¶ä¸ä¼šä½¿ç”¨è¿™ç§æ–¹å¼ï¼Œæ›´å¤šçš„æ˜¯é€šè¿‡æ ‘çŠ¶æˆ–è€…æ˜¯ç¯çŠ¶(ring) å»å®ç° all-reduceã€‚\nå¦‚æœåªæ˜¯åšæˆæ ‘çŠ¶çš„å¯ä»¥åšæˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè™½ç„¶ä¼ é€’çš„æ­¥æ•°å¢å¤šäº†ï¼Œä¸è¿‡æ¶ˆé™¤äº†node d çš„é€šä¿¡ç“¶é¢ˆï¼Œå®Œæˆæ‰€æœ‰çš„é€šä¿¡çš„æ—¶é—´å¤§æ¦‚æ˜¯ 2log_2N(M/B)*ï¼Œéšç€èŠ‚ç‚¹æ•°ç›® P çš„å¢åŠ ï¼Œæ ‘å½¢ç»“æ„çš„æ•ˆæœä¼šè¶Šæ¥è¶Šæ˜æ˜¾ã€‚\nä¸šç•Œç”¨å¾—æœ€å¤šä¸€ç§ä¼˜åŒ–çš„æ–¹å¼æ˜¯ï¼Œæ¯æ¬¡åªä¼ ä¸€éƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†æ˜¯ç™¾åº¦æå‡ºçš„ ring-allreduce çš„æ–¹æ¡ˆï¼Œå…·ä½“çš„ä»‹ç»å¯ä»¥å‚è€ƒè¿™ç¯‡åšå®¢Bringing HPC Techniques to Deep Learningï¼Œè¿™è¾¹å°±ä¸èµ˜è¿°äº†ã€‚æ•´ä½“ä¸Šå°±æ˜¯æ¯æ¬¡ä¸ä¼šåƒä¸Šé¢è¿™æ ·æ•´ä»½æ•°æ®ä¼ é€’ï¼Œè€Œæ˜¯ä¸€éƒ¨åˆ†ä¸€éƒ¨åˆ†ä¼ ï¼Œä¼˜åŒ–åï¼Œæ‰€æœ‰èŠ‚ç‚¹éœ€è¦ä¼ è¾“çš„æ•°æ®é‡çš„ä¼ è¾“ 2(Nâˆ’1)M/N æ¯”è¾ƒå¹³å‡ï¼Œæ‰€éœ€è¦çš„æ—¶é—´å¯ä»¥å¤§æ¦‚æ˜¯ 2(Nâˆ’1)M/(NB)ï¼Œhorovod ä¹Ÿæ˜¯åŸºäºè¿™ç§ all-reduce çš„å½¢å¼å®ç°çš„ã€‚\nå®è·µ: pytorch.distributed å°è¯•ä½¿ç”¨ pytorch è‡ªå¸¦çš„åˆ†å¸ƒå¼å·¥å…·åŒ… torch.distributedï¼Œè¿›è¡Œä¸€äº›æ¦‚å¿µæ€§çš„å°è¯•ã€‚\nä¸ºäº†æ–¹ä¾¿å°è¯•ï¼Œæˆ‘è¿™é‡Œæä¾›äº†ä¸€ä¸ªç®€å•çš„ demoï¼Œå¤§å®¶å¦‚æœå®‰è£…äº† gpu ç‰ˆæœ¬çš„ pytorch \u003e= 1.3ï¼Œåº”è¯¥éƒ½å¯ä»¥å°è¯•ä¸‹é¢çš„ä¾‹å­å°è¯•ä½¿ç”¨å¤šè¿›ç¨‹æ¨¡æ‹Ÿåˆ†å¸ƒå¼ï¼ˆå•æœºä¸Šå¯ä»¥è·‘ï¼‰ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import os import torch import torch.distributed as dist import time import argparse from torch.multiprocessing import Process parser = argparse.ArgumentParser(description='PyTorch MNIST Example') parser.add_argument('-m', '--mode', type=str, default='one_device', metavar='N', help='distribute mode, distributed/one_device') parser.add_argument('-f', '--function', type=str, default='p2p', metavar='N', help='function to run (p2p/all_reduce/gpu_all_reduce)') parser.add_argument('-b', '--backend', type=str, default=\"nccl\", metavar='N', help='distribute backend (gloo/nccl)') def init_process(rank, size, fn, backend='nccl'): \"\"\" Initialize the distributed environment. \"\"\" os.environ['MASTER_ADDR'] = '127.0.0.1' os.environ['MASTER_PORT'] = '29500' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) def run(rank, size): tensor = torch.zeros(1) print('Rank ', rank, ' has data before send/recv', tensor) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print('Rank ', rank, ' has data after send/recv', tensor) def run_allreduce(rank, size): \"\"\" Simple reduce communication. \"\"\" group = dist.new_group([0, 1]) device = torch.device('cuda:%d' % rank) tensor = torch.ones(1).to(device) dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) def run_multigpu_allreduce(rank, size): group = dist.new_group([0, 1]) tensor_list = [] for dev_idx in range(2): device = torch.device('cuda:%d' % (2 * rank + dev_idx)) tensor = torch.ones(1).to(device) tensor_list.append(tensor) dist.all_reduce_multigpu(tensor_list) print('all_reduce_multigpu', tensor_list) dist.all_reduce(tensor_list[0], op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data tensor[0]:', tensor_list[0], \", tensor[1]:\", tensor_list[1]) if __name__ == \"__main__\": args = parser.parse_args() backend = args.backend if args.mode == \"distributed\" or os.environ.get('RANK',None): print(\"in distribute mode\") if args.function == \"all_reduce\": function, size = run_allreduce, 2 elif args.function == \"gpu_all_reduce\": function, size = run_multigpu_allreduce, 2 else: function, size, backend = run, 2, \"gloo\" rank = int(os.environ['RANK']) p = Process(target=init_process, args=(rank, size, function, backend)) p.start() p.join() else: print(\"in one device mode\") if args.function == \"all_reduce\": function, size = run_allreduce, 2 elif args.function == \"gpu_all_reduce\": function, size = run_multigpu_allreduce, 2 else: function, size, backend = run, 2, \"gloo\" processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, function, backend)) p.start() processes.append(p) for p in processes: p.join() å¯ä»¥ç®€å•åœ°è¿è¡Œä¸Šé¢çš„ä¾‹å­ï¼š\nsend/recv:\n1 2 3 4 5 6 7 8 $ python3 distribute_test.py # è¾“å‡ºå¦‚ä¸‹ï¼š in one device mode Rank 0 has data before send/recv tensor([0.]) Rank 1 has data before send/recv tensor([0.]) Rank 0 has data after send/recv tensor([1.]) Rank 1 has data after send/recv tensor([1.]) ä¸Šé¢æ˜¯æ¼”ç¤ºçš„æ˜¯é€šè¿‡ pytorch çš„ multiprocessing åŒ…ï¼Œæ¨¡æ‹Ÿä¸€æ¬¡åˆ†å¸ƒå¼çš„ send/recv è¿‡ç¨‹ï¼Œè¿™é‡Œæ˜¯ rank0 çš„è¿›ç¨‹å¾€ rank1 çš„è¿›ç¨‹å‘é€ä¸€ä¸ª tensorï¼Œå¯ä»¥çœ‹åˆ° rank 1 tensor åˆå§‹åŒ–ä¸º 0ï¼Œæ˜¯æ¥æ”¶åˆ° rank 0 çš„tensor åå˜ä¸º 1 çš„ã€‚ï¼ˆæ³¨æ„ï¼šè¿™é‡Œç‰¹åˆ«è®¾ç½®äº† backend ä¸º gloo æ˜¯å› ä¸º nccl ä¸æ”¯æŒ point2point çš„ä¼ è¾“ï¼Œå…·ä½“ä¸åŒ backend æ”¯æŒä»€ä¹ˆå½¢å¼çš„åŸè¯­ï¼Œå‚è€ƒæ–‡æ¡£backendéƒ¨åˆ† ï¼‰\nall_reduce\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ python3 distribute_test.py -f all_reduce # è¾“å‡ºå¦‚ä¸‹ï¼š in one device mode Rank 0 has data tensor(2., device='cuda:0') Rank 1 has data tensor(2., device='cuda:1') # å¯¹åº”å‡½æ•° def run_allreduce(rank, size): \"\"\" Simple reduce communication. \"\"\" group = dist.new_group([0, 1]) # use rank 0 and rank 1 device = torch.device('cuda:%d' % rank) tensor = torch.ones(1).to(device) dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) è¿™é‡Œä¹Ÿå¾ˆæµ…ç™½ï¼Œä¸»è¦å°±æ˜¯å¯¹ä¸¤ä¸ªè¿›ç¨‹ä¸Šçš„ tensor è¿›è¡Œä¸€æ¬¡ allreduceï¼Œå¯ä»¥çœ‹åˆ°ä¸¤ä¸ª rank ä¸Šçš„ç»“æœéƒ½ä¸º 2äº†ã€‚\ngpu_all_reduce\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ python3 distribute_test.py -f gpu_all_reduce # è¾“å‡ºå¦‚ä¸‹ï¼š #in one device mode # [tensor([1.], device='cuda:0')] # [tensor([1.], device='cuda:2')] # [tensor([1.], device='cuda:2'), tensor([1.], device='cuda:3')] # [tensor([1.], device='cuda:0'), tensor([1.], device='cuda:1')] #all_reduce_multigpu [tensor([4.], device='cuda:2'), tensor([4.], device='cuda:3')] #all_reduce_multigpu [tensor([4.], device='cuda:0'), tensor([4.], device='cuda:1')] #Rank 0 has data tensor[0]: tensor([8.], device='cuda:0') , tensor[1]: tensor([4.], device='cuda:1') #Rank 1 has data tensor[0]: tensor([8.], device='cuda:2') , tensor[1]: tensor([4.], device='cuda:3') # å¯¹åº”å‡½æ•° def run_multigpu_allreduce(rank, size): group = dist.new_group([0, 1]) tensor_list = [] for dev_idx in range(2): device = torch.device('cuda:%d' % (2 * rank + dev_idx)) tensor = torch.ones(1).to(device) tensor_list.append(tensor) print(tensor_list) dist.all_reduce_multigpu(tensor_list) print('all_reduce_multigpu', tensor_list) dist.all_reduce(tensor_list[0], op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data tensor[0]:', tensor_list[0], \", tensor[1]:\", tensor_list[1]) all_reduce_multigpu: ç›¸å½“äºå°†å¤šä¸ªgpuå†…çš„å¤šè¿›ç¨‹çš„å€¼è¿›è¡Œç›¸åŠ ; all_reduce: ç›¸å½“äºå•ä¸ªgpuå†…çš„å¤šè¿›ç¨‹çš„å€¼ç›¸åŠ \nè¿™é‡Œæ¼”ç¤ºçš„æ˜¯å°è¯•å¯¹ä¸åŒè¿›ç¨‹ä¸‹å¤šä¸ª gpu (è¿™é‡Œæ˜¯ 4 ä¸ª) è¿›è¡Œ reduceï¼Œå…·ä½“é€»è¾‘å°±æ˜¯ï¼š\n- å¯¹ä¸åŒçš„è¿›ç¨‹åˆ†åˆ«æŠŠ tensor åˆå§‹åŒ–åœ¨ä¸åŒçš„ gpu ä¸Šï¼Œrank0 åˆå§‹åŒ–åœ¨ 0ï¼Œ1 gpu ä¸Šï¼Œrank 1 åœ¨ 2ï¼Œ3ä¸Šã€‚ - è¿›è¡Œä¸€æ¬¡ all_reduce_multigpu ï¼ˆè¿™ä¸ªå‡½æ•°è·Ÿ all_reduce ä¸åŒï¼Œæ˜¯æŠŠä¸åŒçš„ node ä¸Šä¸åŒçš„gpu ä¸Šçš„tensor éƒ½æ”¾åˆ°ä¸€ä¸ª list ä¸­ï¼Œè¿›è¡Œreduceï¼‰ï¼Œè¿™æ—¶æ‰€æœ‰ gpu ä¸Šçš„å€¼éƒ½æ˜¯4ï¼Œä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬å¯¹ tensor_list[0] çš„tensor åšä¸€æ¬¡all_reduceï¼Œå¾—åˆ°çš„ç»“æœåœ¨ gpu 0,2 ä¸Šçš„ tensor è¿›è¡Œäº†all_reduce ç»“æœæ˜¯ 8ï¼Œåœ¨ gpu 1,3 çš„ tensor æ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚ å¤šterminalå°è¯•\nåœ¨éªŒè¯åˆ†å¸ƒå¼é€»è¾‘çš„æ—¶å€™ï¼Œå…¶å®æˆ‘ä»¬ä¸ä¸€å®šéœ€è¦å¤šå°æœºå­æ‰å¯ä»¥ï¼Œå¯¹ä¸€äº›ä¸æ¶‰åŠç½‘ç»œæ€§èƒ½çš„éªŒè¯ï¼Œå¯ä»¥å°è¯•åœ¨ä¸€å°æœºå­ä¸Šå¼€å¤šä¸ª terminal è¿›è¡ŒéªŒè¯ã€‚å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„ä¾‹å­ï¼Œåœ¨å¤šä¸ª terminal ä¸‹è·‘ä»¥ä¸‹å‘½ä»¤ã€‚\nterminal0:\n1 2 3 4 5 RANK=0 python3 distribute_test.py -f gpu_all_reduce # è¾“å‡ºå¦‚ä¸‹ in distribute mode all_reduce_multigpu [tensor([4.], device='cuda:0'), tensor([4.], device='cuda:1')] Rank 0 has data tensor[0]: tensor([8.], device='cuda:0') , tensor[1]: tensor([4.], device='cuda:1') terminal1:\n1 2 3 4 5 RANK=1 python3 distribute_test.py -f gpu_all_reduce # è¾“å‡ºå¦‚ä¸‹ in distribute mode all_reduce_multigpu [tensor([4.], device='cuda:2'), tensor([4.], device='cuda:3')] Rank 1 has data tensor[0]: tensor([8.], device='cuda:2') , tensor[1]: tensor([4.], device='cuda:3') è¿™é‡Œæ˜¯é€šè¿‡æœ¬åœ°æœºå­ä¸Šçš„å›é€åœ°å€è¿›è¡Œæ¨¡æ‹Ÿï¼Œç»“æœæ˜¯åˆ†åˆ«åœ¨ä¸åŒçš„ terminal å‘ˆç°ï¼Œå½“ç„¶å¯ä»¥ç”¨ä¸Šé¢çš„demoï¼Œåœ¨å¤šå°æœºå­ä¸Šè·‘ï¼Œä¸è¿‡éœ€è¦ä¿®æ”¹ä¸€ä¸‹ init_process å‡½æ•°ä¸­çš„ os.environ[â€˜MASTER_ADDRâ€™] = â€˜127.0.0.1â€™ ä¸º rank 0 æœºå­çš„ IPï¼Œè¿™é‡Œå°±ä¸æ¼”ç¤ºäº†ã€‚å…·ä½“ pytorch distributed å·¥å…·ç›¸å…³çš„å†…å®¹å¯ä»¥å‚è€ƒå®˜æ–¹åšå®¢\nç»ƒä¹ ï¼š å¦‚æœå¤§æ¦‚ç†è§£äº†ä¸Šé¢çš„ä¸€äº›é›†åˆé€šä¿¡çš„åŸè¯­ï¼Œå¯ä»¥å°è¯•ç€ç”¨ä¸Šé¢ pytorch æä¾›çš„ send/recv å°è¯•å»å®ç°ä¸€ä¸‹ä¸Šé¢çš„æ ‘çŠ¶ allreduceã€‚\nMPI æ›´æ·±å…¥çš„å°è¯•ï¼Œå¯ä»¥å°è¯•äº†è§£ä¸€ä¸‹ mpi çš„çŸ¥è¯†ï¼Œè¿™ä¸ªmpiæ•™ç¨‹ ç®—æ˜¯å†™å¾—æ¯”è¾ƒç³»ç»Ÿçš„ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒä¸€ä¸‹æ¥ç»ƒä¹ ï¼Œç‰¹åˆ«æ˜¯å¯¹åº•å±‚ä¸æ˜¯å¾ˆäº†è§£çš„åŒå­¦ï¼Œå¯ä»¥å¤šçœ‹çœ‹ Running an MPI cluster within a LAN çš„éƒ¨åˆ†ï¼Œå®æ“ä¸€ä¸‹é€šè¿‡ ssh è·‘èµ·ä¸€ä¸ªåˆ†å¸ƒå¼çš„ demoã€‚é›†åˆé€šä¿¡åº“çš„åŸºç¡€å¤§æ¦‚å…ˆåˆ°è¿™é‡Œï¼Œå¦‚æœè¦æ·±å…¥çš„å¯ä»¥å†å»çœ‹çœ‹ openMPIï¼Œå’Œ nccl çš„å®ç°ã€‚\nHorovodæµç¨‹åˆ†æ ä¸‹é¢æˆ‘ä¼šä»¥ä¸€ä¸ªç®€å•çš„ pytorch horovod çš„ demo å°è¯•å»ç†è§£ä¸€ä¸‹ horovod çš„å·¥ä½œæœºç†ï¼Œdemo å¦‚ä¸‹ï¼ˆçœç•¥äº†ä¸€äº›ä¸å…³é”®çš„ä»£ç æ®µï¼‰ã€‚ä¸ºäº†å‡†ç¡®èµ·è§ï¼Œæˆ‘ä»¬æ˜¯æ ¹æ® horovod v0.20.3 çš„ç‰ˆæœ¬è¿›è¡Œé˜…è¯»çš„ï¼Œå¦‚æœæ˜¯å…¶ä»–ç‰ˆæœ¬ï¼Œå¯èƒ½ä¼šè·Ÿè¿™é‡Œçš„å†…å®¹æœ‰ä¸€äº›å‡ºå…¥ã€‚\npytorch demo ä¸€èˆ¬çš„ horovod è®­ç»ƒç¨‹åºéƒ½ä¼šåŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š\n1. hvd.init: å¯¹ horovod 2. åˆå§‹åŒ–ã€‚åˆå§‹åŒ–æ¨¡å‹ï¼Œæ•°æ®é›†ï¼Œä¼˜åŒ–å™¨ï¼Œåˆå§‹åŒ–ä¸åŒ node çš„æ¨¡å‹æƒé‡ã€‚ 3. ä½¿ç”¨ hvd.DistributedOptimizer åŒ…è£…ä¼˜åŒ–å™¨ã€‚ 4. è¿›å…¥è®­ç»ƒæµç¨‹ï¼Œè¿›è¡Œä¼˜åŒ–è¿­ä»£ã€‚ æˆ‘ä»¬ä¼šç€é‡ä»‹ç»ç¬¬ 1 å’Œ 4 æ­¥ï¼Œå› ä¸ºä¸»è¦ä¹Ÿæ˜¯1ï¼Œ4æ­¥ä¼šè·Ÿ c++ åç«¯è¿›è¡Œä¿¡æ¯äº¤æ¢ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import torch.backends.cudnn as cudnn import torch.nn.functional as F import torch.optim as optim import torch.utils.data.distributed from torchvision import models import horovod.torch as hvd import timeit import numpy as np ... # some argparse hvd.init() # Set up standard model. model = getattr(models, args.model)() optimizer = optim.SGD(model.parameters(), lr=0.01 * lr_scaler) # Horovod: (optional) compression algorithm. compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none # Horovod: wrap optimizer with DistributedOptimizer. optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters(), compression=compression, op=hvd.Adasum if args.use_adasum else hvd.Average) # Horovod: broadcast parameters \u0026 optimizer state. hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) # Set up fixed fake data data = torch.randn(args.batch_size, 3, 224, 224) target = torch.LongTensor(args.batch_size).random_() % 1000 if args.cuda: data, target = data.cuda(), target.cuda() def benchmark_step(): optimizer.zero_grad() output = model(data) loss = F.cross_entropy(output, target) loss.backward() optimizer.step() #... some log configuration img_secs = [] for x in range(args.num_iters): time = timeit.timeit(benchmark_step, number=args.num_batches_per_iter) img_sec = args.batch_size * args.num_batches_per_iter / time img_secs.append(img_sec) # Results ... ç„¶åä¸‹å›¾æ˜¯æˆ‘å¯¹ horovod æ•´ä½“æµç¨‹çš„æ¢³ç†ï¼ŒæŠŠä¸€äº›ä¸æ˜¯å¾ˆå…³é”®çš„éƒ¨åˆ†éšè—äº†ï¼Œå¯èƒ½æœ‰ä¸€äº›ç»†èŠ‚çš„åœ°æ–¹å’Œå®ç°æœ‰å‡ºå…¥ï¼Œä¸è¿‡æˆ‘å¾…ä¼šä¼šæœ‰è¯¦ç»†çš„è¯´æ˜ã€‚è¿™é‡Œå…ˆè§£é‡Šä¸€ä¸‹ï¼Œä¸‹é¢å‡ ä¸ªå¤§çš„éƒ¨åˆ†:\nmain.pyï¼š è¡¨ç¤ºè®­ç»ƒè„šæœ¬ï¼Œä¸€èˆ¬æ˜¯ ä½¿ç”¨ horovod æä¾›çš„å‡½æ•°è·Ÿç‰¹å®šçš„è®­ç»ƒæ¡†æ¶ç›¸äº’åˆä½œå®Œæˆåˆ†å¸ƒå¼è®­ç»ƒï¼ˆä¸‹æ–‡ç§°å‰ç«¯ï¼‰ C++ interfaceï¼šæ˜¯æŒ‡ horovod python å‡½æ•°è°ƒç”¨ C++ çš„æ¥å£ GlobalStateï¼šåœ¨ horovod ä¸­æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå…¶ä¸­çš„å…ƒç´ å¯ä»¥ä¾›ä¸åŒçš„çº¿ç¨‹è®¿é—®ï¼Œåœ¨åŠ è½½ C++ çš„ä»£ç æ—¶å€™å°±å·²ç»åˆ›å»ºäº†ï¼ŒåŒæ—¶åˆ›å»ºçš„è¿˜æœ‰å„ç§ contextï¼ˆmpi_context, nccl_context, gpu_contextï¼‰åé¢ä¼šæåˆ°ï¼Œä¸»è¦ä¼šåœ¨ä¸‹å›¾ backgroundThreadLoop ä¸­å®Œæˆ globalstate ä¸åŒå…ƒç´ åˆå§‹åŒ–ï¼Œæ¯”è¾ƒé‡è¦çš„æœ‰ controller ç®¡ç†æ€»ä½“é€šä¿¡æ§åˆ¶æµï¼Œtensor_queue ä¼šå¤„ç†ä»å‰ç«¯è¿‡æ¥çš„é€šä¿¡éœ€æ±‚ï¼ˆallreduceï¼Œbroadcast ç­‰ï¼‰ã€‚ BackgroundThreadLoopï¼šæ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„åå°çº¿ç¨‹ï¼Œä¸»è¦è´Ÿè´£è·Ÿå…¶ä»–èŠ‚ç‚¹çš„é€šä¿¡ï¼Œå’Œå¤„ç†å‰ç«¯è¿‡æ¥çš„é€šä¿¡éœ€æ±‚ï¼ˆrequestï¼‰ï¼Œä¼šè½®è¯¢è°ƒç”¨ RunLoopOnceï¼Œä¸æ–­æŸ¥çœ‹ tensor_queue ä¸­æœ‰æ²¡æœ‰éœ€è¦é€šä¿¡çš„tensorï¼Œå¦‚æœæœ‰è·Ÿå…¶ä»–èŠ‚ç‚¹åŒæ­¥æ›´æ–°ï¼Œç„¶åæ‰§è¡Œé€šä¿¡æ“ä½œã€‚ æµç¨‹åˆ†æ ä¸‹é¢ä½¿ç”¨ mpi_controller è¿›è¡Œ allreduce æ“ä½œè¿›è¡Œåˆ†æã€‚\n1.hvd.init()-\u003eInitializeHorovodOnce\né¦–å…ˆï¼Œhvd.init() ä¼šé€šè¿‡ä¸€ç³»åˆ—çš„è°ƒç”¨å’Œé…ç½®æœ€ç»ˆè°ƒç”¨ horovod/common/http://operations.cc ä¸‹çš„ InitializeHorovodOnce å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šæ ¹æ®åŠ è½½çš„é›†åˆé€šè®¯åº“ï¼ˆmpi æˆ–è€… glooï¼‰ä¸º globalstate åˆ›å»ºå¯¹åº”çš„ controllerï¼Œç„¶åä½¿ç”¨ BackgroundThreadLoop å¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹ã€‚\nhorovod/common/http://operations.cc #628\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 void InitializeHorovodOnce(const int* ranks, int nranks) { // ... some envParse #if HAVE_MPI // Enable mpi is it's used either i[n cpu data transfer or controller if (horovod_global.cpu_operation == LibType::MPI || horovod_global.control_operation == LibType::MPI) { mpi_context.Enable(); } // åˆ›å»ºä¸€ä¸ª MPIController å¯¹è±¡ if (horovod_global.control_operation == LibType::MPI){ horovod_global.controller.reset(new MPIController( horovod_global.response_cache, horovod_global.tensor_queue, horovod_global.timeline, horovod_global.parameter_manager, mpi_context)); horovod_global.controller-\u003eSetRanks(ranks, nranks); } #endif #if HAVE_GLOO //... #endif // Reset initialization flag horovod_global.initialization_done = false; // å¯åŠ¨åå°çº¿ç¨‹ horovod_global.background_thread = std::thread( BackgroundThreadLoop, std::ref(horovod_global)); } while (!horovod_global.initialization_done) { std::this_thread::sleep_for(std::chrono::milliseconds(1)); } } 2.BackgroundThreadLoop\nBackgroundThreadLoop ä¼šä¸º GlobalState åˆå§‹åŒ–ä¸€ç³»åˆ—åŒ…æ‹¬åˆå§‹åŒ– mpi_contextï¼Œ controllerçš„å…ƒç´ ï¼Œç„¶åè½®è¯¢è°ƒç”¨ RunLoopOnceï¼Œè¿˜æœ‰ä¸€äº›å¯¹ RunLoopOnce ç»“æŸåçš„åå¤„ç†ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 void BackgroundThreadLoop(HorovodGlobalState\u0026 state) { #if HAVE_MPI // Initialize mpi context auto mpi_ctx_manager = MPIContextManager(); #endif // mpi_context ä¼šæ ¹æ®å‰ç«¯å’Œç¯å¢ƒå˜é‡ä¼ è¿‡æ¥çš„ä¿¡æ¯ï¼Œåˆ›å»º mpi çº¿ç¨‹ï¼Œå’Œä¸€äº› mpiOps mpi_context.Initialize(state.controller-\u003eGetRanks(), mpi_ctx_manager); #endif // Initialize controller // ä¼šåŒæ­¥ä¸åŒ node çš„ global_size, local_size, rank, is_coordinator ç­‰ä¿¡æ¯ state.controller-\u003eInitialize(); // Set background thread affinity parse_and_set_affinity(std::getenv(HOROVOD_THREAD_AFFINITY), local_size, local_rank); #if HAVE_GPU ... // è®¾ç½® gpu_context çš„ stream æ•°ç›®ç­‰åˆå§‹åŒ–åŠ¨ä½œ #endif // ä¸‹é¢æ˜¯è®¾ç½® parameter_manager è¿™é‡Œä¸ºäº†èŠ‚çœç¯‡å¹…ç›´æ¥ç»™å‡ºï¼Œè®¾ç½®çš„è¯­å¥ï¼Œ // åŸæ¥è¿™é‡Œä¼šè¯»å–å¯¹åº”çš„ç¯å¢ƒå˜é‡çš„ï¼Œå»è®¾ç½® parameter_managerã€‚ // åé¢ä¹Ÿä¼šæœ‰ç¯‡å¹…ä»‹ç» parameter_managerï¼Œè¿™é‡Œå…ˆä¸å±•å¼€ã€‚ state.parameter_manager.SetTensorFusionThresholdBytes(64 * 1024 * 1024); state.parameter_manager.SetCycleTimeMs(5); state.parameter_manager.SetCacheEnabled(true); state.response_cache.set_capacity( (int)state.parameter_manager.CacheEnabled() * state.cache_capacity); state.parameter_manager.SetHierarchicalAllgather(value, true); state.parameter_manager.SetAutoTuning(true); ... // å…¶ä»–ä¸€äº›åˆå§‹åŒ–è®¾ç½® // è®¾ç½®op_managerï¼Œè¿™é‡Œä¸»è¦æ˜¯æ³¨å†Œä¸åŒçš„é›†åˆé€šä¿¡åº“çš„ ops //ï¼ˆ å¦‚ï¼šNCCLAllreduce, MPI_GPUAllgather ç­‰ï¼‰ op_manager.reset(CreateOperationManager(state)); // åˆå§‹åŒ–å®Œæˆ state.initialization_done = true; // Iterate until shutdown. try { while (RunLoopOnce(state)); } catch (const std::exception\u0026 ex) { LOG(ERROR) \u003c\u003c \"Horovod background loop uncaught exception: \" \u003c\u003c ex.what(); } ... // å…¶ä»–ä¸€äº›åå¤„ç†å‡½æ•° } 3.Optimizer.step()-\u003eDoAllReduce è¿™é‡Œæˆ‘ä»¬å…ˆä¸æ€¥ç€çœ‹ RunLoopOnce å‡½æ•°ï¼Œå…ˆå›åˆ° InitializeHorovodOnce ï¼Œå› ä¸ºä¸Šé¢çš„ initialization_done = Trueï¼Œæ‰€ä»¥ InitializeHorovodOnce å¯ä»¥é€€å‡ºäº†ï¼Œå°±æ˜¯å‰ç«¯çš„ hvd.init() å¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥äº†ã€‚è¿™é‡Œ main.py èµ°å®Œå‰å‘ loss = model(data,target)ï¼Œåå‘é€»è¾‘ loss.backward()ï¼Œè°ƒç”¨ optimizer.step() è¿›è¡Œæ¢¯åº¦åŒæ­¥ã€‚optimizer.step() ä¼šé€šè¿‡ä¸€ç³»åˆ—çš„è°ƒç”¨å’Œå¤„ç†ï¼ˆå¦‚ï¼šcompression ç­‰æ“ä½œï¼‰æœ€ç»ˆä¼šè°ƒç”¨ C++ interface çš„ DoAllReduce å‡½æ•°ã€‚\nDoAllReduce å‡½æ•°ä¼šè°ƒç”¨ EnqueueTensorAllreduce å‡½æ•°ä¼šæŠŠéœ€è¦ reduce çš„ tensor ç»„è£…æˆä¸€ä¸ªRequest å¾€ GlobalState çš„ tensor_queue é‡Œé¢å¡ã€‚è¿™é‡Œæ³¨æ„æ¯ä¸ª tensor ä¼šåˆ›å»ºå¯¹åº” TensorTableEntryï¼Œç”¨äºä¿å­˜tensor çš„æƒé‡ï¼Œmessage ä¸»è¦æ˜¯ä¸€äº› å…ƒä¿¡æ¯ metadataã€‚ç„¶åå°±ç­‰åå°çº¿ç¨‹å»è¯»å–è¿™äº›allreduce çš„è¯·æ±‚äº†ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Status EnqueueTensorAllreduce(std::shared_ptr\u003cOpContext\u003e context, std::shared_ptr\u003cTensor\u003e tensor, std::shared_ptr\u003cTensor\u003e output, std::shared_ptr\u003cReadyEvent\u003e ready_event, const std::string name, const int device, StatusCallback callback, ReduceOp reduce_op, double prescale_factor, double postscale_factor) { Status status; ... // some config Request message; message.set_request_rank(horovod_global.controller-\u003eGetRank()); message.set_tensor_name(name); message.set_tensor_type(tensor-\u003edtype()); message.set_device(device); message.set_prescale_factor(prescale_factor); message.set_postscale_factor(postscale_factor); if (reduce_op == ReduceOp::ADASUM) { message.set_request_type(Request::ADASUM); } else { message.set_request_type(Request::ALLREDUCE); } for (int i = 0; i \u003c tensor-\u003eshape().dims(); ++i) { message.add_tensor_shape((int64_t)tensor-\u003eshape().dim_size(i)); } TensorTableEntry e; e.tensor_name = name; e.context = context; e.tensor = tensor; e.output = output; e.ready_event = ready_event; e.device = device; e.callback = callback; if (horovod_global.shut_down) { return SHUT_DOWN_ERROR; } status = horovod_global.tensor_queue.AddToTensorQueue(e, message); if (status.ok()) { LOG(TRACE, horovod_global.controller-\u003eGetRank()) \u003c\u003c \"Enqueued \" \u003c\u003c name; } return status; } 4.RunLoopOnce\nå›åˆ°åå°çº¿ç¨‹ BackgroundThreadLoopï¼Œåé¢ä¼šè½®è¯¢è°ƒç”¨ RunLoopOnceã€‚ RunLoopOnceä¼šé¦–å…ˆè°ƒç”¨ ComputeResponseList å‡½æ•°ï¼Œå…¶ä¸»è¦å·¥ä½œæ˜¯åŒæ­¥ä¸åŒ worker ä¹‹é—´çš„éœ€è¦ allreduce çš„ tensorsï¼Œä¸ºåé¢ allreduce çš„æ‰§è¡Œåšå¥½å‡†å¤‡ã€‚\nï¼Ÿï¼Ÿï¼Ÿä¸ºä»€ä¹ˆä¼šåœ¨æ‰§è¡Œ tensor çš„ allreduce ä¹‹å‰æ‰§è¡Œè¿™æ ·ä¸€æ­¥å·¥ä½œå‘¢ï¼Ÿè€Œä¸æ˜¯ç›´æ¥æ‰§è¡Œ allreduce å‘¢ï¼Ÿæˆ‘è‡ªå·±çš„çŒœæµ‹æ˜¯ï¼Œå› ä¸ºåˆ†å¸ƒå¼è®­ç»ƒæ˜¯è¿è¡Œåœ¨ä¸åŒçš„æœºå­ä¸Šçš„ï¼Œå› ä¸º horovod æ²¡æœ‰å¼•å…¥ç±»ä¼¼å‚æ•°æœåŠ¡å™¨ï¼ˆparameter serverï¼‰çš„èŠ‚ç‚¹ï¼Œè€Œæ˜¯é‡‡å– master-worker çš„å½¢å¼ è¿›è¡Œ allreduceçš„ã€‚æ‰€ä»¥ allreduce çš„æ—¶å€™å¿…é¡»ç¡®ä¿æ‰€æœ‰çš„èŠ‚ç‚¹éƒ½æ˜¯èµ°åˆ°äº†åŒä¸€å¥ allreduce ä¸Šï¼Œç„¶åä¼ è¾“çš„ tensors ä¹Ÿè¦æ±‚æ˜¯ä¸€è‡´çš„ï¼Œå¦åˆ™ä¼ è¾“çš„ tensors æœ‰å¯èƒ½æ²¡æœ‰åŒ¹é…èµ·æ¥å°±æ‰§è¡Œallreduceï¼Œå¯¼è‡´ä¸€äº›ä¸å¯é¢„çŸ¥çš„é”™è¯¯ã€‚å¦å¤–è¿™éƒ¨åˆ†å¼•å…¥äº†ä¸€äº›æé«˜æ€§èƒ½çš„ tricksï¼Œå¦‚å¯¹ä¹‹å‰ reduce è¿‡çš„ tensor é€šè¿‡ä¸€ä¸ª bitmap è¿›è¡Œç¼“å­˜ï¼Œæ¯æ¬¡è°ƒç”¨çœ‹ä¸€ä¸‹æ˜¯ä¸æ˜¯éƒ½æ˜¯ä¹‹å‰çš„ tensorï¼Œå¦‚æœä¸æ˜¯å† update ä¸€ä¸‹ï¼Œä¸éœ€è¦æ¯æ¬¡éƒ½å…¨é‡æ›´æ–°ã€‚ï¼Ÿï¼Ÿï¼Ÿï¼ˆä¸æ˜¯å¾ˆç¡®å®šï¼‰\nComputeResponseListå…·ä½“çš„æµç¨‹æ˜¯(å¯ä»¥å¯¹ç…§ä¸Šé¢æµç¨‹å›¾çœ‹):\nä»è‡ªå·±è¿›ç¨‹çš„ GlobalState è¯»å– tensor_queue çš„ä¿¡æ¯ï¼Œå¦‚æœæœ‰æ–°çš„å…ƒç´ ï¼Œä¼šé€šè¿‡å›¾ä¸­ popMessagesFromQueue pop å‡ºæ¥ï¼Œç„¶åç»è¿‡ä¸€ç³»åˆ—å¤„ç†ç¼“å­˜åˆ° message_queue_tmp ä¸­ã€‚ å½“ worker åˆ°è¾¾äº†å‰ç«¯ all_reduce è¿™å¥çš„æ—¶å€™ï¼Œä¼šç”¨ message_queue_tmp æ•´ç†æˆä¸€ä¸ª message_listé€šè¿‡æµç¨‹å›¾ä¸­çš„ SendReadyTensors å‡½æ•°å¾€ä¸»èŠ‚ç‚¹( coordinator ) å‘é€ä¸€ä¸ªè¯·æ±‚è¡¨æ˜æˆ‘æ‰“ç®—reduceï¼Œç„¶åä¼šæŠŠå‡†å¤‡ reduce çš„ tensor ä¿¡æ¯é€šè¿‡ message_list è¿­ä»£åœ°é€è¿‡å»ï¼Œæœ€åæœ‰ä¸€ä¸ª Done çš„è¯·æ±‚ coordinator ä¼šæ¥æ”¶é€šè¿‡å›¾ä¸­ RecvReadyTensors è¿™äº› requestsï¼Œç„¶åä¿å­˜åœ¨ ready_to_reduce ä¸­ï¼Œcoordinator ä¼šæŒç»­æ¥æ”¶è¿™äº›ä¿¡æ¯ï¼Œç›´åˆ°è·å–çš„ Done çš„æ•°ç›®ç­‰äº global_sizeã€‚ coordinator ä¼šæ‰¾åˆ°æ‰€æœ‰å‡†å¤‡å¥½ reduce çš„ tensorsï¼Œé€šè¿‡ SendFinalTensors è¿”å›ä¸€ä¸ª response ç»™æ‰€æœ‰çš„ workerï¼Œå¦‚æœä¿¡æ¯æœ‰è¯¯ä¼šè¿”å›ä¸€ä¸ª errorï¼Œå‘é€å®Œæˆä¹Ÿä¼šå‘é€ä¸€ä¸ª Doneã€‚ worker ä¼šé€šè¿‡ RecvFinalTensors ç›‘å¬ response çš„ä¿¡æ¯ï¼Œæ•´ç†å‡ºéœ€è¦ reduce çš„ tensorï¼Œå½“æ”¶åˆ° Doneï¼Œä¼šå°è¯•è°ƒç”¨ performation å»è¿›è¡Œ reduce ã€‚ coordinator å’Œ worker éƒ½ä¼šæŠŠåŒæ­¥çš„ä¿¡æ¯æ•´ç†æˆä¸€ä¸ª responses çš„æ•°ç»„ç»™åˆ°åé¢çš„ PerformOperation æ“ä½œã€‚ è¿™é‡Œè¯´ä¸€ä¸‹mpiæ˜¯æ€ä¹ˆå®ç°çš„ï¼Œå°±æ˜¯å¯¹åº”çš„ coordinator å’Œ worker ä¼šé˜»å¡åœ°åˆ°åŒä¸€æ¡æŒ‡ä»¤ï¼š\nSendReadyTensors å’Œ RecvReadyTensors é˜»å¡åˆ° MPI_Gatherï¼ŒSendFinalTensors å’Œ RecvFinalTensors åˆ° MPI_Bcast ï¼Œå¯ä»¥è¿™æ ·åˆ†è¾¨ï¼šå¦‚æœæ˜¯ coordinator å‘é€çš„å°±æ˜¯ MPI_Bcastï¼Œå¦‚æœæ˜¯worker å‘é€çš„æ˜¯ MPI_Gatherã€‚é€šä¿¡éƒ½æ˜¯å…ˆåŒæ­¥éœ€è¦é€šä¿¡messageçš„å¤§å° lengthï¼Œå†åŒæ­¥messageï¼Œä»£ç å¦‚ä¸‹ï¼š\nhorovod/common/mpi/http://mpi_controller.cc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 void MPIController::SendReadyTensors(RequestList\u0026 message_list) { std::string encoded_message; RequestList::SerializeToString(message_list, encoded_message); int encoded_message_length = (int)encoded_message.length() + 1; // å…ˆ gather è¿™ä¸ª message çš„å¤§å° int ret_code = MPI_Gather(\u0026encoded_message_length, 1, MPI_INT, nullptr, 1, MPI_INT, RANK_ZERO, mpi_ctx_.mpi_comm); if (ret_code != MPI_SUCCESS) { throw std::runtime_error(\"MPI_Gather failed, see MPI output for details.\"); } // å† gather è¿™ä¸ª message ret_code = MPI_Gatherv((void*)encoded_message.c_str(), encoded_message_length, MPI_BYTE, nullptr, nullptr, nullptr, MPI_BYTE, RANK_ZERO, mpi_ctx_.mpi_comm); ... } void MPIController::RecvReadyTensors(std::vector\u003cstd::string\u003e\u0026 ready_to_reduce,std::vector\u003cRequestList\u003e\u0026 ready_list) { MPI_Gather(MPI_IN_PLACE, 1, MPI_INT, recvcounts, 1, MPI_INT, RANK_ZERO, mpi_ctx_.mpi_comm); ... MPI_Gatherv(nullptr, 0, MPI_BYTE, buffer, recvcounts, displcmnts, MPI_BYTE, RANK_ZERO, mpi_ctx_.mpi_comm); ... } void MPIController::RecvFinalTensors(ResponseList\u0026 response_list) { int msg_length; int ret_code = MPI_Bcast(\u0026msg_length, 1, MPI_INT, RANK_ZERO, mpi_ctx_.mpi_comm); if (ret_code != MPI_SUCCESS) { throw std::runtime_error( \"MPI_Broadcast failed, see MPI output for details.\"); } auto buffer = new uint8_t[msg_length]; ret_code = MPI_Bcast(buffer, msg_length, MPI_BYTE, RANK_ZERO, mpi_ctx_.mpi_comm); ... } 5.PerformOperation\nä» ComputeResponseList ç»§ç»­è·‘ RunLoopOnceï¼Œ ä¸åŒ node ä¸‹é¢ä¼šæ ¹æ®å‰é¢ ComputeResponseList è¿”å›çš„ response_list å¯¹æ¯ä¸ª response è½®è¯¢è°ƒç”¨ PerformOperation å®Œæˆå¯¹åº”çš„ reduce å·¥ä½œã€‚\nPerformOperation æµç¨‹ï¼š\nhorovod/common/http://operations.cc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void PerformOperation(Response response, HorovodGlobalState\u0026 state) { std::vector\u003cTensorTableEntry\u003e entries; auto\u0026 timeline = horovod_global.timeline; if (response.response_type() != Response::JOIN) { horovod_global.tensor_queue.GetTensorEntriesFromResponse(response, entries, state.joined); ... // å¯¹æ•°æ®é¢„å¤„ç†å’Œ buffer åˆå§‹åŒ– Status status; // æ‰§è¡Œ all_reduce ç­‰æ“ä½œ try { status = op_manager-\u003eExecuteOperation(entries, response); } catch (const std::exception\u0026 ex) { status = Status::UnknownError(ex.what()); } ... // è°ƒç”¨ callback å‡½æ•° } PerformOperation ä¼šä» horovod_global.tensor_queue é€šè¿‡å‡½æ•° GetTensorEntriesFromResponse å–å‡ºå¯¹åº”çš„ TensorEntry å¦‚æœè¿˜æ²¡åˆå§‹åŒ–bufferï¼Œè°ƒç”¨ horovod_global.fusion_buffer.InitializeBuffer åˆå§‹åŒ– ç„¶å status = op_manager-\u003eExecuteOperation(entries, response) ä¼šè°ƒç”¨ä¸åŒçš„ op-\u003eExecute(entries, response) æ‰§è¡Œreduce è¿ç®— ä¸‹é¢ä»¥ MPIAllreduce::Execute ä¸ºä¾‹ï¼š horovod/common/ops/http://mpi_operations.cc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Status MPIAllreduce::Execute(std::vector\u003cTensorTableEntry\u003e\u0026 entries, const Response\u0026 response) { ... // ä¸€äº›å˜é‡å£°æ˜ // æŠŠ tensor copy åˆ° buffer ä¸­ if (entries.size() \u003e 1) { timeline.ActivityStartAll(entries, MEMCPY_IN_FUSION_BUFFER); MemcpyInFusionBuffer(entries, fused_input_data, buffer_data, buffer_len); timeline.ActivityEndAll(entries); } else { fused_input_data = first_entry.tensor-\u003edata(); buffer_data = (void*) first_entry.output-\u003edata(); buffer_len = (size_t) first_entry.output-\u003esize(); } // Do allreduce const void* sendbuf = entries.size() \u003e 1 || fused_input_data == buffer_data ? MPI_IN_PLACE : fused_input_data; int op = MPI_Allreduce(sendbuf, buffer_data, (int) num_elements, mpi_context_-\u003eGetMPIDataType(first_entry.tensor), mpi_context_-\u003eGetMPISumOp(first_entry.tensor-\u003edtype()), mpi_context_-\u003eGetMPICommunicator(Communicator::GLOBAL)); if (op != MPI_SUCCESS) { throw std::runtime_error(\"MPI_Allreduce failed, see MPI output for details.\"); } // Copy memory out of the fusion buffer. // æŠŠ allreduce åçš„ tensor copy ä¼š entries if (entries.size() \u003e 1) { timeline.ActivityStartAll(entries, MEMCPY_OUT_FUSION_BUFFER); MemcpyOutFusionBuffer(buffer_data, entries); timeline.ActivityEndAll(entries); } return Status::OK(); } ç„¶åè°ƒç”¨ä¸åŒ entries çš„ callbackï¼Œè¿™é‡Œ callback ä¸€èˆ¬æ˜¯ç»™å‰ç«¯ä½œç›¸åº”çš„ã€‚ 6.parameter_manager.update\nå®Œæˆä¸Šè¿°æ­¥éª¤ä¹‹åï¼Œå¦‚æœè®¾ç½®äº† state.parameter_manager.IsAutoTuning()ï¼ŒRunLoopOnce è¿˜ä¼šè°ƒç”¨ç›¸å…³çš„é€»è¾‘ï¼Œè°ƒæ•´ä¼ è¾“çš„å‚æ•°ï¼Œç„¶åè¿”å› BackgroundThreadLoop é‡æ–°è°ƒç”¨ã€‚_é‡æ–°è°ƒç”¨æ—¶ä¼šç¡ä¸€å®šæ—¶é—´å†ç»§ç»­_ä¸Šè¿°ç¬¬ 3 - 5 æ­¥çš„å·¥ä½œã€‚\nå…¶ä»–å…³é”®æ¨¡å— ä¸Šé¢åªæ˜¯ä»‹ç»äº† horovod ä¸»æµç¨‹å·¥ä½œåŸç†ï¼Œä¸è¿‡ horovod è¿˜æœ‰å…¶ä»–ä¸€äº›æ¨¡å—ååŒä¸»æµç¨‹å·¥ä½œçš„ï¼Œä¸‹é¢ä¼šå¯¹å…¶ä¸­çš„ä¸€äº›æˆ‘è®¤ä¸ºå¯ä»¥å€¼å¾—ä¸€è¯´çš„æ¨¡å—è¯´ä¸€ä¸‹ã€‚\nParameter_manager: Parameter_manager ä¸»è¦æ˜¯ GlobalState çš„ä¸€ä¸ªç”¨äºç®¡ç†ä¸€äº›è°ƒèŠ‚ horovod æ€§èƒ½çš„å‚æ•°çš„ç®¡ç†å™¨ï¼Œåœ¨ BackgroundThreadLoop ä¸­è·Ÿå…¶ä»–çš„ GlobalState çš„å…ƒç´ ä¸€åŒåˆå§‹åŒ–ï¼Œç„¶åä¼šè¯»å–ä¸‹é¢è¿™äº›å¯¹åº”çš„ç¯å¢ƒå˜é‡ï¼Œç„¶åè¿›è¡Œè®¾ç½®ã€‚\nHOROVOD_FUSION_THRESHOLDï¼šæŒ‡ä¼ è¾“æ•°æ®åˆ‡ç‰‡çš„å¤§å°ï¼Œé»˜è®¤æ˜¯64Mï¼Œå¦‚æœåˆ‡ç‰‡å¤ªå¤§ï¼Œä¼ è¾“çš„æ—¶å€™å°±ä¸èƒ½å¾ˆå¥½åœ° pipeline ä¼ è¾“ï¼Œå¦‚æœå¤ªå°ï¼Œä¸€ä¸ª tensor éœ€è¦ä¼ è¾“å¤šæ¬¡ï¼Œå¢åŠ  IO çš„ overheadã€‚\nHOROVOD_CYCLE_TIMEï¼šæŒ‡ RunLoopOnce çš„ç¡çœ æ—¶é•¿ï¼Œé»˜è®¤æ˜¯ 5msï¼Œæˆ‘è‡ªå·±çš„çŒœæµ‹ï¼ˆè¿˜æ²¡è¿›è¡ŒéªŒè¯ï¼‰æ¯”è¾ƒç†æƒ³çš„ç¡çœ æ—¶é—´åº”è¯¥æ˜¯ RunLoopOnce å…¶ä½™é€»è¾‘å¤„ç†çš„æ—¶é—´ + HOROVOD_CYCLE_TIME åˆšå¥½ç­‰äºä¸€æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ‰€ç”¨çš„æ—¶é—´ï¼Œå› ä¸ºç¡å¤ªä¹…å‰ç«¯ä¼šåœ¨ç­‰ RunLoopOnce ç¡é†’ï¼›å¦‚æœç¡å¤ªçŸ­ï¼Œä¸æ–­åœ°è·‘ä¸€æ¬¡ RunLoopOnceï¼Œtensor_queue ä¹Ÿä¸ä¼šæœ‰æ–°çš„å…ƒç´ ï¼Œåªæ˜¯ç™½è·‘ã€‚\nHOROVOD_CACHE_CAPACITYï¼šæŒ‡ cache çš„å¤§å°ï¼Œè¿™ä¸ªå¯èƒ½è·Ÿ model å±‚æ•°å‚æ•°é‡ç›¸å…³äº†ã€‚\nHOROVOD_HIERARCHICAL_ALLGATHERï¼šæ˜¯å¦ä½¿ç”¨åˆ†å±‚çš„allgatherçš„æ–¹å¼ç­‰\nParameter_managerä¹Ÿæä¾›äº†å¯¹è¿™äº›å‚æ•°è‡ªåŠ¨è°ƒèŠ‚çš„åŠŸèƒ½ã€‚é€šè¿‡Parameter_manager.SetAutoTuningè¿›è¡Œè®¾ç½®ï¼Œè®¾ç½®åä¼šåœ¨åˆå§‹çš„å‡ ä¸ªbatchå°è¯•ä¸åŒçš„å‚æ•°ç»„åˆè¿›è¡Œé€šä¿¡ï¼Œåé¢ä¼šæ”¶æ•›åˆ°ä¸€ç»„æœ€ä¼˜çš„å‚æ•°å€¼ã€‚\nMPIContext mpi_context æ˜¯åœ¨åŠ è½½ C++ çš„ä»£ç æ—¶å€™å°±å·²ç»åˆ›å»ºäº†ï¼ŒåŒæ—¶åˆ›å»ºçš„è¿˜æœ‰å…¶ä»– contextï¼ˆ nccl_context, gpu_contextï¼‰ï¼Œä¸»è¦æ˜¯ç»´æŠ¤ä¸€äº›èŠ‚ç‚¹ä¸Š mpi é€šä¿¡çš„å¿…è¦ç¯å¢ƒä¿¡æ¯å’Œè®¾ç½®ï¼Œå¦‚ï¼š\n3 ä¸ª MPI communicatorï¼Œmpi_commï¼Œlocal_commï¼Œcross_comm åˆ†åˆ«è´Ÿè´£ horovod mpi ä¼ è¾“ï¼ŒèŠ‚ç‚¹å†…ä¼ è¾“ï¼Œå’ŒèŠ‚ç‚¹é—´åˆ†å±‚ä¼ è¾“ï¼ˆä¸»è¦ç”¨äº hierarchical allreduceï¼‰ã€‚ mpi_float16_t: horovod ä¸»è¦ä»¥ float16 ä¼ è¾“ã€‚ mpi_float16_sum: float16 å¯¹åº”çš„sum æ“ä½œã€‚ åœ¨ horovod ä½¿ç”¨ mpi çš„æ—¶å€™ï¼Œéƒ½ä¼šä½¿ç”¨ä¸Šé¢çš„ communicator è¿›è¡Œæ•°æ®ä¼ è¾“ã€‚\nTensorflow2 TensorFlow2 å‰ç«¯å¯¹ horovod çš„è°ƒç”¨è·Ÿ pytorch ç±»ä¼¼ï¼Œåªæ˜¯å› ä¸º tensorflow 2 æ˜¯é€šè¿‡ tape ç­‰çº§åˆ¶è®°å½•æ¢¯åº¦çš„, æ‰€ä»¥ä¼šæœ‰ä¸€äº›ä¸åŒã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 hvd.init() # Set up standard model. model = getattr(applications, args.model)(weights=None) opt = tf.optimizers.SGD(0.01) data = tf.random.uniform([args.batch_size, 224, 224, 3]) target = tf.random.uniform([args.batch_size, 1], minval=0, maxval=999, dtype=tf.int64) @tf.function def benchmark_step(first_batch): # Horovod: (optional) compression algorithm. compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none # Horovod: use DistributedGradientTape with tf.GradientTape() as tape: probs = model(data, training=True) loss = tf.losses.sparse_categorical_crossentropy(target, probs) # Horovod: add Horovod Distributed GradientTape. tape = hvd.DistributedGradientTape(tape, compression=compression) gradients = tape.gradient(loss, model.trainable_variables) opt.apply_gradients(zip(gradients, model.trainable_variables)) if first_batch: hvd.broadcast_variables(model.variables, root_rank=0) hvd.broadcast_variables(opt.variables(), root_rank=0) for x in range(args.num_iters): benchmark_step(first_batch=False) with tf.GradientTape() as tapeè¿™ä¸€å¥ä¼šè°ƒç”¨ horovod/tensorflow/__init__.py ä¸­_DistributedGradientTape ä¸‹ init å‡½æ•°æ³¨å†Œ allreduce çš„å¥æŸ„ï¼ˆhandleï¼‰ ç„¶åè°ƒç”¨ gradients = tape.gradient(loss, model.trainable_variables) ä¼šè°ƒç”¨ä¸€ç³»åˆ—çš„è·³è½¬æœ€åä¼šè°ƒç”¨ tensorflow/mpi_ops.py ä¸‹çš„ _allreduce ï¼Œè¿›è€Œè°ƒç”¨ `MPI_LIB.horovod_allreduce MPI_LIB.horovod_allreduce åœ¨ horovod/tensorflow/http://mpi_ops.cc ä¸­è¢« HorovodAllreduceOp æ‰€æ³¨å†Œï¼Œæ ¹æ® TensorFlow çš„ opsæµç¨‹ï¼Œä¼šè°ƒç”¨ ops.ComputeAsyncï¼Œåˆ°è¿™é‡Œä¼šè·Ÿ pytorch ç±»ä¼¼ä¼šè°ƒç”¨ EnqueueTensorAllreduce æŠŠå¯¹åº”çš„ tensor å’Œ ops é€åˆ° GlobalState çš„ tensor_queue ä¸­ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class HorovodAllreduceOp : public AsyncOpKernel { public: explicit HorovodAllreduceOp(OpKernelConstruction* context) : AsyncOpKernel(context) { OP_REQUIRES_OK(context, context-\u003eGetAttr(\"reduce_op\", \u0026reduce_op_)); OP_REQUIRES_OK(context, context-\u003eGetAttr(\"prescale_factor\", \u0026prescale_factor_)); OP_REQUIRES_OK(context, context-\u003eGetAttr(\"postscale_factor\", \u0026postscale_factor_)); OP_REQUIRES_OK(context, context-\u003eGetAttr(\"ignore_name_scope\", \u0026ignore_name_scope_)); } void ComputeAsync(OpKernelContext* context, DoneCallback done) override { OP_REQUIRES_OK_ASYNC(context, ConvertStatus(common::CheckInitialized()), done); ... // ä¸€äº›å˜é‡éªŒè¯ï¼Œåˆå§‹åŒ– auto enqueue_result = EnqueueTensorAllreduce( hvd_context, hvd_tensor, hvd_output, ready_event, node_name, device, [context, done](const common::Status\u0026 status) { context-\u003eSetStatus(ConvertStatus(status)); done(); }, reduce_op, (double) prescale_factor_, (double) postscale_factor_); OP_REQUIRES_OK_ASYNC(context, ConvertStatus(enqueue_result), done); } private: int reduce_op_; // Using float since TF does not support double OP attributes float prescale_factor_; float postscale_factor_; bool ignore_name_scope_; }; æ€»ç»“ horovod çš„æµç¨‹åˆ†æå¤§æ¦‚å°±æ˜¯è¿™æ ·ï¼Œæ²¡æœ‰ç‰¹åˆ«å¤æ‚ï¼Œä»£ç çš„é˜…è¯»ä½“éªŒä¹Ÿæ˜¯æ¯”è¾ƒå¥½çš„ï¼Œåœ¨ä¸»æµç¨‹çš„å…³é”®å‡½æ•°éƒ½æœ‰æ¯”è¾ƒæ¸…æ™°çš„æ³¨é‡Šã€‚å¯¹äºç¬¬ä¸‰æ–¹å¼€å‘è€…æ¥è¯´ï¼Œhorovod æœ¬èº«å·²ç»ç”¨äº†å¾ˆå¤šæé«˜æ€§èƒ½çš„ tricksï¼Œå¯ä»¥ custom ä¼˜åŒ–çš„åœ°æ–¹ä¸å¤šï¼Œä¸€äº›å¯ä»¥åŠ¨çš„å‚æ•°ï¼Œä¹Ÿå·²ç»æä¾›äº†autotuningï¼Œç›´æ¥ä½¿ç”¨å°±å¯ä»¥å¾—åˆ°å¾ˆå¥½çš„æ€§èƒ½ã€‚å¦‚æœå°è¯•ä¼˜åŒ–ï¼Œå¯èƒ½è¦ä»ä¼ è¾“ä¸Šç€æ‰‹ï¼Œå¦‚ BytePS ä¼šå°è¯•ä½¿ç”¨ä¸åŒçš„ç½‘ç»œæ‹“æ‰‘å¼•å…¥ä¸€äº› PS èŠ‚ç‚¹æé«˜å¸¦å®½ç­‰ï¼Œå¦‚æœæœ‰æ—¶é—´æˆ‘ä¹Ÿä¼šèŠä¸€ä¸‹è¿™ä¸ªã€‚å¦å¤–ä¸Šé¢çš„åˆ†æä¹Ÿæœ‰å¾ˆå¤šæ˜¯æˆ‘è‡ªå·±é˜…è¯»ä»£ç æ—¶å€™çš„ä¸€äº›æ€è€ƒå¯èƒ½ä¸ä¸€å®šå‡†ç¡®ï¼Œå¦‚æœæœ‰ä¸å‡†ç¡®æˆ–è€…æ¨¡ç³Šçš„åœ°æ–¹ï¼Œä¹Ÿå¸Œæœ›å¤§å®¶å¯ä»¥å¤šå¤šæ–§æ­£ã€‚\nReferences: [1]. https://zhuanlan.zhihu.com/p/332825987 [2]. https://zhuanlan.zhihu.com/p/158584571 [3]. https://zhuanlan.zhihu.com/p/79030485 [4]. https://github.com/zjykzj/pytorch-distributed [5]. MPIæ•™ç¨‹ https://blog.csdn.net/qq_47058489/article/details/125980505\nhttps://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1\u0026utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup\u0026depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup\u0026utm_relevant_index=1\n[5.] ubuntu20.04 + docker + horovod\nHorovod and Distributed Training ",
  "wordCount" : "7975",
  "inLanguage": "en",
  "datePublished": "2022-07-27T17:31:57+08:00",
  "dateModified": "2022-07-27T17:31:57+08:00",
  "author":[{
    "@type": "Person",
    "name": "Jian"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jian's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jianye0428.github.io/favicon/jian_icon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jianye0428.github.io/en/" accesskey="h" title="Jian&#39;s Blog (Alt + H)">
                <img src="https://jianye0428.github.io/favicon/jian_icon.png" alt="logo" aria-label="logo"
                    height="30">Jian&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://jianye0428.github.io/cn/" title="Chinese"
                            aria-label="Chinese">Chinese</a>
                    </li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jianye0428.github.io/en/myresume/" title="My Resume">
                    <span>My Resume</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/tags/" title="ğŸ”–Tags">
                    <span>ğŸ”–Tags</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/archives" title="ğŸ™‹ğŸ»â€â™‚ï¸Archive">
                    <span>ğŸ™‹ğŸ»â€â™‚ï¸Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/search/" title="ğŸ”Search (Alt &#43; /)" accesskey=/>
                    <span>ğŸ”Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jianye0428.github.io/en/">Home</a>&nbsp;Â»&nbsp;<a href="https://jianye0428.github.io/en/posts/">Posts</a></div>
    <h1 class="post-title">
      [Distributed Training] Horovod_and_Openmpi
    </h1>
    <div class="post-meta"><span title='2022-07-27 17:31:57 +0800 CST'>2022-07-27</span>&nbsp;Â·&nbsp;16 min&nbsp;Â·&nbsp;Jian&nbsp;|&nbsp;<a href="https://github.com/jianye0428/myblog/tree/main/content/posts/notes/2022-07-27_Horovod_and_Openmpi.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul><ul>
                    <li>
                        <a href="#horovod-%e4%bb%8b%e7%bb%8d" aria-label="Horovod ä»‹ç»">Horovod ä»‹ç»</a></li>
                    <li>
                        <a href="#%e9%9b%86%e5%90%88%e9%80%9a%e4%bf%a1%e5%ba%93" aria-label="é›†åˆé€šä¿¡åº“">é›†åˆé€šä¿¡åº“</a><ul>
                            
                    <li>
                        <a href="#%e7%82%b9%e5%af%b9%e7%82%b9%e9%80%9a%e4%bf%a1-point-to-point-communication" aria-label="ç‚¹å¯¹ç‚¹é€šä¿¡: Point-to-Point Communication">ç‚¹å¯¹ç‚¹é€šä¿¡: Point-to-Point Communication</a></li>
                    <li>
                        <a href="#%e9%9b%86%e5%90%88%e9%80%9a%e4%bf%a1" aria-label="é›†åˆé€šä¿¡">é›†åˆé€šä¿¡</a></li>
                    <li>
                        <a href="#%e5%ae%9e%e8%b7%b5" aria-label="å®è·µ:">å®è·µ:</a><ul>
                            
                    <li>
                        <a href="#pytorchdistributed" aria-label="pytorch.distributed">pytorch.distributed</a></li></ul>
                    </li>
                    <li>
                        <a href="#mpi" aria-label="MPI">MPI</a></li></ul>
                    </li>
                    <li>
                        <a href="#horovod%e6%b5%81%e7%a8%8b%e5%88%86%e6%9e%90" aria-label="Horovodæµç¨‹åˆ†æ">Horovodæµç¨‹åˆ†æ</a><ul>
                            
                    <li>
                        <a href="#pytorch-demo" aria-label="pytorch demo">pytorch demo</a></li>
                    <li>
                        <a href="#%e6%b5%81%e7%a8%8b%e5%88%86%e6%9e%90" aria-label="æµç¨‹åˆ†æ">æµç¨‹åˆ†æ</a></li>
                    <li>
                        <a href="#%e5%85%b6%e4%bb%96%e5%85%b3%e9%94%ae%e6%a8%a1%e5%9d%97" aria-label="å…¶ä»–å…³é”®æ¨¡å—">å…¶ä»–å…³é”®æ¨¡å—</a></li>
                    <li>
                        <a href="#mpicontext" aria-label="MPIContext">MPIContext</a></li>
                    <li>
                        <a href="#tensorflow2" aria-label="Tensorflow2">Tensorflow2</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e6%80%bb%e7%bb%93" aria-label="æ€»ç»“">æ€»ç»“</a></li></ul>
                        
                    <li>
                        <a href="#horovod-and-distributed-training" aria-label="Horovod and Distributed Training">Horovod and Distributed Training</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="horovod-ä»‹ç»">Horovod ä»‹ç»<a hidden class="anchor" aria-hidden="true" href="#horovod-ä»‹ç»">#</a></h2>
<p>Horovod æ˜¯ Uber å¼€æºçš„æ·±åº¦å­¦ä¹ å·¥å…·ï¼Œå®ƒçš„å‘å±•å¸å–äº†Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; ä¸ç™¾åº¦ &ldquo;Ring Allreduce&rdquo; çš„ä¼˜ç‚¹ï¼Œåœ¨ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„åŒæ—¶ï¼Œå…¼é¡¾äº†å‰ç«¯çš„ç®€æ´å’Œå¯¹ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æ¶çš„æ”¯æŒï¼Œä½¿ç”¨èµ·æ¥å¯¹å¼€å‘äººå‘˜æ¯”è¾ƒçš„å‹å¥½ï¼Œç®—æ˜¯åˆ†å¸ƒå¼è®­ç»ƒæ–¹å‘çš„æ ‡æ†é¡¹ç›®äº†ã€‚</p>
<h2 id="é›†åˆé€šä¿¡åº“">é›†åˆé€šä¿¡åº“<a hidden class="anchor" aria-hidden="true" href="#é›†åˆé€šä¿¡åº“">#</a></h2>
<p>é›†åˆé€šä¿¡åº“ï¼Œè¿™ä¸ªè¯å¯èƒ½å¬èµ·æ¥ä¼šæ¯”è¾ƒçš„é™Œç”Ÿï¼Œä¸è¿‡å¦‚æœæˆ‘å†æå‡ ä¸ªå…³é”®å­—ï¼Œå¯èƒ½å¤§å®¶å¤šå°‘éƒ½ä¼šæœ‰æ‰€è€³é—»ã€‚èµ„å†æ¯”è¾ƒè€çš„æ˜¯ MPI (<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Message_Passing_Interface">Message Passing Interface</a> åŠå…¶å®ç° <a href="https://link.zhihu.com/?target=https%3A//www.open-mpi.org/">OpenMPI</a> å’Œ <a href="https://link.zhihu.com/?target=https%3A//www.mpich.org/">MPICH</a>ï¼Œå¹´è½»ä¸€ç‚¹çš„ä¼šæ˜¯ Nvidia é’ˆå¯¹å…¶æ˜¾å¡å¼€æºçš„ NCCLï¼Œæˆ–è€…æ˜¯ facebook å¼€æºçš„ glooï¼Œæˆ–è€…æ˜¯åƒåä¸ºé’ˆå¯¹å…¶é«˜æ€§èƒ½ç¡¬ä»¶æä¾›çš„HCCLï¼Œå¤§ä½“ä¸Šéƒ½å¯ä»¥å½’å…¥åˆ°<strong>é›†åˆé€šä¿¡åº“</strong>çš„ç±»åˆ«ã€‚ä»–ä»¬ç›¸åŒçš„åœ°æ–¹æ˜¯å¤§ä½“ä¸Šä¼šéµç…§ MPI æä¾›çš„æ¥å£è§„å®šï¼Œå®ç°äº†åŒ…æ‹¬<font color=red><em>ç‚¹å¯¹ç‚¹é€šä¿¡</em></font>ï¼ˆSEND,RECVç­‰ï¼‰ï¼Œ<font color=red><em>é›†åˆé€šä¿¡</em></font>ï¼ˆ REDUCEï¼ŒBROADCASTï¼ŒALLREDUCEç­‰ï¼‰ç­‰ç›¸å…³æ¥å£ï¼Œç„¶åæ ¹æ®è‡ªå·±ç¡¬ä»¶æˆ–è€…æ˜¯ç³»ç»Ÿçš„éœ€è¦ï¼Œåœ¨åº•å±‚å®ç°ä¸Šè¿›è¡Œäº†ç›¸åº”çš„æ”¹åŠ¨ï¼Œä¿è¯æ¥å£çš„ç¨³å®šå’Œæ€§èƒ½ã€‚</p>
<h3 id="ç‚¹å¯¹ç‚¹é€šä¿¡-point-to-point-communication">ç‚¹å¯¹ç‚¹é€šä¿¡: Point-to-Point Communication<a hidden class="anchor" aria-hidden="true" href="#ç‚¹å¯¹ç‚¹é€šä¿¡-point-to-point-communication">#</a></h3>
<p><strong>Send/Recv:</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_send_and_recv.jpg" alt="Send_and_recv"  />
</p>
<h3 id="é›†åˆé€šä¿¡">é›†åˆé€šä¿¡<a hidden class="anchor" aria-hidden="true" href="#é›†åˆé€šä¿¡">#</a></h3>
<p><strong>Scatter/Gather</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_scatter_and_gather.jpg" alt="Scatter_and_gather"  />
</p>
<p><strong>reduce/allreduce</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_reduce_and_allreduce.jpg" alt="reduce_and_allreduce"  />
</p>
<p><strong>boradcast/all-gather</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_broadcast_and_all_gather.jpg" alt="broadcast_and_all_gather"  />
</p>
<p>è¿™é‡Œåœ¨æœºå™¨å­¦ä¹ è®­ç»ƒä¸­ä½¿ç”¨æ¯”è¾ƒå¤šçš„æ˜¯ <strong>all-reduce</strong>ï¼Œåœºæ™¯ç±»ä¼¼åœ¨ä¸åŒçš„ node ä¸Šè·‘ä¸åŒ batch çš„æ•°æ®ï¼Œç„¶åæ›´æ–°æ¢¯åº¦éœ€è¦ä»å„ä¸ªæ±‡æ€»ä¹‹åå¹³å‡å†å›ä¼ åˆ°å„è‡ªçš„ node ä¸­ã€‚è€Œè¿™éƒ¨åˆ†ï¼Œæœ‰å¾ˆå¤šç§å®ç°çš„æ–¹å¼ï¼Œæ¯”è¾ƒç›´è§‚å’Œç®€å•çš„æ˜¯æŠŠæ‰€æœ‰çš„æ¢¯åº¦éƒ½æ±‡æ€»åˆ°çš„æŸä¸€ä¸ª node ä¸Šï¼ˆå¦‚ä¸‹å›¾ node d æ‰€ç¤ºï¼‰ï¼Œç„¶åå†æŠŠæ±‡æ€»çš„ä¿¡æ¯é‡æ–°åˆ†å‘åˆ°ä¸åŒçš„ node ä¸Š ï¼Œè¿™æ ·å¯ä»¥è®¡ç®—é€šä¿¡é‡ï¼Œå¦‚ä¸‹ï¼šå¯¹äº P ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ¶ˆæ¯å¤§å°ä¸º Mï¼Œnode d èŠ‚ç‚¹çš„é€šä¿¡é‡ä¸º 2*(P-1)Mï¼Œè¿™é‡Œå‡è®¾èŠ‚ç‚¹ä¹‹é—´äº’è”äº’é€šï¼Œå¸¦å®½ä¸ºBã€‚</p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_Allreduce.jpg" alt="broadcast_and_all_gather"  />
</p>
<p>ä¸è¿‡è¿™ç§æƒ…å†µä¸‹ï¼Œå¾ˆå®¹æ˜“å¯¼è‡´ <strong>node d</strong> ä¼šæˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œå› ä¸º <strong>node d</strong> éœ€è¦è·Ÿå…¶ä»–æ‰€æœ‰ <strong>node</strong> é€šä¿¡æ‰€ä»¥å®ƒçš„é€šä¿¡é‡æ˜¯å…¶ä»–èŠ‚ç‚¹çš„ <strong>P</strong> å€ã€‚å‡è®¾èŠ‚ç‚¹é—´çš„å¸¦å®½è¿˜æ˜¯ä¸€æ ·ï¼Œ<strong>node d</strong> å®Œæˆæ‰€æœ‰é€šä¿¡æ‰€éœ€è¦çš„æ—¶é—´æ˜¯ <em><em>2</em>(P-1)M/B</em>*ã€‚æ‰€ä»¥ç°åœ¨å¾ˆå¤šçš„é›†åˆé€šä¿¡æ¡†æ¶ä¸ä¼šä½¿ç”¨è¿™ç§æ–¹å¼ï¼Œæ›´å¤šçš„æ˜¯<strong>é€šè¿‡æ ‘çŠ¶æˆ–è€…æ˜¯ç¯çŠ¶(ring) å»å®ç° all-reduce</strong>ã€‚</p>
<p>å¦‚æœåªæ˜¯åšæˆæ ‘çŠ¶çš„å¯ä»¥åšæˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè™½ç„¶ä¼ é€’çš„æ­¥æ•°å¢å¤šäº†ï¼Œä¸è¿‡æ¶ˆé™¤äº†node d çš„é€šä¿¡ç“¶é¢ˆï¼Œå®Œæˆæ‰€æœ‰çš„é€šä¿¡çš„æ—¶é—´å¤§æ¦‚æ˜¯ <em><em>2log_2N</em>(M/B)</em>*ï¼Œéšç€èŠ‚ç‚¹æ•°ç›® P çš„å¢åŠ ï¼Œæ ‘å½¢ç»“æ„çš„æ•ˆæœä¼šè¶Šæ¥è¶Šæ˜æ˜¾ã€‚</p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_Tree_Allreduce.jpg" alt="broadcast_and_all_gather"  />
</p>
<p>ä¸šç•Œç”¨å¾—æœ€å¤šä¸€ç§ä¼˜åŒ–çš„æ–¹å¼æ˜¯ï¼Œæ¯æ¬¡åªä¼ ä¸€éƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†æ˜¯ç™¾åº¦æå‡ºçš„ ring-allreduce çš„æ–¹æ¡ˆï¼Œå…·ä½“çš„ä»‹ç»å¯ä»¥å‚è€ƒè¿™ç¯‡åšå®¢<a href="https://link.zhihu.com/?target=https%3A//andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/">Bringing HPC Techniques to Deep Learning</a>ï¼Œè¿™è¾¹å°±ä¸èµ˜è¿°äº†ã€‚æ•´ä½“ä¸Šå°±æ˜¯æ¯æ¬¡ä¸ä¼šåƒä¸Šé¢è¿™æ ·æ•´ä»½æ•°æ®ä¼ é€’ï¼Œè€Œæ˜¯ä¸€éƒ¨åˆ†ä¸€éƒ¨åˆ†ä¼ ï¼Œä¼˜åŒ–åï¼Œæ‰€æœ‰èŠ‚ç‚¹éœ€è¦ä¼ è¾“çš„æ•°æ®é‡çš„ä¼ è¾“ <strong>2(Nâˆ’1)M/N</strong> æ¯”è¾ƒå¹³å‡ï¼Œæ‰€éœ€è¦çš„æ—¶é—´å¯ä»¥å¤§æ¦‚æ˜¯ <strong>2(Nâˆ’1)M/(NB)</strong>ï¼Œhorovod ä¹Ÿæ˜¯åŸºäºè¿™ç§ all-reduce çš„å½¢å¼å®ç°çš„ã€‚</p>
<h3 id="å®è·µ">å®è·µ:<a hidden class="anchor" aria-hidden="true" href="#å®è·µ">#</a></h3>
<h4 id="pytorchdistributed">pytorch.distributed<a hidden class="anchor" aria-hidden="true" href="#pytorchdistributed">#</a></h4>
<p>å°è¯•ä½¿ç”¨ pytorch è‡ªå¸¦çš„åˆ†å¸ƒå¼å·¥å…·åŒ… <a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/distributed.html">torch.distributed</a>ï¼Œè¿›è¡Œä¸€äº›æ¦‚å¿µæ€§çš„å°è¯•ã€‚</p>
<p>ä¸ºäº†æ–¹ä¾¿å°è¯•ï¼Œæˆ‘è¿™é‡Œæä¾›äº†ä¸€ä¸ªç®€å•çš„ demoï¼Œå¤§å®¶å¦‚æœå®‰è£…äº† gpu ç‰ˆæœ¬çš„ pytorch &gt;= 1.3ï¼Œåº”è¯¥éƒ½å¯ä»¥å°è¯•ä¸‹é¢çš„ä¾‹å­å°è¯•ä½¿ç”¨å¤šè¿›ç¨‹æ¨¡æ‹Ÿåˆ†å¸ƒå¼ï¼ˆå•æœºä¸Šå¯ä»¥è·‘ï¼‰ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">argparse</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;PyTorch MNIST Example&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-m&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--mode&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;one_device&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute mode, distributed/one_device&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--function&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;p2p&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;function to run (p2p/all_reduce/gpu_all_reduce)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-b&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--backend&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s2">&#34;nccl&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute backend (gloo/nccl)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Initialize the distributed environment. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">fn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data before send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Send the tensor to process 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Receive tensor from process 0</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data after send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">backend</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">backend</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;distributed&#34;</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;RANK&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in distribute mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;RANK&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in one device mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>å¯ä»¥ç®€å•åœ°è¿è¡Œä¸Šé¢çš„ä¾‹å­ï¼š</p>
<p><strong>send/recv:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span>
</span></span><span class="line"><span class="cl"><span class="c1"># è¾“å‡ºå¦‚ä¸‹ï¼š</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ä¸Šé¢æ˜¯æ¼”ç¤ºçš„æ˜¯é€šè¿‡ pytorch çš„ multiprocessing åŒ…ï¼Œæ¨¡æ‹Ÿä¸€æ¬¡åˆ†å¸ƒå¼çš„ send/recv è¿‡ç¨‹ï¼Œè¿™é‡Œæ˜¯ rank0 çš„è¿›ç¨‹å¾€ rank1 çš„è¿›ç¨‹å‘é€ä¸€ä¸ª tensorï¼Œå¯ä»¥çœ‹åˆ° rank 1 tensor åˆå§‹åŒ–ä¸º 0ï¼Œæ˜¯æ¥æ”¶åˆ° rank 0 çš„tensor åå˜ä¸º 1 çš„ã€‚ï¼ˆæ³¨æ„ï¼šè¿™é‡Œç‰¹åˆ«è®¾ç½®äº† backend ä¸º gloo æ˜¯å› ä¸º nccl ä¸æ”¯æŒ point2point çš„ä¼ è¾“ï¼Œå…·ä½“ä¸åŒ backend æ”¯æŒä»€ä¹ˆå½¢å¼çš„åŸè¯­ï¼Œå‚è€ƒæ–‡æ¡£backendéƒ¨åˆ† ï¼‰</p>
<p><strong>all_reduce</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># è¾“å‡ºå¦‚ä¸‹ï¼š</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># å¯¹åº”å‡½æ•° </span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># use rank 0 and rank 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>è¿™é‡Œä¹Ÿå¾ˆæµ…ç™½ï¼Œä¸»è¦å°±æ˜¯å¯¹ä¸¤ä¸ªè¿›ç¨‹ä¸Šçš„ tensor è¿›è¡Œä¸€æ¬¡ allreduceï¼Œå¯ä»¥çœ‹åˆ°ä¸¤ä¸ª rank ä¸Šçš„ç»“æœéƒ½ä¸º 2äº†ã€‚</p>
<p><strong>gpu_all_reduce</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># è¾“å‡ºå¦‚ä¸‹ï¼š</span>
</span></span><span class="line"><span class="cl"><span class="c1">#in one device mode</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;), tensor([1.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;), tensor([1.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:2&#39;), tensor([4.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:0&#39;), tensor([4.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  0  has data tensor[0]: tensor([8.], device=&#39;cuda:0&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:1&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  1  has data tensor[0]: tensor([8.], device=&#39;cuda:2&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:3&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># å¯¹åº”å‡½æ•° </span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<blockquote>
<p>all_reduce_multigpu: ç›¸å½“äºå°†å¤šä¸ªgpuå†…çš„å¤šè¿›ç¨‹çš„å€¼è¿›è¡Œç›¸åŠ ;
all_reduce: ç›¸å½“äºå•ä¸ªgpuå†…çš„å¤šè¿›ç¨‹çš„å€¼ç›¸åŠ </p>
</blockquote>
</blockquote>
<p>è¿™é‡Œæ¼”ç¤ºçš„æ˜¯å°è¯•å¯¹ä¸åŒè¿›ç¨‹ä¸‹å¤šä¸ª gpu (è¿™é‡Œæ˜¯ 4 ä¸ª) è¿›è¡Œ reduceï¼Œå…·ä½“é€»è¾‘å°±æ˜¯ï¼š</p>
<pre><code>- å¯¹ä¸åŒçš„è¿›ç¨‹åˆ†åˆ«æŠŠ tensor åˆå§‹åŒ–åœ¨ä¸åŒçš„ gpu ä¸Šï¼Œrank0 åˆå§‹åŒ–åœ¨ 0ï¼Œ1 gpu ä¸Šï¼Œrank 1 åœ¨ 2ï¼Œ3ä¸Šã€‚
- è¿›è¡Œä¸€æ¬¡ all_reduce_multigpu ï¼ˆè¿™ä¸ªå‡½æ•°è·Ÿ all_reduce ä¸åŒï¼Œæ˜¯æŠŠä¸åŒçš„ node ä¸Šä¸åŒçš„gpu ä¸Šçš„tensor éƒ½æ”¾åˆ°ä¸€ä¸ª list ä¸­ï¼Œè¿›è¡Œreduceï¼‰ï¼Œè¿™æ—¶æ‰€æœ‰ gpu ä¸Šçš„å€¼éƒ½æ˜¯4ï¼Œä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬å¯¹ tensor_list[0] çš„tensor åšä¸€æ¬¡all_reduceï¼Œå¾—åˆ°çš„ç»“æœåœ¨ gpu 0,2 ä¸Šçš„ tensor è¿›è¡Œäº†all_reduce ç»“æœæ˜¯ 8ï¼Œåœ¨ gpu 1,3 çš„ tensor æ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚
</code></pre>
<p><strong>å¤šterminalå°è¯•</strong></p>
<p>åœ¨éªŒè¯åˆ†å¸ƒå¼é€»è¾‘çš„æ—¶å€™ï¼Œå…¶å®æˆ‘ä»¬ä¸ä¸€å®šéœ€è¦å¤šå°æœºå­æ‰å¯ä»¥ï¼Œå¯¹ä¸€äº›ä¸æ¶‰åŠç½‘ç»œæ€§èƒ½çš„éªŒè¯ï¼Œå¯ä»¥å°è¯•åœ¨ä¸€å°æœºå­ä¸Šå¼€å¤šä¸ª terminal è¿›è¡ŒéªŒè¯ã€‚å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„ä¾‹å­ï¼Œåœ¨å¤šä¸ª terminal ä¸‹è·‘ä»¥ä¸‹å‘½ä»¤ã€‚</p>
<p><em>terminal0:</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">0</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># è¾“å‡ºå¦‚ä¸‹</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em>terminal1:</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">1</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># è¾“å‡ºå¦‚ä¸‹</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>è¿™é‡Œæ˜¯é€šè¿‡æœ¬åœ°æœºå­ä¸Šçš„å›é€åœ°å€è¿›è¡Œæ¨¡æ‹Ÿï¼Œç»“æœæ˜¯åˆ†åˆ«åœ¨ä¸åŒçš„ terminal å‘ˆç°ï¼Œå½“ç„¶å¯ä»¥ç”¨ä¸Šé¢çš„demoï¼Œåœ¨å¤šå°æœºå­ä¸Šè·‘ï¼Œä¸è¿‡éœ€è¦ä¿®æ”¹ä¸€ä¸‹ init_process å‡½æ•°ä¸­çš„ os.environ[&lsquo;MASTER_ADDR&rsquo;] = &lsquo;127.0.0.1&rsquo; ä¸º rank 0 æœºå­çš„ IPï¼Œè¿™é‡Œå°±ä¸æ¼”ç¤ºäº†ã€‚å…·ä½“ pytorch distributed å·¥å…·ç›¸å…³çš„å†…å®¹å¯ä»¥å‚è€ƒ<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/intermediate/dist_tuto.html">å®˜æ–¹åšå®¢</a></p>
<p>ç»ƒä¹ ï¼š å¦‚æœå¤§æ¦‚ç†è§£äº†ä¸Šé¢çš„ä¸€äº›é›†åˆé€šä¿¡çš„åŸè¯­ï¼Œå¯ä»¥å°è¯•ç€ç”¨ä¸Šé¢ pytorch æä¾›çš„ send/recv å°è¯•å»å®ç°ä¸€ä¸‹ä¸Šé¢çš„æ ‘çŠ¶ allreduceã€‚</p>
<h3 id="mpi">MPI<a hidden class="anchor" aria-hidden="true" href="#mpi">#</a></h3>
<p>æ›´æ·±å…¥çš„å°è¯•ï¼Œå¯ä»¥å°è¯•äº†è§£ä¸€ä¸‹ mpi çš„çŸ¥è¯†ï¼Œè¿™ä¸ª<a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/">mpi</a>æ•™ç¨‹ ç®—æ˜¯å†™å¾—æ¯”è¾ƒç³»ç»Ÿçš„ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒä¸€ä¸‹æ¥ç»ƒä¹ ï¼Œç‰¹åˆ«æ˜¯å¯¹åº•å±‚ä¸æ˜¯å¾ˆäº†è§£çš„åŒå­¦ï¼Œå¯ä»¥å¤šçœ‹çœ‹ <a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/running-an-mpi-cluster-within-a-lan/">Running an MPI cluster within a LAN</a> çš„éƒ¨åˆ†ï¼Œå®æ“ä¸€ä¸‹é€šè¿‡ ssh è·‘èµ·ä¸€ä¸ªåˆ†å¸ƒå¼çš„ demoã€‚é›†åˆé€šä¿¡åº“çš„åŸºç¡€å¤§æ¦‚å…ˆåˆ°è¿™é‡Œï¼Œå¦‚æœè¦æ·±å…¥çš„å¯ä»¥å†å»çœ‹çœ‹ <a href="https://link.zhihu.com/?target=https%3A//github.com/open-mpi/ompi/blob/98afc838aa53da88cba339f6dcbab256806a5745/ompi/mca/coll/tuned/coll_tuned_allreduce_decision.c">openMPI</a>ï¼Œå’Œ <a href="https://github.com/NVIDIA/nccl">nccl</a> çš„å®ç°ã€‚</p>
<h2 id="horovodæµç¨‹åˆ†æ">Horovodæµç¨‹åˆ†æ<a hidden class="anchor" aria-hidden="true" href="#horovodæµç¨‹åˆ†æ">#</a></h2>
<p>ä¸‹é¢æˆ‘ä¼šä»¥ä¸€ä¸ªç®€å•çš„ pytorch horovod çš„ demo å°è¯•å»ç†è§£ä¸€ä¸‹ horovod çš„å·¥ä½œæœºç†ï¼Œdemo å¦‚ä¸‹ï¼ˆçœç•¥äº†ä¸€äº›ä¸å…³é”®çš„ä»£ç æ®µï¼‰ã€‚ä¸ºäº†å‡†ç¡®èµ·è§ï¼Œæˆ‘ä»¬æ˜¯æ ¹æ® horovod v0.20.3 çš„ç‰ˆæœ¬è¿›è¡Œé˜…è¯»çš„ï¼Œå¦‚æœæ˜¯å…¶ä»–ç‰ˆæœ¬ï¼Œå¯èƒ½ä¼šè·Ÿè¿™é‡Œçš„å†…å®¹æœ‰ä¸€äº›å‡ºå…¥ã€‚</p>
<h3 id="pytorch-demo">pytorch demo<a hidden class="anchor" aria-hidden="true" href="#pytorch-demo">#</a></h3>
<p>ä¸€èˆ¬çš„ horovod è®­ç»ƒç¨‹åºéƒ½ä¼šåŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š</p>
<pre><code>1. hvd.init: å¯¹ horovod 
2. åˆå§‹åŒ–ã€‚åˆå§‹åŒ–æ¨¡å‹ï¼Œæ•°æ®é›†ï¼Œä¼˜åŒ–å™¨ï¼Œåˆå§‹åŒ–ä¸åŒ node çš„æ¨¡å‹æƒé‡ã€‚
3. ä½¿ç”¨ hvd.DistributedOptimizer åŒ…è£…ä¼˜åŒ–å™¨ã€‚
4. è¿›å…¥è®­ç»ƒæµç¨‹ï¼Œè¿›è¡Œä¼˜åŒ–è¿­ä»£ã€‚
</code></pre>
<p>æˆ‘ä»¬ä¼šç€é‡ä»‹ç»ç¬¬ 1 å’Œ 4 æ­¥ï¼Œå› ä¸ºä¸»è¦ä¹Ÿæ˜¯1ï¼Œ4æ­¥ä¼šè·Ÿ c++ åç«¯è¿›è¡Œä¿¡æ¯äº¤æ¢ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">timeit</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">...</span> <span class="c1"># some argparse</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">lr_scaler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl"><span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: wrap optimizer with DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">named_parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">op</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">Adasum</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_adasum</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Average</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: broadcast parameters &amp; optimizer state.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_optimizer_state</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up fixed fake data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">()</span> <span class="o">%</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1">#... some log configuration</span>
</span></span><span class="line"><span class="cl"><span class="n">img_secs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="n">benchmark_step</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span> <span class="o">/</span> <span class="n">time</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_secs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img_sec</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Results</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ç„¶åä¸‹å›¾æ˜¯æˆ‘å¯¹ horovod æ•´ä½“æµç¨‹çš„æ¢³ç†ï¼ŒæŠŠä¸€äº›ä¸æ˜¯å¾ˆå…³é”®çš„éƒ¨åˆ†éšè—äº†ï¼Œå¯èƒ½æœ‰ä¸€äº›ç»†èŠ‚çš„åœ°æ–¹å’Œå®ç°æœ‰å‡ºå…¥ï¼Œä¸è¿‡æˆ‘å¾…ä¼šä¼šæœ‰è¯¦ç»†çš„è¯´æ˜ã€‚è¿™é‡Œå…ˆè§£é‡Šä¸€ä¸‹ï¼Œä¸‹é¢å‡ ä¸ªå¤§çš„éƒ¨åˆ†:</p>
<ul>
<li>main.pyï¼š è¡¨ç¤ºè®­ç»ƒè„šæœ¬ï¼Œä¸€èˆ¬æ˜¯ ä½¿ç”¨ horovod æä¾›çš„å‡½æ•°è·Ÿç‰¹å®šçš„è®­ç»ƒæ¡†æ¶ç›¸äº’åˆä½œå®Œæˆåˆ†å¸ƒå¼è®­ç»ƒï¼ˆä¸‹æ–‡ç§°å‰ç«¯ï¼‰</li>
<li>C++ interfaceï¼šæ˜¯æŒ‡ horovod python å‡½æ•°è°ƒç”¨ C++ çš„æ¥å£</li>
<li>GlobalStateï¼šåœ¨ horovod ä¸­æ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œå…¶ä¸­çš„å…ƒç´ å¯ä»¥ä¾›ä¸åŒçš„çº¿ç¨‹è®¿é—®ï¼Œåœ¨åŠ è½½ C++ çš„ä»£ç æ—¶å€™å°±å·²ç»åˆ›å»ºäº†ï¼ŒåŒæ—¶åˆ›å»ºçš„è¿˜æœ‰å„ç§ contextï¼ˆmpi_context, nccl_context, gpu_contextï¼‰åé¢ä¼šæåˆ°ï¼Œä¸»è¦ä¼šåœ¨ä¸‹å›¾ backgroundThreadLoop ä¸­å®Œæˆ globalstate ä¸åŒå…ƒç´ åˆå§‹åŒ–ï¼Œæ¯”è¾ƒé‡è¦çš„æœ‰ controller ç®¡ç†æ€»ä½“é€šä¿¡æ§åˆ¶æµï¼Œtensor_queue ä¼šå¤„ç†ä»å‰ç«¯è¿‡æ¥çš„é€šä¿¡éœ€æ±‚ï¼ˆallreduceï¼Œbroadcast ç­‰ï¼‰ã€‚</li>
<li>BackgroundThreadLoopï¼šæ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„åå°çº¿ç¨‹ï¼Œä¸»è¦è´Ÿè´£è·Ÿå…¶ä»–èŠ‚ç‚¹çš„é€šä¿¡ï¼Œå’Œå¤„ç†å‰ç«¯è¿‡æ¥çš„é€šä¿¡éœ€æ±‚ï¼ˆrequestï¼‰ï¼Œä¼šè½®è¯¢è°ƒç”¨ RunLoopOnceï¼Œä¸æ–­æŸ¥çœ‹ tensor_queue ä¸­æœ‰æ²¡æœ‰éœ€è¦é€šä¿¡çš„tensorï¼Œå¦‚æœæœ‰è·Ÿå…¶ä»–èŠ‚ç‚¹åŒæ­¥æ›´æ–°ï¼Œç„¶åæ‰§è¡Œé€šä¿¡æ“ä½œã€‚</li>
</ul>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_Horovod_process.jpg" alt="Horovod Process"  />
</p>
<h3 id="æµç¨‹åˆ†æ">æµç¨‹åˆ†æ<a hidden class="anchor" aria-hidden="true" href="#æµç¨‹åˆ†æ">#</a></h3>
<p>ä¸‹é¢ä½¿ç”¨ mpi_controller è¿›è¡Œ allreduce æ“ä½œè¿›è¡Œåˆ†æã€‚</p>
<p><strong>1.hvd.init()-&gt;InitializeHorovodOnce</strong></p>
<p>é¦–å…ˆï¼Œhvd.init() ä¼šé€šè¿‡ä¸€ç³»åˆ—çš„è°ƒç”¨å’Œé…ç½®æœ€ç»ˆè°ƒç”¨ horovod/common/http://operations.cc ä¸‹çš„ InitializeHorovodOnce å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šæ ¹æ®åŠ è½½çš„<strong>é›†åˆé€šè®¯åº“</strong>ï¼ˆ<em>mpi</em> æˆ–è€… <em>gloo</em>ï¼‰ä¸º globalstate åˆ›å»ºå¯¹åº”çš„ controllerï¼Œç„¶åä½¿ç”¨ BackgroundThreadLoop å¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹ã€‚</p>
<p>horovod/common/http://operations.cc #628</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">InitializeHorovodOnce</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="kt">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ... some envParse
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Enable mpi is it&#39;s used either i[n cpu data transfer or controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">mpi_context</span><span class="p">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// åˆ›å»ºä¸€ä¸ª MPIController å¯¹è±¡
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">MPIController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">mpi_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SetRanks</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GLOO
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Reset initialization flag
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// å¯åŠ¨åå°çº¿ç¨‹
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">background_thread</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">BackgroundThreadLoop</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>2.BackgroundThreadLoop</strong></p>
<p>BackgroundThreadLoop ä¼šä¸º GlobalState åˆå§‹åŒ–ä¸€ç³»åˆ—åŒ…æ‹¬åˆå§‹åŒ– mpi_contextï¼Œ controllerçš„å…ƒç´ ï¼Œç„¶åè½®è¯¢è°ƒç”¨ RunLoopOnceï¼Œè¿˜æœ‰ä¸€äº›å¯¹ RunLoopOnce ç»“æŸåçš„åå¤„ç†ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">BackgroundThreadLoop</span><span class="p">(</span><span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize mpi context
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">mpi_ctx_manager</span> <span class="o">=</span> <span class="n">MPIContextManager</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// mpi_context ä¼šæ ¹æ®å‰ç«¯å’Œç¯å¢ƒå˜é‡ä¼ è¿‡æ¥çš„ä¿¡æ¯ï¼Œåˆ›å»º mpi çº¿ç¨‹ï¼Œå’Œä¸€äº› mpiOps 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">mpi_context</span><span class="p">.</span><span class="n">Initialize</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRanks</span><span class="p">(),</span> <span class="n">mpi_ctx_manager</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// ä¼šåŒæ­¥ä¸åŒ node çš„ global_size, local_size, rank, is_coordinator ç­‰ä¿¡æ¯
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">Initialize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set background thread affinity
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">parse_and_set_affinity</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">getenv</span><span class="p">(</span><span class="n">HOROVOD_THREAD_AFFINITY</span><span class="p">),</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GPU
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="p">...</span> <span class="c1">// è®¾ç½® gpu_context çš„ stream æ•°ç›®ç­‰åˆå§‹åŒ–åŠ¨ä½œ
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// ä¸‹é¢æ˜¯è®¾ç½® parameter_manager è¿™é‡Œä¸ºäº†èŠ‚çœç¯‡å¹…ç›´æ¥ç»™å‡ºï¼Œè®¾ç½®çš„è¯­å¥ï¼Œ
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// åŸæ¥è¿™é‡Œä¼šè¯»å–å¯¹åº”çš„ç¯å¢ƒå˜é‡çš„ï¼Œå»è®¾ç½® parameter_managerã€‚
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// åé¢ä¹Ÿä¼šæœ‰ç¯‡å¹…ä»‹ç» parameter_managerï¼Œè¿™é‡Œå…ˆä¸å±•å¼€ã€‚
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetTensorFusionThresholdBytes</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCycleTimeMs</span><span class="p">(</span><span class="mi">5</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCacheEnabled</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">response_cache</span><span class="p">.</span><span class="n">set_capacity</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">CacheEnabled</span><span class="p">()</span> <span class="o">*</span> <span class="n">state</span><span class="p">.</span><span class="n">cache_capacity</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetHierarchicalAllgather</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetAutoTuning</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// å…¶ä»–ä¸€äº›åˆå§‹åŒ–è®¾ç½®
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// è®¾ç½®op_managerï¼Œè¿™é‡Œä¸»è¦æ˜¯æ³¨å†Œä¸åŒçš„é›†åˆé€šä¿¡åº“çš„ ops
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//ï¼ˆ å¦‚ï¼šNCCLAllreduce, MPI_GPUAllgather ç­‰ï¼‰
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_manager</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">CreateOperationManager</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// åˆå§‹åŒ–å®Œæˆ
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Iterate until shutdown.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="p">(</span><span class="n">RunLoopOnce</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Horovod background loop uncaught exception: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// å…¶ä»–ä¸€äº›åå¤„ç†å‡½æ•°
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>3.Optimizer.step()-&gt;DoAllReduce</strong>
è¿™é‡Œæˆ‘ä»¬å…ˆä¸æ€¥ç€çœ‹ RunLoopOnce å‡½æ•°ï¼Œå…ˆå›åˆ° InitializeHorovodOnce ï¼Œå› ä¸ºä¸Šé¢çš„ initialization_done = Trueï¼Œæ‰€ä»¥ InitializeHorovodOnce å¯ä»¥é€€å‡ºäº†ï¼Œå°±æ˜¯å‰ç«¯çš„ hvd.init() å¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥äº†ã€‚è¿™é‡Œ main.py èµ°å®Œå‰å‘ loss = model(data,target)ï¼Œåå‘é€»è¾‘ loss.backward()ï¼Œè°ƒç”¨ optimizer.step() è¿›è¡Œæ¢¯åº¦åŒæ­¥ã€‚optimizer.step() ä¼šé€šè¿‡ä¸€ç³»åˆ—çš„è°ƒç”¨å’Œå¤„ç†ï¼ˆå¦‚ï¼šcompression ç­‰æ“ä½œï¼‰æœ€ç»ˆä¼šè°ƒç”¨ C++ interface çš„ DoAllReduce å‡½æ•°ã€‚</p>
<p><em><strong>DoAllReduce</strong></em> å‡½æ•°ä¼šè°ƒç”¨ EnqueueTensorAllreduce å‡½æ•°ä¼šæŠŠéœ€è¦ reduce çš„ tensor ç»„è£…æˆä¸€ä¸ªRequest å¾€ GlobalState çš„ tensor_queue é‡Œé¢å¡ã€‚è¿™é‡Œæ³¨æ„æ¯ä¸ª tensor ä¼šåˆ›å»ºå¯¹åº” TensorTableEntryï¼Œç”¨äºä¿å­˜tensor çš„æƒé‡ï¼Œmessage ä¸»è¦æ˜¯ä¸€äº› å…ƒä¿¡æ¯ metadataã€‚ç„¶åå°±ç­‰åå°çº¿ç¨‹å»è¯»å–è¿™äº›allreduce çš„è¯·æ±‚äº†ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="nf">EnqueueTensorAllreduce</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">OpContext</span><span class="o">&gt;</span> <span class="n">context</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">output</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyEvent</span><span class="o">&gt;</span> <span class="n">ready_event</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">StatusCallback</span> <span class="n">callback</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">ReduceOp</span> <span class="n">reduce_op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">prescale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">postscale_factor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span> <span class="c1">// some config
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Request</span> <span class="n">message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_request_rank</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_name</span><span class="p">(</span><span class="n">name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_type</span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_prescale_factor</span><span class="p">(</span><span class="n">prescale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_postscale_factor</span><span class="p">(</span><span class="n">postscale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">reduce_op</span> <span class="o">==</span> <span class="n">ReduceOp</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ALLREDUCE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dims</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">add_tensor_shape</span><span class="p">((</span><span class="kt">int64_t</span><span class="p">)</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dim_size</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">TensorTableEntry</span> <span class="n">e</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">ready_event</span> <span class="o">=</span> <span class="n">ready_event</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">shut_down</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">SHUT_DOWN_ERROR</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">status</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">AddToTensorQueue</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">TRACE</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">())</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Enqueued &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>4.RunLoopOnce</strong></p>
<p>å›åˆ°åå°çº¿ç¨‹ BackgroundThreadLoopï¼Œåé¢ä¼šè½®è¯¢è°ƒç”¨ RunLoopOnceã€‚ RunLoopOnceä¼šé¦–å…ˆè°ƒç”¨ ComputeResponseList å‡½æ•°ï¼Œå…¶ä¸»è¦å·¥ä½œæ˜¯åŒæ­¥ä¸åŒ worker ä¹‹é—´çš„éœ€è¦ allreduce çš„ tensorsï¼Œä¸ºåé¢ allreduce çš„æ‰§è¡Œåšå¥½å‡†å¤‡ã€‚</p>
<p>ï¼Ÿï¼Ÿï¼Ÿä¸ºä»€ä¹ˆä¼šåœ¨æ‰§è¡Œ tensor çš„ allreduce ä¹‹å‰æ‰§è¡Œè¿™æ ·ä¸€æ­¥å·¥ä½œå‘¢ï¼Ÿè€Œä¸æ˜¯ç›´æ¥æ‰§è¡Œ allreduce å‘¢ï¼Ÿæˆ‘è‡ªå·±çš„çŒœæµ‹æ˜¯ï¼Œå› ä¸ºåˆ†å¸ƒå¼è®­ç»ƒæ˜¯è¿è¡Œåœ¨ä¸åŒçš„æœºå­ä¸Šçš„ï¼Œå› ä¸º <u>horovod æ²¡æœ‰å¼•å…¥ç±»ä¼¼å‚æ•°æœåŠ¡å™¨ï¼ˆparameter serverï¼‰çš„èŠ‚ç‚¹ï¼Œè€Œæ˜¯é‡‡å– master-worker</u> çš„å½¢å¼ è¿›è¡Œ allreduceçš„ã€‚æ‰€ä»¥ allreduce çš„æ—¶å€™å¿…é¡»ç¡®ä¿æ‰€æœ‰çš„èŠ‚ç‚¹éƒ½æ˜¯èµ°åˆ°äº†åŒä¸€å¥ allreduce ä¸Šï¼Œç„¶åä¼ è¾“çš„ tensors ä¹Ÿè¦æ±‚æ˜¯ä¸€è‡´çš„ï¼Œå¦åˆ™ä¼ è¾“çš„ tensors æœ‰å¯èƒ½æ²¡æœ‰åŒ¹é…èµ·æ¥å°±æ‰§è¡Œallreduceï¼Œå¯¼è‡´ä¸€äº›ä¸å¯é¢„çŸ¥çš„é”™è¯¯ã€‚å¦å¤–è¿™éƒ¨åˆ†å¼•å…¥äº†ä¸€äº›æé«˜æ€§èƒ½çš„ tricksï¼Œå¦‚å¯¹ä¹‹å‰ reduce è¿‡çš„ tensor é€šè¿‡ä¸€ä¸ª bitmap è¿›è¡Œç¼“å­˜ï¼Œæ¯æ¬¡è°ƒç”¨çœ‹ä¸€ä¸‹æ˜¯ä¸æ˜¯éƒ½æ˜¯ä¹‹å‰çš„ tensorï¼Œå¦‚æœä¸æ˜¯å† update ä¸€ä¸‹ï¼Œä¸éœ€è¦æ¯æ¬¡éƒ½å…¨é‡æ›´æ–°ã€‚ï¼Ÿï¼Ÿï¼Ÿï¼ˆä¸æ˜¯å¾ˆç¡®å®šï¼‰</p>
<p><strong>ComputeResponseList</strong>å…·ä½“çš„æµç¨‹æ˜¯(å¯ä»¥å¯¹ç…§ä¸Šé¢æµç¨‹å›¾çœ‹):</p>
<ul>
<li>ä»è‡ªå·±è¿›ç¨‹çš„ GlobalState è¯»å– tensor_queue çš„ä¿¡æ¯ï¼Œå¦‚æœæœ‰æ–°çš„å…ƒç´ ï¼Œä¼šé€šè¿‡å›¾ä¸­ popMessagesFromQueue pop å‡ºæ¥ï¼Œç„¶åç»è¿‡ä¸€ç³»åˆ—å¤„ç†ç¼“å­˜åˆ° message_queue_tmp ä¸­ã€‚</li>
<li>å½“ worker åˆ°è¾¾äº†å‰ç«¯ all_reduce è¿™å¥çš„æ—¶å€™ï¼Œä¼šç”¨ message_queue_tmp æ•´ç†æˆä¸€ä¸ª message_listé€šè¿‡æµç¨‹å›¾ä¸­çš„ SendReadyTensors å‡½æ•°å¾€ä¸»èŠ‚ç‚¹( coordinator ) å‘é€ä¸€ä¸ªè¯·æ±‚è¡¨æ˜æˆ‘æ‰“ç®—reduceï¼Œç„¶åä¼šæŠŠå‡†å¤‡ reduce çš„ tensor ä¿¡æ¯é€šè¿‡ message_list è¿­ä»£åœ°é€è¿‡å»ï¼Œæœ€åæœ‰ä¸€ä¸ª Done çš„è¯·æ±‚</li>
<li>coordinator ä¼šæ¥æ”¶é€šè¿‡å›¾ä¸­ RecvReadyTensors è¿™äº› requestsï¼Œç„¶åä¿å­˜åœ¨ ready_to_reduce ä¸­ï¼Œcoordinator ä¼šæŒç»­æ¥æ”¶è¿™äº›ä¿¡æ¯ï¼Œç›´åˆ°è·å–çš„ Done çš„æ•°ç›®ç­‰äº global_sizeã€‚</li>
<li>coordinator ä¼šæ‰¾åˆ°æ‰€æœ‰å‡†å¤‡å¥½ reduce çš„ tensorsï¼Œé€šè¿‡ SendFinalTensors è¿”å›ä¸€ä¸ª response ç»™æ‰€æœ‰çš„ workerï¼Œå¦‚æœä¿¡æ¯æœ‰è¯¯ä¼šè¿”å›ä¸€ä¸ª errorï¼Œå‘é€å®Œæˆä¹Ÿä¼šå‘é€ä¸€ä¸ª Doneã€‚</li>
<li>worker ä¼šé€šè¿‡ RecvFinalTensors ç›‘å¬ response çš„ä¿¡æ¯ï¼Œæ•´ç†å‡ºéœ€è¦ reduce çš„ tensorï¼Œå½“æ”¶åˆ° Doneï¼Œä¼šå°è¯•è°ƒç”¨ performation å»è¿›è¡Œ reduce ã€‚</li>
<li>coordinator å’Œ worker éƒ½ä¼šæŠŠåŒæ­¥çš„ä¿¡æ¯æ•´ç†æˆä¸€ä¸ª responses çš„æ•°ç»„ç»™åˆ°åé¢çš„ PerformOperation æ“ä½œã€‚</li>
</ul>
<p>è¿™é‡Œè¯´ä¸€ä¸‹mpiæ˜¯æ€ä¹ˆå®ç°çš„ï¼Œå°±æ˜¯<u>å¯¹åº”çš„ coordinator å’Œ worker ä¼šé˜»å¡åœ°åˆ°åŒä¸€æ¡æŒ‡ä»¤</u>ï¼š</p>
<p>SendReadyTensors å’Œ RecvReadyTensors é˜»å¡åˆ° MPI_Gatherï¼ŒSendFinalTensors å’Œ RecvFinalTensors åˆ° MPI_Bcast ï¼Œå¯ä»¥è¿™æ ·åˆ†è¾¨ï¼š<font color=red><em>å¦‚æœæ˜¯ coordinator å‘é€çš„å°±æ˜¯ MPI_Bcastï¼Œå¦‚æœæ˜¯worker å‘é€çš„æ˜¯ MPI_Gather</font></em>ã€‚é€šä¿¡éƒ½æ˜¯å…ˆåŒæ­¥éœ€è¦é€šä¿¡messageçš„å¤§å° lengthï¼Œå†åŒæ­¥messageï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<p>horovod/common/mpi/http://mpi_controller.cc</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">SendReadyTensors</span><span class="p">(</span><span class="n">RequestList</span><span class="o">&amp;</span> <span class="n">message_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">encoded_message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">RequestList</span><span class="o">::</span><span class="n">SerializeToString</span><span class="p">(</span><span class="n">message_list</span><span class="p">,</span> <span class="n">encoded_message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">encoded_message_length</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">length</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// å…ˆ gather è¿™ä¸ª message çš„å¤§å°
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gather</span><span class="p">(</span><span class="o">&amp;</span><span class="n">encoded_message_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Gather failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// å† gather è¿™ä¸ª message
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gatherv</span><span class="p">((</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">encoded_message_length</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvReadyTensors</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;&amp;</span> <span class="n">ready_to_reduce</span><span class="p">,</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">RequestList</span><span class="o">&gt;&amp;</span> <span class="n">ready_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gather</span><span class="p">(</span><span class="n">MPI_IN_PLACE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gatherv</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="n">displcmnts</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvFinalTensors</span><span class="p">(</span><span class="n">ResponseList</span><span class="o">&amp;</span> <span class="n">response_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">msg_length</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="o">&amp;</span><span class="n">msg_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s">&#34;MPI_Broadcast failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">buffer</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">uint8_t</span><span class="p">[</span><span class="n">msg_length</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">  <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">msg_length</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>5.PerformOperation</strong></p>
<p>ä» ComputeResponseList ç»§ç»­è·‘ RunLoopOnceï¼Œ ä¸åŒ node ä¸‹é¢ä¼šæ ¹æ®å‰é¢ ComputeResponseList è¿”å›çš„ response_list å¯¹æ¯ä¸ª response è½®è¯¢è°ƒç”¨ PerformOperation å®Œæˆå¯¹åº”çš„ reduce å·¥ä½œã€‚</p>
<p>PerformOperation æµç¨‹ï¼š</p>
<p><code>horovod/common/http://operations.cc</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">PerformOperation</span><span class="p">(</span><span class="n">Response</span> <span class="n">response</span><span class="p">,</span> <span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;</span> <span class="n">entries</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">timeline</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">response_type</span><span class="p">()</span> <span class="o">!=</span> <span class="n">Response</span><span class="o">::</span><span class="n">JOIN</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">GetTensorEntriesFromResponse</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">entries</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                             <span class="n">state</span><span class="p">.</span><span class="n">joined</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// å¯¹æ•°æ®é¢„å¤„ç†å’Œ buffer åˆå§‹åŒ–
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// æ‰§è¡Œ all_reduce ç­‰æ“ä½œ
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">op_manager</span><span class="o">-&gt;</span><span class="n">ExecuteOperation</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">response</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">Status</span><span class="o">::</span><span class="n">UnknownError</span><span class="p">(</span><span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// è°ƒç”¨ callback å‡½æ•°
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>PerformOperation ä¼šä» horovod_global.tensor_queue é€šè¿‡å‡½æ•° <code>GetTensorEntriesFromResponse</code> å–å‡ºå¯¹åº”çš„ TensorEntry</li>
<li>å¦‚æœè¿˜æ²¡åˆå§‹åŒ–bufferï¼Œè°ƒç”¨ horovod_global.fusion_buffer.InitializeBuffer åˆå§‹åŒ–</li>
<li>ç„¶å status = op_manager-&gt;ExecuteOperation(entries, response) ä¼šè°ƒç”¨ä¸åŒçš„ op-&gt;Execute(entries, response) æ‰§è¡Œreduce è¿ç®—</li>
</ul>
<p>ä¸‹é¢ä»¥ <strong>MPIAllreduce::Execute</strong> ä¸ºä¾‹ï¼š
<code>horovod/common/ops/http://mpi_operations.cc</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="n">MPIAllreduce</span><span class="o">::</span><span class="n">Execute</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;&amp;</span> <span class="n">entries</span><span class="p">,</span> <span class="k">const</span> <span class="n">Response</span><span class="o">&amp;</span> <span class="n">response</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// ä¸€äº›å˜é‡å£°æ˜
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// æŠŠ tensor copy åˆ° buffer ä¸­
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_IN_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyInFusionBuffer</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">fused_input_data</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span> <span class="n">buffer_len</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">fused_input_data</span> <span class="o">=</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_data</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">size_t</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Do allreduce
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">sendbuf</span> <span class="o">=</span> <span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">fused_input_data</span> <span class="o">==</span> <span class="n">buffer_data</span>
</span></span><span class="line"><span class="cl">                        <span class="o">?</span> <span class="nl">MPI_IN_PLACE</span> <span class="p">:</span> <span class="n">fused_input_data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">op</span> <span class="o">=</span> <span class="n">MPI_Allreduce</span><span class="p">(</span><span class="n">sendbuf</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">num_elements</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPIDataType</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPISumOp</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPICommunicator</span><span class="p">(</span><span class="n">Communicator</span><span class="o">::</span><span class="n">GLOBAL</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Allreduce failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Copy memory out of the fusion buffer.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// æŠŠ allreduce åçš„ tensor copy ä¼š entries
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_OUT_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyOutFusionBuffer</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">,</span> <span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>ç„¶åè°ƒç”¨ä¸åŒ entries çš„ callbackï¼Œè¿™é‡Œ callback ä¸€èˆ¬æ˜¯ç»™å‰ç«¯ä½œç›¸åº”çš„ã€‚</li>
</ul>
<p><strong>6.parameter_manager.update</strong></p>
<p>å®Œæˆä¸Šè¿°æ­¥éª¤ä¹‹åï¼Œå¦‚æœè®¾ç½®äº† state.parameter_manager.IsAutoTuning()ï¼ŒRunLoopOnce è¿˜ä¼šè°ƒç”¨ç›¸å…³çš„é€»è¾‘ï¼Œè°ƒæ•´ä¼ è¾“çš„å‚æ•°ï¼Œç„¶åè¿”å› BackgroundThreadLoop é‡æ–°è°ƒç”¨ã€‚_é‡æ–°è°ƒç”¨æ—¶ä¼šç¡ä¸€å®šæ—¶é—´å†ç»§ç»­_ä¸Šè¿°ç¬¬ 3 - 5 æ­¥çš„å·¥ä½œã€‚</p>
<h3 id="å…¶ä»–å…³é”®æ¨¡å—">å…¶ä»–å…³é”®æ¨¡å—<a hidden class="anchor" aria-hidden="true" href="#å…¶ä»–å…³é”®æ¨¡å—">#</a></h3>
<p>ä¸Šé¢åªæ˜¯ä»‹ç»äº† horovod ä¸»æµç¨‹å·¥ä½œåŸç†ï¼Œä¸è¿‡ horovod è¿˜æœ‰å…¶ä»–ä¸€äº›æ¨¡å—ååŒä¸»æµç¨‹å·¥ä½œçš„ï¼Œä¸‹é¢ä¼šå¯¹å…¶ä¸­çš„ä¸€äº›æˆ‘è®¤ä¸ºå¯ä»¥å€¼å¾—ä¸€è¯´çš„æ¨¡å—è¯´ä¸€ä¸‹ã€‚</p>
<p><strong>Parameter_manager:</strong> Parameter_manager ä¸»è¦æ˜¯ GlobalState çš„ä¸€ä¸ªç”¨äºç®¡ç†ä¸€äº›è°ƒèŠ‚ horovod æ€§èƒ½çš„å‚æ•°çš„ç®¡ç†å™¨ï¼Œåœ¨ BackgroundThreadLoop ä¸­è·Ÿå…¶ä»–çš„ GlobalState çš„å…ƒç´ ä¸€åŒåˆå§‹åŒ–ï¼Œç„¶åä¼šè¯»å–ä¸‹é¢è¿™äº›å¯¹åº”çš„ç¯å¢ƒå˜é‡ï¼Œç„¶åè¿›è¡Œè®¾ç½®ã€‚</p>
<p><strong>HOROVOD_FUSION_THRESHOLD</strong>ï¼šæŒ‡ä¼ è¾“æ•°æ®åˆ‡ç‰‡çš„å¤§å°ï¼Œé»˜è®¤æ˜¯64Mï¼Œå¦‚æœåˆ‡ç‰‡å¤ªå¤§ï¼Œä¼ è¾“çš„æ—¶å€™å°±ä¸èƒ½å¾ˆå¥½åœ° pipeline ä¼ è¾“ï¼Œå¦‚æœå¤ªå°ï¼Œä¸€ä¸ª tensor éœ€è¦ä¼ è¾“å¤šæ¬¡ï¼Œå¢åŠ  IO çš„ overheadã€‚</p>
<p><strong>HOROVOD_CYCLE_TIME</strong>ï¼šæŒ‡ <u>RunLoopOnce çš„ç¡çœ æ—¶é•¿</u>ï¼Œé»˜è®¤æ˜¯ <strong>5ms</strong>ï¼Œæˆ‘è‡ªå·±çš„çŒœæµ‹ï¼ˆè¿˜æ²¡è¿›è¡ŒéªŒè¯ï¼‰æ¯”è¾ƒç†æƒ³çš„ç¡çœ æ—¶é—´åº”è¯¥æ˜¯ RunLoopOnce å…¶ä½™é€»è¾‘å¤„ç†çš„æ—¶é—´ + HOROVOD_CYCLE_TIME åˆšå¥½ç­‰äºä¸€æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ‰€ç”¨çš„æ—¶é—´ï¼Œå› ä¸ºç¡å¤ªä¹…å‰ç«¯ä¼šåœ¨ç­‰ RunLoopOnce ç¡é†’ï¼›å¦‚æœç¡å¤ªçŸ­ï¼Œä¸æ–­åœ°è·‘ä¸€æ¬¡ RunLoopOnceï¼Œtensor_queue ä¹Ÿä¸ä¼šæœ‰æ–°çš„å…ƒç´ ï¼Œåªæ˜¯ç™½è·‘ã€‚</p>
<p><strong>HOROVOD_CACHE_CAPACITY</strong>ï¼šæŒ‡ cache çš„å¤§å°ï¼Œè¿™ä¸ªå¯èƒ½è·Ÿ model å±‚æ•°å‚æ•°é‡ç›¸å…³äº†ã€‚</p>
<p><strong>HOROVOD_HIERARCHICAL_ALLGATHER</strong>ï¼šæ˜¯å¦ä½¿ç”¨åˆ†å±‚çš„allgatherçš„æ–¹å¼ç­‰</p>
<p>Parameter_managerä¹Ÿæä¾›äº†å¯¹è¿™äº›å‚æ•°è‡ªåŠ¨è°ƒèŠ‚çš„åŠŸèƒ½ã€‚é€šè¿‡Parameter_manager.SetAutoTuningè¿›è¡Œè®¾ç½®ï¼Œè®¾ç½®åä¼šåœ¨åˆå§‹çš„å‡ ä¸ªbatchå°è¯•ä¸åŒçš„å‚æ•°ç»„åˆè¿›è¡Œé€šä¿¡ï¼Œåé¢ä¼šæ”¶æ•›åˆ°ä¸€ç»„æœ€ä¼˜çš„å‚æ•°å€¼ã€‚</p>
<h3 id="mpicontext">MPIContext<a hidden class="anchor" aria-hidden="true" href="#mpicontext">#</a></h3>
<p>mpi_context æ˜¯åœ¨åŠ è½½ C++ çš„ä»£ç æ—¶å€™å°±å·²ç»åˆ›å»ºäº†ï¼ŒåŒæ—¶åˆ›å»ºçš„è¿˜æœ‰å…¶ä»– contextï¼ˆ nccl_context, gpu_contextï¼‰ï¼Œä¸»è¦æ˜¯ç»´æŠ¤ä¸€äº›èŠ‚ç‚¹ä¸Š mpi é€šä¿¡çš„å¿…è¦ç¯å¢ƒä¿¡æ¯å’Œè®¾ç½®ï¼Œå¦‚ï¼š</p>
<ul>
<li>3 ä¸ª MPI communicatorï¼Œmpi_commï¼Œlocal_commï¼Œcross_comm åˆ†åˆ«è´Ÿè´£ horovod mpi ä¼ è¾“ï¼ŒèŠ‚ç‚¹å†…ä¼ è¾“ï¼Œå’ŒèŠ‚ç‚¹é—´åˆ†å±‚ä¼ è¾“ï¼ˆä¸»è¦ç”¨äº hierarchical allreduceï¼‰ã€‚</li>
<li>mpi_float16_t: horovod ä¸»è¦ä»¥ float16 ä¼ è¾“ã€‚</li>
<li>mpi_float16_sum: float16 å¯¹åº”çš„sum æ“ä½œã€‚</li>
</ul>
<p>åœ¨ horovod ä½¿ç”¨ mpi çš„æ—¶å€™ï¼Œéƒ½ä¼šä½¿ç”¨ä¸Šé¢çš„ communicator è¿›è¡Œæ•°æ®ä¼ è¾“ã€‚</p>
<h3 id="tensorflow2">Tensorflow2<a hidden class="anchor" aria-hidden="true" href="#tensorflow2">#</a></h3>
<p>TensorFlow2 å‰ç«¯å¯¹ horovod çš„è°ƒç”¨è·Ÿ pytorch ç±»ä¼¼ï¼Œåªæ˜¯å› ä¸º tensorflow 2 æ˜¯é€šè¿‡ tape ç­‰çº§åˆ¶è®°å½•æ¢¯åº¦çš„, æ‰€ä»¥ä¼šæœ‰ä¸€äº›ä¸åŒã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">applications</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">999</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tf.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl">    <span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: use DistributedGradientTape</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: add Horovod Distributed GradientTape.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tape</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedGradientTape</span><span class="p">(</span><span class="n">tape</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>with tf.GradientTape() as tape</code>è¿™ä¸€å¥ä¼šè°ƒç”¨ <code>horovod/tensorflow/__init__.py</code> ä¸­<code>_DistributedGradientTape</code> ä¸‹ <strong>init</strong> å‡½æ•°æ³¨å†Œ allreduce çš„å¥æŸ„ï¼ˆhandleï¼‰</li>
<li>ç„¶åè°ƒç”¨ <code>gradients = tape.gradient(loss, model.trainable_variables)</code> ä¼šè°ƒç”¨ä¸€ç³»åˆ—çš„è·³è½¬æœ€åä¼šè°ƒç”¨ <code>tensorflow/mpi_ops.py</code> ä¸‹çš„ _allreduce ï¼Œè¿›è€Œè°ƒç”¨ `MPI_LIB.horovod_allreduce</li>
<li><code>MPI_LIB.horovod_allreduce</code> åœ¨ <code>horovod/tensorflow/http://mpi_ops.cc</code> ä¸­è¢« <code>HorovodAllreduceOp</code> æ‰€æ³¨å†Œï¼Œæ ¹æ® TensorFlow çš„ opsæµç¨‹ï¼Œä¼šè°ƒç”¨ <code>ops.ComputeAsync</code>ï¼Œåˆ°è¿™é‡Œä¼šè·Ÿ pytorch ç±»ä¼¼ä¼šè°ƒç”¨ <code>EnqueueTensorAllreduce</code> æŠŠå¯¹åº”çš„ tensor å’Œ ops é€åˆ° GlobalState çš„ tensor_queue ä¸­ã€‚</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodAllreduceOp</span> <span class="p">:</span> <span class="n">public</span> <span class="n">AsyncOpKernel</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="n">public</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">explicit</span> <span class="n">HorovodAllreduceOp</span><span class="p">(</span><span class="n">OpKernelConstruction</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">:</span> <span class="n">AsyncOpKernel</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;reduce_op&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">reduce_op_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;prescale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">prescale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;postscale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">postscale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;ignore_name_scope&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ignore_name_scope_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">void</span> <span class="n">ComputeAsync</span><span class="p">(</span><span class="n">OpKernelContext</span><span class="o">*</span> <span class="n">context</span><span class="p">,</span> <span class="n">DoneCallback</span> <span class="n">done</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">common</span><span class="p">::</span><span class="n">CheckInitialized</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span> <span class="o">//</span> <span class="n">ä¸€äº›å˜é‡éªŒè¯</span><span class="err">ï¼Œ</span><span class="n">åˆå§‹åŒ–</span>
</span></span><span class="line"><span class="cl">    <span class="n">auto</span> <span class="n">enqueue_result</span> <span class="o">=</span> <span class="n">EnqueueTensorAllreduce</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd_context</span><span class="p">,</span> <span class="n">hvd_tensor</span><span class="p">,</span> <span class="n">hvd_output</span><span class="p">,</span> <span class="n">ready_event</span><span class="p">,</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">done</span><span class="p">](</span><span class="n">const</span> <span class="n">common</span><span class="p">::</span><span class="n">Status</span><span class="o">&amp;</span> <span class="n">status</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">context</span><span class="o">-&gt;</span><span class="n">SetStatus</span><span class="p">(</span><span class="n">ConvertStatus</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">          <span class="n">done</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">prescale_factor_</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">postscale_factor_</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">enqueue_result</span><span class="p">),</span> <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">private</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="nb">int</span> <span class="n">reduce_op_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Using</span> <span class="nb">float</span> <span class="n">since</span> <span class="n">TF</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">support</span> <span class="n">double</span> <span class="n">OP</span> <span class="n">attributes</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">prescale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">postscale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">bool</span> <span class="n">ignore_name_scope_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="æ€»ç»“">æ€»ç»“<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“">#</a></h2>
<p>horovod çš„æµç¨‹åˆ†æå¤§æ¦‚å°±æ˜¯è¿™æ ·ï¼Œæ²¡æœ‰ç‰¹åˆ«å¤æ‚ï¼Œä»£ç çš„é˜…è¯»ä½“éªŒä¹Ÿæ˜¯æ¯”è¾ƒå¥½çš„ï¼Œåœ¨ä¸»æµç¨‹çš„å…³é”®å‡½æ•°éƒ½æœ‰æ¯”è¾ƒæ¸…æ™°çš„æ³¨é‡Šã€‚å¯¹äºç¬¬ä¸‰æ–¹å¼€å‘è€…æ¥è¯´ï¼Œhorovod æœ¬èº«å·²ç»ç”¨äº†å¾ˆå¤šæé«˜æ€§èƒ½çš„ tricksï¼Œå¯ä»¥ custom ä¼˜åŒ–çš„åœ°æ–¹ä¸å¤šï¼Œä¸€äº›å¯ä»¥åŠ¨çš„å‚æ•°ï¼Œä¹Ÿå·²ç»æä¾›äº†autotuningï¼Œç›´æ¥ä½¿ç”¨å°±å¯ä»¥å¾—åˆ°å¾ˆå¥½çš„æ€§èƒ½ã€‚å¦‚æœå°è¯•ä¼˜åŒ–ï¼Œå¯èƒ½è¦ä»ä¼ è¾“ä¸Šç€æ‰‹ï¼Œå¦‚ BytePS ä¼šå°è¯•ä½¿ç”¨ä¸åŒçš„ç½‘ç»œæ‹“æ‰‘å¼•å…¥ä¸€äº› PS èŠ‚ç‚¹æé«˜å¸¦å®½ç­‰ï¼Œå¦‚æœæœ‰æ—¶é—´æˆ‘ä¹Ÿä¼šèŠä¸€ä¸‹è¿™ä¸ªã€‚å¦å¤–ä¸Šé¢çš„åˆ†æä¹Ÿæœ‰å¾ˆå¤šæ˜¯æˆ‘è‡ªå·±é˜…è¯»ä»£ç æ—¶å€™çš„ä¸€äº›æ€è€ƒå¯èƒ½ä¸ä¸€å®šå‡†ç¡®ï¼Œå¦‚æœæœ‰ä¸å‡†ç¡®æˆ–è€…æ¨¡ç³Šçš„åœ°æ–¹ï¼Œä¹Ÿå¸Œæœ›å¤§å®¶å¯ä»¥å¤šå¤šæ–§æ­£ã€‚</p>
<p>References:
[1]. <a href="https://zhuanlan.zhihu.com/p/332825987">https://zhuanlan.zhihu.com/p/332825987</a>
[2]. <a href="https://zhuanlan.zhihu.com/p/158584571">https://zhuanlan.zhihu.com/p/158584571</a>
[3]. <a href="https://zhuanlan.zhihu.com/p/79030485">https://zhuanlan.zhihu.com/p/79030485</a>
[4]. <a href="https://github.com/zjykzj/pytorch-distributed">https://github.com/zjykzj/pytorch-distributed</a>
[5]. <a href="https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/">MPIæ•™ç¨‹</a>
<a href="https://blog.csdn.net/qq_47058489/article/details/125980505">https://blog.csdn.net/qq_47058489/article/details/125980505</a></p>
<p><a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1">https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1</a></p>
<p>[5.] <a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1">ubuntu20.04 + docker + horovod</a></p>
<h1 id="horovod-and-distributed-training">Horovod and Distributed Training<a hidden class="anchor" aria-hidden="true" href="#horovod-and-distributed-training">#</a></h1>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jianye0428.github.io/en/tags/distributed-training/">distributed training</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jianye0428.github.io/en/posts/tech/2022-08-01_c&#43;&#43;_data_structure/">
    <span class="title"><i class="fas fa-angle-double-left"></i> Prev Page</span>
    <br>
    <span>C&#43;&#43; STL (Standard Template Library) Containers</span>
  </a>
  <a class="next" href="https://jianye0428.github.io/en/posts/notes/2022-07-19_softwareinstallation/">
    <span class="title">Next Page <i class="fas fa-angle-double-right"></i></span>
    <br>
    <span>Software Installation Notes</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on twitter"
        href="https://twitter.com/intent/tweet/?text=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f&amp;hashtags=distributedtraining">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f&amp;title=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;summary=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;source=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f&title=%5bDistributed%20Training%5d%20Horovod_and_Openmpi">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on whatsapp"
        href="https://api.whatsapp.com/send?text=%5bDistributed%20Training%5d%20Horovod_and_Openmpi%20-%20https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on telegram"
        href="https://telegram.me/share/url?text=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>




<footer class="tc-container" id="comment">
    <div class="tc-title"><p class="c-title">Discussion</p></div>
    <div id="tcomments"></div>
</footer>
<script crossorigin="anonymous" src="/js/twikoo.min.b16100b7cf8a61759eab076a122482054e083087aad37c3be1fe2e293934dc34.js" integrity="sha256-sWEAt8&#43;KYXWeqwdqEiSCBU4IMIeq03w74f4uKTk03DQ="></script>
<script>
    twikoo.init({
        envId: 'https://my-repository-pink.vercel.app/',
        el: '#tcomments',
        region: 'ap-shanghai', 
        
        lang: 'zh-CN', 
    });
</script>

</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
