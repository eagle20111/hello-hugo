<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>[Distributed Training] Horovod_and_Openmpi | Jian&#39;s Blog</title>
<meta name="keywords" content="distributed training">
<meta name="description" content="Horovod 介绍 Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; 与百度 &ldquo;Ring Allreduce&rdquo; 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不">
<meta name="author" content="Jian">
<link rel="canonical" href="https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.1ea9c8832138446635789668415e5c75b8a534b191ee749a44f5ab404c9f27c2.css" integrity="sha256-HqnIgyE4RGY1eJZoQV5cdbilNLGR7nSaRPWrQEyfJ8I=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="apple-touch-icon" href="https://jianye0428.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://jianye0428.github.io/favicon/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="baidu-site-verification" content="code-9oLyeix0aK" />
<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4a41bf85d719f0e8c3165fc76904f546";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>



<script defer crossorigin="anonymous" src="/js/katex.min.8f5024e83d2055dd60e021751066111b0057e230db34911dd56242d67f0a4c86.js" integrity="sha256-j1Ak6D0gVd1g4CF1EGYRGwBX4jDbNJEd1WJC1n8KTIY="></script>


<script defer crossorigin="anonymous" src="/js/auto-render.min.b09accad850e4e87b8a2fc8b93fae790def79172b68de72fd777958c52e566ad.js" integrity="sha256-sJrMrYUOToe4ovyLk/rnkN73kXK2jecv13eVjFLlZq0="></script>

<script>
    
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });

    
    window.WebFontConfig = {
        custom: {
            families: ['KaTeX_AMS', 'KaTeX_Caligraphic:n4,n7', 'KaTeX_Fraktur:n4,n7',
            'KaTeX_Main:n4,n7,i4,i7', 'KaTeX_Math:i4,i7', 'KaTeX_Script',
            'KaTeX_SansSerif:n4,n7,i4', 'KaTeX_Size1', 'KaTeX_Size2', 'KaTeX_Size3',
            'KaTeX_Size4', 'KaTeX_Typewriter'],
        },
    };
</script>


<script defer crossorigin="anonymous" src="/js/webfontloader.min.min.d1c6c39d18e2decb5c99dc9efc579098ab37b9654725df3f9c0737bc2dd00760.js" integrity="sha256-0cbDnRji3stcmdye/FeQmKs3uWVHJd8/nAc3vC3QB2A="></script>


 

<script async src="https://www.googletagmanager.com/gtag/js?id=G-C6GDZ56F4S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-C6GDZ56F4S', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="[Distributed Training] Horovod_and_Openmpi" />
<meta property="og:description" content="Horovod 介绍 Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; 与百度 &ldquo;Ring Allreduce&rdquo; 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/" /><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-27T17:31:57&#43;08:00" />
<meta property="article:modified_time" content="2022-07-27T17:31:57&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/>

<meta name="twitter:title" content="[Distributed Training] Horovod_and_Openmpi"/>
<meta name="twitter:description" content="Horovod 介绍 Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; 与百度 &ldquo;Ring Allreduce&rdquo; 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jianye0428.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "[Distributed Training] Horovod_and_Openmpi",
      "item": "https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[Distributed Training] Horovod_and_Openmpi",
  "name": "[Distributed Training] Horovod_and_Openmpi",
  "description": "Horovod 介绍 Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook \u0026ldquo;Training ImageNet In 1 Hour\u0026rdquo; 与百度 \u0026ldquo;Ring Allreduce\u0026rdquo; 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不",
  "keywords": [
    "distributed training"
  ],
  "articleBody": "Horovod 介绍 Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook “Training ImageNet In 1 Hour” 与百度 “Ring Allreduce” 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不同深度学习框架的支持，使用起来对开发人员比较的友好，算是分布式训练方向的标杆项目了。\n集合通信库 集合通信库，这个词可能听起来会比较的陌生，不过如果我再提几个关键字，可能大家多少都会有所耳闻。资历比较老的是 MPI (Message Passing Interface 及其实现 OpenMPI 和 MPICH，年轻一点的会是 Nvidia 针对其显卡开源的 NCCL，或者是 facebook 开源的 gloo，或者是像华为针对其高性能硬件提供的HCCL，大体上都可以归入到集合通信库的类别。他们相同的地方是大体上会遵照 MPI 提供的接口规定，实现了包括点对点通信（SEND,RECV等），集合通信（ REDUCE，BROADCAST，ALLREDUCE等）等相关接口，然后根据自己硬件或者是系统的需要，在底层实现上进行了相应的改动，保证接口的稳定和性能。\n点对点通信: Point-to-Point Communication Send/Recv:\n集合通信 Scatter/Gather\nreduce/allreduce\nboradcast/all-gather\n这里在机器学习训练中使用比较多的是 all-reduce，场景类似在不同的 node 上跑不同 batch 的数据，然后更新梯度需要从各个汇总之后平均再回传到各自的 node 中。而这部分，有很多种实现的方式，比较直观和简单的是把所有的梯度都汇总到的某一个 node 上（如下图 node d 所示），然后再把汇总的信息重新分发到不同的 node 上 ，这样可以计算通信量，如下：对于 P 个节点，每个节点消息大小为 M，node d 节点的通信量为 2*(P-1)M，这里假设节点之间互联互通，带宽为B。\n不过这种情况下，很容易导致 node d 会成为性能瓶颈，因为 node d 需要跟其他所有 node 通信所以它的通信量是其他节点的 P 倍。假设节点间的带宽还是一样，node d 完成所有通信所需要的时间是 2(P-1)M/B*。所以现在很多的集合通信框架不会使用这种方式，更多的是通过树状或者是环状(ring) 去实现 all-reduce。\n如果只是做成树状的可以做成如下图所示，虽然传递的步数增多了，不过消除了node d 的通信瓶颈，完成所有的通信的时间大概是 2log_2N(M/B)*，随着节点数目 P 的增加，树形结构的效果会越来越明显。\n业界用得最多一种优化的方式是，每次只传一部分，这部分是百度提出的 ring-allreduce 的方案，具体的介绍可以参考这篇博客Bringing HPC Techniques to Deep Learning，这边就不赘述了。整体上就是每次不会像上面这样整份数据传递，而是一部分一部分传，优化后，所有节点需要传输的数据量的传输 2(N−1)M/N 比较平均，所需要的时间可以大概是 2(N−1)M/(NB)，horovod 也是基于这种 all-reduce 的形式实现的。\n实践: pytorch.distributed 尝试使用 pytorch 自带的分布式工具包 torch.distributed，进行一些概念性的尝试。\n为了方便尝试，我这里提供了一个简单的 demo，大家如果安装了 gpu 版本的 pytorch \u003e= 1.3，应该都可以尝试下面的例子尝试使用多进程模拟分布式（单机上可以跑）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import os import torch import torch.distributed as dist import time import argparse from torch.multiprocessing import Process parser = argparse.ArgumentParser(description='PyTorch MNIST Example') parser.add_argument('-m', '--mode', type=str, default='one_device', metavar='N', help='distribute mode, distributed/one_device') parser.add_argument('-f', '--function', type=str, default='p2p', metavar='N', help='function to run (p2p/all_reduce/gpu_all_reduce)') parser.add_argument('-b', '--backend', type=str, default=\"nccl\", metavar='N', help='distribute backend (gloo/nccl)') def init_process(rank, size, fn, backend='nccl'): \"\"\" Initialize the distributed environment. \"\"\" os.environ['MASTER_ADDR'] = '127.0.0.1' os.environ['MASTER_PORT'] = '29500' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) def run(rank, size): tensor = torch.zeros(1) print('Rank ', rank, ' has data before send/recv', tensor) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print('Rank ', rank, ' has data after send/recv', tensor) def run_allreduce(rank, size): \"\"\" Simple reduce communication. \"\"\" group = dist.new_group([0, 1]) device = torch.device('cuda:%d' % rank) tensor = torch.ones(1).to(device) dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) def run_multigpu_allreduce(rank, size): group = dist.new_group([0, 1]) tensor_list = [] for dev_idx in range(2): device = torch.device('cuda:%d' % (2 * rank + dev_idx)) tensor = torch.ones(1).to(device) tensor_list.append(tensor) dist.all_reduce_multigpu(tensor_list) print('all_reduce_multigpu', tensor_list) dist.all_reduce(tensor_list[0], op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data tensor[0]:', tensor_list[0], \", tensor[1]:\", tensor_list[1]) if __name__ == \"__main__\": args = parser.parse_args() backend = args.backend if args.mode == \"distributed\" or os.environ.get('RANK',None): print(\"in distribute mode\") if args.function == \"all_reduce\": function, size = run_allreduce, 2 elif args.function == \"gpu_all_reduce\": function, size = run_multigpu_allreduce, 2 else: function, size, backend = run, 2, \"gloo\" rank = int(os.environ['RANK']) p = Process(target=init_process, args=(rank, size, function, backend)) p.start() p.join() else: print(\"in one device mode\") if args.function == \"all_reduce\": function, size = run_allreduce, 2 elif args.function == \"gpu_all_reduce\": function, size = run_multigpu_allreduce, 2 else: function, size, backend = run, 2, \"gloo\" processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, function, backend)) p.start() processes.append(p) for p in processes: p.join() 可以简单地运行上面的例子：\nsend/recv:\n1 2 3 4 5 6 7 8 $ python3 distribute_test.py # 输出如下： in one device mode Rank 0 has data before send/recv tensor([0.]) Rank 1 has data before send/recv tensor([0.]) Rank 0 has data after send/recv tensor([1.]) Rank 1 has data after send/recv tensor([1.]) 上面是演示的是通过 pytorch 的 multiprocessing 包，模拟一次分布式的 send/recv 过程，这里是 rank0 的进程往 rank1 的进程发送一个 tensor，可以看到 rank 1 tensor 初始化为 0，是接收到 rank 0 的tensor 后变为 1 的。（注意：这里特别设置了 backend 为 gloo 是因为 nccl 不支持 point2point 的传输，具体不同 backend 支持什么形式的原语，参考文档backend部分 ）\nall_reduce\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ python3 distribute_test.py -f all_reduce # 输出如下： in one device mode Rank 0 has data tensor(2., device='cuda:0') Rank 1 has data tensor(2., device='cuda:1') # 对应函数 def run_allreduce(rank, size): \"\"\" Simple reduce communication. \"\"\" group = dist.new_group([0, 1]) # use rank 0 and rank 1 device = torch.device('cuda:%d' % rank) tensor = torch.ones(1).to(device) dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data ', tensor[0]) 这里也很浅白，主要就是对两个进程上的 tensor 进行一次 allreduce，可以看到两个 rank 上的结果都为 2了。\ngpu_all_reduce\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ python3 distribute_test.py -f gpu_all_reduce # 输出如下： #in one device mode # [tensor([1.], device='cuda:0')] # [tensor([1.], device='cuda:2')] # [tensor([1.], device='cuda:2'), tensor([1.], device='cuda:3')] # [tensor([1.], device='cuda:0'), tensor([1.], device='cuda:1')] #all_reduce_multigpu [tensor([4.], device='cuda:2'), tensor([4.], device='cuda:3')] #all_reduce_multigpu [tensor([4.], device='cuda:0'), tensor([4.], device='cuda:1')] #Rank 0 has data tensor[0]: tensor([8.], device='cuda:0') , tensor[1]: tensor([4.], device='cuda:1') #Rank 1 has data tensor[0]: tensor([8.], device='cuda:2') , tensor[1]: tensor([4.], device='cuda:3') # 对应函数 def run_multigpu_allreduce(rank, size): group = dist.new_group([0, 1]) tensor_list = [] for dev_idx in range(2): device = torch.device('cuda:%d' % (2 * rank + dev_idx)) tensor = torch.ones(1).to(device) tensor_list.append(tensor) print(tensor_list) dist.all_reduce_multigpu(tensor_list) print('all_reduce_multigpu', tensor_list) dist.all_reduce(tensor_list[0], op=dist.ReduceOp.SUM, group=group) print('Rank ', rank, ' has data tensor[0]:', tensor_list[0], \", tensor[1]:\", tensor_list[1]) all_reduce_multigpu: 相当于将多个gpu内的多进程的值进行相加; all_reduce: 相当于单个gpu内的多进程的值相加\n这里演示的是尝试对不同进程下多个 gpu (这里是 4 个) 进行 reduce，具体逻辑就是：\n- 对不同的进程分别把 tensor 初始化在不同的 gpu 上，rank0 初始化在 0，1 gpu 上，rank 1 在 2，3上。 - 进行一次 all_reduce_multigpu （这个函数跟 all_reduce 不同，是把不同的 node 上不同的gpu 上的tensor 都放到一个 list 中，进行reduce），这时所有 gpu 上的值都是4，作为对比，我们对 tensor_list[0] 的tensor 做一次all_reduce，得到的结果在 gpu 0,2 上的 tensor 进行了all_reduce 结果是 8，在 gpu 1,3 的 tensor 没有任何变化。 多terminal尝试\n在验证分布式逻辑的时候，其实我们不一定需要多台机子才可以，对一些不涉及网络性能的验证，可以尝试在一台机子上开多个 terminal 进行验证。可以使用上面的例子，在多个 terminal 下跑以下命令。\nterminal0:\n1 2 3 4 5 RANK=0 python3 distribute_test.py -f gpu_all_reduce # 输出如下 in distribute mode all_reduce_multigpu [tensor([4.], device='cuda:0'), tensor([4.], device='cuda:1')] Rank 0 has data tensor[0]: tensor([8.], device='cuda:0') , tensor[1]: tensor([4.], device='cuda:1') terminal1:\n1 2 3 4 5 RANK=1 python3 distribute_test.py -f gpu_all_reduce # 输出如下 in distribute mode all_reduce_multigpu [tensor([4.], device='cuda:2'), tensor([4.], device='cuda:3')] Rank 1 has data tensor[0]: tensor([8.], device='cuda:2') , tensor[1]: tensor([4.], device='cuda:3') 这里是通过本地机子上的回送地址进行模拟，结果是分别在不同的 terminal 呈现，当然可以用上面的demo，在多台机子上跑，不过需要修改一下 init_process 函数中的 os.environ[‘MASTER_ADDR’] = ‘127.0.0.1’ 为 rank 0 机子的 IP，这里就不演示了。具体 pytorch distributed 工具相关的内容可以参考官方博客\n练习： 如果大概理解了上面的一些集合通信的原语，可以尝试着用上面 pytorch 提供的 send/recv 尝试去实现一下上面的树状 allreduce。\nMPI 更深入的尝试，可以尝试了解一下 mpi 的知识，这个mpi教程 算是写得比较系统的，大家可以参考一下来练习，特别是对底层不是很了解的同学，可以多看看 Running an MPI cluster within a LAN 的部分，实操一下通过 ssh 跑起一个分布式的 demo。集合通信库的基础大概先到这里，如果要深入的可以再去看看 openMPI，和 nccl 的实现。\nHorovod流程分析 下面我会以一个简单的 pytorch horovod 的 demo 尝试去理解一下 horovod 的工作机理，demo 如下（省略了一些不关键的代码段）。为了准确起见，我们是根据 horovod v0.20.3 的版本进行阅读的，如果是其他版本，可能会跟这里的内容有一些出入。\npytorch demo 一般的 horovod 训练程序都会包含以下几个关键步骤：\n1. hvd.init: 对 horovod 2. 初始化。初始化模型，数据集，优化器，初始化不同 node 的模型权重。 3. 使用 hvd.DistributedOptimizer 包装优化器。 4. 进入训练流程，进行优化迭代。 我们会着重介绍第 1 和 4 步，因为主要也是1，4步会跟 c++ 后端进行信息交换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import torch.backends.cudnn as cudnn import torch.nn.functional as F import torch.optim as optim import torch.utils.data.distributed from torchvision import models import horovod.torch as hvd import timeit import numpy as np ... # some argparse hvd.init() # Set up standard model. model = getattr(models, args.model)() optimizer = optim.SGD(model.parameters(), lr=0.01 * lr_scaler) # Horovod: (optional) compression algorithm. compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none # Horovod: wrap optimizer with DistributedOptimizer. optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters(), compression=compression, op=hvd.Adasum if args.use_adasum else hvd.Average) # Horovod: broadcast parameters \u0026 optimizer state. hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) # Set up fixed fake data data = torch.randn(args.batch_size, 3, 224, 224) target = torch.LongTensor(args.batch_size).random_() % 1000 if args.cuda: data, target = data.cuda(), target.cuda() def benchmark_step(): optimizer.zero_grad() output = model(data) loss = F.cross_entropy(output, target) loss.backward() optimizer.step() #... some log configuration img_secs = [] for x in range(args.num_iters): time = timeit.timeit(benchmark_step, number=args.num_batches_per_iter) img_sec = args.batch_size * args.num_batches_per_iter / time img_secs.append(img_sec) # Results ... 然后下图是我对 horovod 整体流程的梳理，把一些不是很关键的部分隐藏了，可能有一些细节的地方和实现有出入，不过我待会会有详细的说明。这里先解释一下，下面几个大的部分:\nmain.py： 表示训练脚本，一般是 使用 horovod 提供的函数跟特定的训练框架相互合作完成分布式训练（下文称前端） C++ interface：是指 horovod python 函数调用 C++ 的接口 GlobalState：在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问，在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）后面会提到，主要会在下图 backgroundThreadLoop 中完成 globalstate 不同元素初始化，比较重要的有 controller 管理总体通信控制流，tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等）。 BackgroundThreadLoop：是训练过程中的后台线程，主要负责跟其他节点的通信，和处理前端过来的通信需求（request），会轮询调用 RunLoopOnce，不断查看 tensor_queue 中有没有需要通信的tensor，如果有跟其他节点同步更新，然后执行通信操作。 流程分析 下面使用 mpi_controller 进行 allreduce 操作进行分析。\n1.hvd.init()-\u003eInitializeHorovodOnce\n首先，hvd.init() 会通过一系列的调用和配置最终调用 horovod/common/http://operations.cc 下的 InitializeHorovodOnce 函数，这个函数会根据加载的集合通讯库（mpi 或者 gloo）为 globalstate 创建对应的 controller，然后使用 BackgroundThreadLoop 启动一个后台线程。\nhorovod/common/http://operations.cc #628\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 void InitializeHorovodOnce(const int* ranks, int nranks) { // ... some envParse #if HAVE_MPI // Enable mpi is it's used either i[n cpu data transfer or controller if (horovod_global.cpu_operation == LibType::MPI || horovod_global.control_operation == LibType::MPI) { mpi_context.Enable(); } // 创建一个 MPIController 对象 if (horovod_global.control_operation == LibType::MPI){ horovod_global.controller.reset(new MPIController( horovod_global.response_cache, horovod_global.tensor_queue, horovod_global.timeline, horovod_global.parameter_manager, mpi_context)); horovod_global.controller-\u003eSetRanks(ranks, nranks); } #endif #if HAVE_GLOO //... #endif // Reset initialization flag horovod_global.initialization_done = false; // 启动后台线程 horovod_global.background_thread = std::thread( BackgroundThreadLoop, std::ref(horovod_global)); } while (!horovod_global.initialization_done) { std::this_thread::sleep_for(std::chrono::milliseconds(1)); } } 2.BackgroundThreadLoop\nBackgroundThreadLoop 会为 GlobalState 初始化一系列包括初始化 mpi_context， controller的元素，然后轮询调用 RunLoopOnce，还有一些对 RunLoopOnce 结束后的后处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 void BackgroundThreadLoop(HorovodGlobalState\u0026 state) { #if HAVE_MPI // Initialize mpi context auto mpi_ctx_manager = MPIContextManager(); #endif // mpi_context 会根据前端和环境变量传过来的信息，创建 mpi 线程，和一些 mpiOps mpi_context.Initialize(state.controller-\u003eGetRanks(), mpi_ctx_manager); #endif // Initialize controller // 会同步不同 node 的 global_size, local_size, rank, is_coordinator 等信息 state.controller-\u003eInitialize(); // Set background thread affinity parse_and_set_affinity(std::getenv(HOROVOD_THREAD_AFFINITY), local_size, local_rank); #if HAVE_GPU ... // 设置 gpu_context 的 stream 数目等初始化动作 #endif // 下面是设置 parameter_manager 这里为了节省篇幅直接给出，设置的语句， // 原来这里会读取对应的环境变量的，去设置 parameter_manager。 // 后面也会有篇幅介绍 parameter_manager，这里先不展开。 state.parameter_manager.SetTensorFusionThresholdBytes(64 * 1024 * 1024); state.parameter_manager.SetCycleTimeMs(5); state.parameter_manager.SetCacheEnabled(true); state.response_cache.set_capacity( (int)state.parameter_manager.CacheEnabled() * state.cache_capacity); state.parameter_manager.SetHierarchicalAllgather(value, true); state.parameter_manager.SetAutoTuning(true); ... // 其他一些初始化设置 // 设置op_manager，这里主要是注册不同的集合通信库的 ops //（ 如：NCCLAllreduce, MPI_GPUAllgather 等） op_manager.reset(CreateOperationManager(state)); // 初始化完成 state.initialization_done = true; // Iterate until shutdown. try { while (RunLoopOnce(state)); } catch (const std::exception\u0026 ex) { LOG(ERROR) \u003c\u003c \"Horovod background loop uncaught exception: \" \u003c\u003c ex.what(); } ... // 其他一些后处理函数 } 3.Optimizer.step()-\u003eDoAllReduce 这里我们先不急着看 RunLoopOnce 函数，先回到 InitializeHorovodOnce ，因为上面的 initialization_done = True，所以 InitializeHorovodOnce 可以退出了，就是前端的 hvd.init() 可以进行下一步了。这里 main.py 走完前向 loss = model(data,target)，后向逻辑 loss.backward()，调用 optimizer.step() 进行梯度同步。optimizer.step() 会通过一系列的调用和处理（如：compression 等操作）最终会调用 C++ interface 的 DoAllReduce 函数。\nDoAllReduce 函数会调用 EnqueueTensorAllreduce 函数会把需要 reduce 的 tensor 组装成一个Request 往 GlobalState 的 tensor_queue 里面塞。这里注意每个 tensor 会创建对应 TensorTableEntry，用于保存tensor 的权重，message 主要是一些 元信息 metadata。然后就等后台线程去读取这些allreduce 的请求了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Status EnqueueTensorAllreduce(std::shared_ptr\u003cOpContext\u003e context, std::shared_ptr\u003cTensor\u003e tensor, std::shared_ptr\u003cTensor\u003e output, std::shared_ptr\u003cReadyEvent\u003e ready_event, const std::string name, const int device, StatusCallback callback, ReduceOp reduce_op, double prescale_factor, double postscale_factor) { Status status; ... // some config Request message; message.set_request_rank(horovod_global.controller-\u003eGetRank()); message.set_tensor_name(name); message.set_tensor_type(tensor-\u003edtype()); message.set_device(device); message.set_prescale_factor(prescale_factor); message.set_postscale_factor(postscale_factor); if (reduce_op == ReduceOp::ADASUM) { message.set_request_type(Request::ADASUM); } else { message.set_request_type(Request::ALLREDUCE); } for (int i = 0; i \u003c tensor-\u003eshape().dims(); ++i) { message.add_tensor_shape((int64_t)tensor-\u003eshape().dim_size(i)); } TensorTableEntry e; e.tensor_name = name; e.context = context; e.tensor = tensor; e.output = output; e.ready_event = ready_event; e.device = device; e.callback = callback; if (horovod_global.shut_down) { return SHUT_DOWN_ERROR; } status = horovod_global.tensor_queue.AddToTensorQueue(e, message); if (status.ok()) { LOG(TRACE, horovod_global.controller-\u003eGetRank()) \u003c\u003c \"Enqueued \" \u003c\u003c name; } return status; } 4.RunLoopOnce\n回到后台线程 BackgroundThreadLoop，后面会轮询调用 RunLoopOnce。 RunLoopOnce会首先调用 ComputeResponseList 函数，其主要工作是同步不同 worker 之间的需要 allreduce 的 tensors，为后面 allreduce 的执行做好准备。\n？？？为什么会在执行 tensor 的 allreduce 之前执行这样一步工作呢？而不是直接执行 allreduce 呢？我自己的猜测是，因为分布式训练是运行在不同的机子上的，因为 horovod 没有引入类似参数服务器（parameter server）的节点，而是采取 master-worker 的形式 进行 allreduce的。所以 allreduce 的时候必须确保所有的节点都是走到了同一句 allreduce 上，然后传输的 tensors 也要求是一致的，否则传输的 tensors 有可能没有匹配起来就执行allreduce，导致一些不可预知的错误。另外这部分引入了一些提高性能的 tricks，如对之前 reduce 过的 tensor 通过一个 bitmap 进行缓存，每次调用看一下是不是都是之前的 tensor，如果不是再 update 一下，不需要每次都全量更新。？？？（不是很确定）\nComputeResponseList具体的流程是(可以对照上面流程图看):\n从自己进程的 GlobalState 读取 tensor_queue 的信息，如果有新的元素，会通过图中 popMessagesFromQueue pop 出来，然后经过一系列处理缓存到 message_queue_tmp 中。 当 worker 到达了前端 all_reduce 这句的时候，会用 message_queue_tmp 整理成一个 message_list通过流程图中的 SendReadyTensors 函数往主节点( coordinator ) 发送一个请求表明我打算reduce，然后会把准备 reduce 的 tensor 信息通过 message_list 迭代地送过去，最后有一个 Done 的请求 coordinator 会接收通过图中 RecvReadyTensors 这些 requests，然后保存在 ready_to_reduce 中，coordinator 会持续接收这些信息，直到获取的 Done 的数目等于 global_size。 coordinator 会找到所有准备好 reduce 的 tensors，通过 SendFinalTensors 返回一个 response 给所有的 worker，如果信息有误会返回一个 error，发送完成也会发送一个 Done。 worker 会通过 RecvFinalTensors 监听 response 的信息，整理出需要 reduce 的 tensor，当收到 Done，会尝试调用 performation 去进行 reduce 。 coordinator 和 worker 都会把同步的信息整理成一个 responses 的数组给到后面的 PerformOperation 操作。 这里说一下mpi是怎么实现的，就是对应的 coordinator 和 worker 会阻塞地到同一条指令：\nSendReadyTensors 和 RecvReadyTensors 阻塞到 MPI_Gather，SendFinalTensors 和 RecvFinalTensors 到 MPI_Bcast ，可以这样分辨：如果是 coordinator 发送的就是 MPI_Bcast，如果是worker 发送的是 MPI_Gather。通信都是先同步需要通信message的大小 length，再同步message，代码如下：\nhorovod/common/mpi/http://mpi_controller.cc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 void MPIController::SendReadyTensors(RequestList\u0026 message_list) { std::string encoded_message; RequestList::SerializeToString(message_list, encoded_message); int encoded_message_length = (int)encoded_message.length() + 1; // 先 gather 这个 message 的大小 int ret_code = MPI_Gather(\u0026encoded_message_length, 1, MPI_INT, nullptr, 1, MPI_INT, RANK_ZERO, mpi_ctx_.mpi_comm); if (ret_code != MPI_SUCCESS) { throw std::runtime_error(\"MPI_Gather failed, see MPI output for details.\"); } // 再 gather 这个 message ret_code = MPI_Gatherv((void*)encoded_message.c_str(), encoded_message_length, MPI_BYTE, nullptr, nullptr, nullptr, MPI_BYTE, RANK_ZERO, mpi_ctx_.mpi_comm); ... } void MPIController::RecvReadyTensors(std::vector\u003cstd::string\u003e\u0026 ready_to_reduce,std::vector\u003cRequestList\u003e\u0026 ready_list) { MPI_Gather(MPI_IN_PLACE, 1, MPI_INT, recvcounts, 1, MPI_INT, RANK_ZERO, mpi_ctx_.mpi_comm); ... MPI_Gatherv(nullptr, 0, MPI_BYTE, buffer, recvcounts, displcmnts, MPI_BYTE, RANK_ZERO, mpi_ctx_.mpi_comm); ... } void MPIController::RecvFinalTensors(ResponseList\u0026 response_list) { int msg_length; int ret_code = MPI_Bcast(\u0026msg_length, 1, MPI_INT, RANK_ZERO, mpi_ctx_.mpi_comm); if (ret_code != MPI_SUCCESS) { throw std::runtime_error( \"MPI_Broadcast failed, see MPI output for details.\"); } auto buffer = new uint8_t[msg_length]; ret_code = MPI_Bcast(buffer, msg_length, MPI_BYTE, RANK_ZERO, mpi_ctx_.mpi_comm); ... } 5.PerformOperation\n从 ComputeResponseList 继续跑 RunLoopOnce， 不同 node 下面会根据前面 ComputeResponseList 返回的 response_list 对每个 response 轮询调用 PerformOperation 完成对应的 reduce 工作。\nPerformOperation 流程：\nhorovod/common/http://operations.cc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void PerformOperation(Response response, HorovodGlobalState\u0026 state) { std::vector\u003cTensorTableEntry\u003e entries; auto\u0026 timeline = horovod_global.timeline; if (response.response_type() != Response::JOIN) { horovod_global.tensor_queue.GetTensorEntriesFromResponse(response, entries, state.joined); ... // 对数据预处理和 buffer 初始化 Status status; // 执行 all_reduce 等操作 try { status = op_manager-\u003eExecuteOperation(entries, response); } catch (const std::exception\u0026 ex) { status = Status::UnknownError(ex.what()); } ... // 调用 callback 函数 } PerformOperation 会从 horovod_global.tensor_queue 通过函数 GetTensorEntriesFromResponse 取出对应的 TensorEntry 如果还没初始化buffer，调用 horovod_global.fusion_buffer.InitializeBuffer 初始化 然后 status = op_manager-\u003eExecuteOperation(entries, response) 会调用不同的 op-\u003eExecute(entries, response) 执行reduce 运算 下面以 MPIAllreduce::Execute 为例： horovod/common/ops/http://mpi_operations.cc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Status MPIAllreduce::Execute(std::vector\u003cTensorTableEntry\u003e\u0026 entries, const Response\u0026 response) { ... // 一些变量声明 // 把 tensor copy 到 buffer 中 if (entries.size() \u003e 1) { timeline.ActivityStartAll(entries, MEMCPY_IN_FUSION_BUFFER); MemcpyInFusionBuffer(entries, fused_input_data, buffer_data, buffer_len); timeline.ActivityEndAll(entries); } else { fused_input_data = first_entry.tensor-\u003edata(); buffer_data = (void*) first_entry.output-\u003edata(); buffer_len = (size_t) first_entry.output-\u003esize(); } // Do allreduce const void* sendbuf = entries.size() \u003e 1 || fused_input_data == buffer_data ? MPI_IN_PLACE : fused_input_data; int op = MPI_Allreduce(sendbuf, buffer_data, (int) num_elements, mpi_context_-\u003eGetMPIDataType(first_entry.tensor), mpi_context_-\u003eGetMPISumOp(first_entry.tensor-\u003edtype()), mpi_context_-\u003eGetMPICommunicator(Communicator::GLOBAL)); if (op != MPI_SUCCESS) { throw std::runtime_error(\"MPI_Allreduce failed, see MPI output for details.\"); } // Copy memory out of the fusion buffer. // 把 allreduce 后的 tensor copy 会 entries if (entries.size() \u003e 1) { timeline.ActivityStartAll(entries, MEMCPY_OUT_FUSION_BUFFER); MemcpyOutFusionBuffer(buffer_data, entries); timeline.ActivityEndAll(entries); } return Status::OK(); } 然后调用不同 entries 的 callback，这里 callback 一般是给前端作相应的。 6.parameter_manager.update\n完成上述步骤之后，如果设置了 state.parameter_manager.IsAutoTuning()，RunLoopOnce 还会调用相关的逻辑，调整传输的参数，然后返回 BackgroundThreadLoop 重新调用。_重新调用时会睡一定时间再继续_上述第 3 - 5 步的工作。\n其他关键模块 上面只是介绍了 horovod 主流程工作原理，不过 horovod 还有其他一些模块协同主流程工作的，下面会对其中的一些我认为可以值得一说的模块说一下。\nParameter_manager: Parameter_manager 主要是 GlobalState 的一个用于管理一些调节 horovod 性能的参数的管理器，在 BackgroundThreadLoop 中跟其他的 GlobalState 的元素一同初始化，然后会读取下面这些对应的环境变量，然后进行设置。\nHOROVOD_FUSION_THRESHOLD：指传输数据切片的大小，默认是64M，如果切片太大，传输的时候就不能很好地 pipeline 传输，如果太小，一个 tensor 需要传输多次，增加 IO 的 overhead。\nHOROVOD_CYCLE_TIME：指 RunLoopOnce 的睡眠时长，默认是 5ms，我自己的猜测（还没进行验证）比较理想的睡眠时间应该是 RunLoopOnce 其余逻辑处理的时间 + HOROVOD_CYCLE_TIME 刚好等于一次前向传播和后向传播所用的时间，因为睡太久前端会在等 RunLoopOnce 睡醒；如果睡太短，不断地跑一次 RunLoopOnce，tensor_queue 也不会有新的元素，只是白跑。\nHOROVOD_CACHE_CAPACITY：指 cache 的大小，这个可能跟 model 层数参数量相关了。\nHOROVOD_HIERARCHICAL_ALLGATHER：是否使用分层的allgather的方式等\nParameter_manager也提供了对这些参数自动调节的功能。通过Parameter_manager.SetAutoTuning进行设置，设置后会在初始的几个batch尝试不同的参数组合进行通信，后面会收敛到一组最优的参数值。\nMPIContext mpi_context 是在加载 C++ 的代码时候就已经创建了，同时创建的还有其他 context（ nccl_context, gpu_context），主要是维护一些节点上 mpi 通信的必要环境信息和设置，如：\n3 个 MPI communicator，mpi_comm，local_comm，cross_comm 分别负责 horovod mpi 传输，节点内传输，和节点间分层传输（主要用于 hierarchical allreduce）。 mpi_float16_t: horovod 主要以 float16 传输。 mpi_float16_sum: float16 对应的sum 操作。 在 horovod 使用 mpi 的时候，都会使用上面的 communicator 进行数据传输。\nTensorflow2 TensorFlow2 前端对 horovod 的调用跟 pytorch 类似，只是因为 tensorflow 2 是通过 tape 等级制记录梯度的, 所以会有一些不同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 hvd.init() # Set up standard model. model = getattr(applications, args.model)(weights=None) opt = tf.optimizers.SGD(0.01) data = tf.random.uniform([args.batch_size, 224, 224, 3]) target = tf.random.uniform([args.batch_size, 1], minval=0, maxval=999, dtype=tf.int64) @tf.function def benchmark_step(first_batch): # Horovod: (optional) compression algorithm. compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none # Horovod: use DistributedGradientTape with tf.GradientTape() as tape: probs = model(data, training=True) loss = tf.losses.sparse_categorical_crossentropy(target, probs) # Horovod: add Horovod Distributed GradientTape. tape = hvd.DistributedGradientTape(tape, compression=compression) gradients = tape.gradient(loss, model.trainable_variables) opt.apply_gradients(zip(gradients, model.trainable_variables)) if first_batch: hvd.broadcast_variables(model.variables, root_rank=0) hvd.broadcast_variables(opt.variables(), root_rank=0) for x in range(args.num_iters): benchmark_step(first_batch=False) with tf.GradientTape() as tape这一句会调用 horovod/tensorflow/__init__.py 中_DistributedGradientTape 下 init 函数注册 allreduce 的句柄（handle） 然后调用 gradients = tape.gradient(loss, model.trainable_variables) 会调用一系列的跳转最后会调用 tensorflow/mpi_ops.py 下的 _allreduce ，进而调用 `MPI_LIB.horovod_allreduce MPI_LIB.horovod_allreduce 在 horovod/tensorflow/http://mpi_ops.cc 中被 HorovodAllreduceOp 所注册，根据 TensorFlow 的 ops流程，会调用 ops.ComputeAsync，到这里会跟 pytorch 类似会调用 EnqueueTensorAllreduce 把对应的 tensor 和 ops 送到 GlobalState 的 tensor_queue 中。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class HorovodAllreduceOp : public AsyncOpKernel { public: explicit HorovodAllreduceOp(OpKernelConstruction* context) : AsyncOpKernel(context) { OP_REQUIRES_OK(context, context-\u003eGetAttr(\"reduce_op\", \u0026reduce_op_)); OP_REQUIRES_OK(context, context-\u003eGetAttr(\"prescale_factor\", \u0026prescale_factor_)); OP_REQUIRES_OK(context, context-\u003eGetAttr(\"postscale_factor\", \u0026postscale_factor_)); OP_REQUIRES_OK(context, context-\u003eGetAttr(\"ignore_name_scope\", \u0026ignore_name_scope_)); } void ComputeAsync(OpKernelContext* context, DoneCallback done) override { OP_REQUIRES_OK_ASYNC(context, ConvertStatus(common::CheckInitialized()), done); ... // 一些变量验证，初始化 auto enqueue_result = EnqueueTensorAllreduce( hvd_context, hvd_tensor, hvd_output, ready_event, node_name, device, [context, done](const common::Status\u0026 status) { context-\u003eSetStatus(ConvertStatus(status)); done(); }, reduce_op, (double) prescale_factor_, (double) postscale_factor_); OP_REQUIRES_OK_ASYNC(context, ConvertStatus(enqueue_result), done); } private: int reduce_op_; // Using float since TF does not support double OP attributes float prescale_factor_; float postscale_factor_; bool ignore_name_scope_; }; 总结 horovod 的流程分析大概就是这样，没有特别复杂，代码的阅读体验也是比较好的，在主流程的关键函数都有比较清晰的注释。对于第三方开发者来说，horovod 本身已经用了很多提高性能的 tricks，可以 custom 优化的地方不多，一些可以动的参数，也已经提供了autotuning，直接使用就可以得到很好的性能。如果尝试优化，可能要从传输上着手，如 BytePS 会尝试使用不同的网络拓扑引入一些 PS 节点提高带宽等，如果有时间我也会聊一下这个。另外上面的分析也有很多是我自己阅读代码时候的一些思考可能不一定准确，如果有不准确或者模糊的地方，也希望大家可以多多斧正。\nReferences: [1]. https://zhuanlan.zhihu.com/p/332825987 [2]. https://zhuanlan.zhihu.com/p/158584571 [3]. https://zhuanlan.zhihu.com/p/79030485 [4]. https://github.com/zjykzj/pytorch-distributed [5]. MPI教程 https://blog.csdn.net/qq_47058489/article/details/125980505\nhttps://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1\u0026utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup\u0026depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup\u0026utm_relevant_index=1\n[5.] ubuntu20.04 + docker + horovod\nHorovod and Distributed Training ",
  "wordCount" : "7975",
  "inLanguage": "en",
  "datePublished": "2022-07-27T17:31:57+08:00",
  "dateModified": "2022-07-27T17:31:57+08:00",
  "author":[{
    "@type": "Person",
    "name": "Jian"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jianye0428.github.io/en/posts/notes/2022-07-27_horovod_and_openmpi/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jian's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jianye0428.github.io/favicon/jian_icon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jianye0428.github.io/en/" accesskey="h" title="Jian&#39;s Blog (Alt + H)">
                <img src="https://jianye0428.github.io/favicon/jian_icon.png" alt="logo" aria-label="logo"
                    height="30">Jian&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://jianye0428.github.io/cn/" title="Chinese"
                            aria-label="Chinese">Chinese</a>
                    </li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jianye0428.github.io/en/myresume/" title="My Resume">
                    <span>My Resume</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/tags/" title="🔖Tags">
                    <span>🔖Tags</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/archives" title="🙋🏻‍♂️Archive">
                    <span>🙋🏻‍♂️Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/search/" title="🔍Search (Alt &#43; /)" accesskey=/>
                    <span>🔍Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jianye0428.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://jianye0428.github.io/en/posts/">Posts</a></div>
    <h1 class="post-title">
      [Distributed Training] Horovod_and_Openmpi
    </h1>
    <div class="post-meta"><span title='2022-07-27 17:31:57 +0800 CST'>2022-07-27</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;Jian&nbsp;|&nbsp;<a href="https://github.com/jianye0428/myblog/tree/main/content/posts/notes/2022-07-27_Horovod_and_Openmpi.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul><ul>
                    <li>
                        <a href="#horovod-%e4%bb%8b%e7%bb%8d" aria-label="Horovod 介绍">Horovod 介绍</a></li>
                    <li>
                        <a href="#%e9%9b%86%e5%90%88%e9%80%9a%e4%bf%a1%e5%ba%93" aria-label="集合通信库">集合通信库</a><ul>
                            
                    <li>
                        <a href="#%e7%82%b9%e5%af%b9%e7%82%b9%e9%80%9a%e4%bf%a1-point-to-point-communication" aria-label="点对点通信: Point-to-Point Communication">点对点通信: Point-to-Point Communication</a></li>
                    <li>
                        <a href="#%e9%9b%86%e5%90%88%e9%80%9a%e4%bf%a1" aria-label="集合通信">集合通信</a></li>
                    <li>
                        <a href="#%e5%ae%9e%e8%b7%b5" aria-label="实践:">实践:</a><ul>
                            
                    <li>
                        <a href="#pytorchdistributed" aria-label="pytorch.distributed">pytorch.distributed</a></li></ul>
                    </li>
                    <li>
                        <a href="#mpi" aria-label="MPI">MPI</a></li></ul>
                    </li>
                    <li>
                        <a href="#horovod%e6%b5%81%e7%a8%8b%e5%88%86%e6%9e%90" aria-label="Horovod流程分析">Horovod流程分析</a><ul>
                            
                    <li>
                        <a href="#pytorch-demo" aria-label="pytorch demo">pytorch demo</a></li>
                    <li>
                        <a href="#%e6%b5%81%e7%a8%8b%e5%88%86%e6%9e%90" aria-label="流程分析">流程分析</a></li>
                    <li>
                        <a href="#%e5%85%b6%e4%bb%96%e5%85%b3%e9%94%ae%e6%a8%a1%e5%9d%97" aria-label="其他关键模块">其他关键模块</a></li>
                    <li>
                        <a href="#mpicontext" aria-label="MPIContext">MPIContext</a></li>
                    <li>
                        <a href="#tensorflow2" aria-label="Tensorflow2">Tensorflow2</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e6%80%bb%e7%bb%93" aria-label="总结">总结</a></li></ul>
                        
                    <li>
                        <a href="#horovod-and-distributed-training" aria-label="Horovod and Distributed Training">Horovod and Distributed Training</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="horovod-介绍">Horovod 介绍<a hidden class="anchor" aria-hidden="true" href="#horovod-介绍">#</a></h2>
<p>Horovod 是 Uber 开源的深度学习工具，它的发展吸取了Facebook &ldquo;Training ImageNet In 1 Hour&rdquo; 与百度 &ldquo;Ring Allreduce&rdquo; 的优点，在保证分布式训练性能的同时，兼顾了前端的简洁和对不同深度学习框架的支持，使用起来对开发人员比较的友好，算是分布式训练方向的标杆项目了。</p>
<h2 id="集合通信库">集合通信库<a hidden class="anchor" aria-hidden="true" href="#集合通信库">#</a></h2>
<p>集合通信库，这个词可能听起来会比较的陌生，不过如果我再提几个关键字，可能大家多少都会有所耳闻。资历比较老的是 MPI (<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Message_Passing_Interface">Message Passing Interface</a> 及其实现 <a href="https://link.zhihu.com/?target=https%3A//www.open-mpi.org/">OpenMPI</a> 和 <a href="https://link.zhihu.com/?target=https%3A//www.mpich.org/">MPICH</a>，年轻一点的会是 Nvidia 针对其显卡开源的 NCCL，或者是 facebook 开源的 gloo，或者是像华为针对其高性能硬件提供的HCCL，大体上都可以归入到<strong>集合通信库</strong>的类别。他们相同的地方是大体上会遵照 MPI 提供的接口规定，实现了包括<font color=red><em>点对点通信</em></font>（SEND,RECV等），<font color=red><em>集合通信</em></font>（ REDUCE，BROADCAST，ALLREDUCE等）等相关接口，然后根据自己硬件或者是系统的需要，在底层实现上进行了相应的改动，保证接口的稳定和性能。</p>
<h3 id="点对点通信-point-to-point-communication">点对点通信: Point-to-Point Communication<a hidden class="anchor" aria-hidden="true" href="#点对点通信-point-to-point-communication">#</a></h3>
<p><strong>Send/Recv:</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_send_and_recv.jpg" alt="Send_and_recv"  />
</p>
<h3 id="集合通信">集合通信<a hidden class="anchor" aria-hidden="true" href="#集合通信">#</a></h3>
<p><strong>Scatter/Gather</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_scatter_and_gather.jpg" alt="Scatter_and_gather"  />
</p>
<p><strong>reduce/allreduce</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_reduce_and_allreduce.jpg" alt="reduce_and_allreduce"  />
</p>
<p><strong>boradcast/all-gather</strong></p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_broadcast_and_all_gather.jpg" alt="broadcast_and_all_gather"  />
</p>
<p>这里在机器学习训练中使用比较多的是 <strong>all-reduce</strong>，场景类似在不同的 node 上跑不同 batch 的数据，然后更新梯度需要从各个汇总之后平均再回传到各自的 node 中。而这部分，有很多种实现的方式，比较直观和简单的是把所有的梯度都汇总到的某一个 node 上（如下图 node d 所示），然后再把汇总的信息重新分发到不同的 node 上 ，这样可以计算通信量，如下：对于 P 个节点，每个节点消息大小为 M，node d 节点的通信量为 2*(P-1)M，这里假设节点之间互联互通，带宽为B。</p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_Allreduce.jpg" alt="broadcast_and_all_gather"  />
</p>
<p>不过这种情况下，很容易导致 <strong>node d</strong> 会成为性能瓶颈，因为 <strong>node d</strong> 需要跟其他所有 <strong>node</strong> 通信所以它的通信量是其他节点的 <strong>P</strong> 倍。假设节点间的带宽还是一样，<strong>node d</strong> 完成所有通信所需要的时间是 <em><em>2</em>(P-1)M/B</em>*。所以现在很多的集合通信框架不会使用这种方式，更多的是<strong>通过树状或者是环状(ring) 去实现 all-reduce</strong>。</p>
<p>如果只是做成树状的可以做成如下图所示，虽然传递的步数增多了，不过消除了node d 的通信瓶颈，完成所有的通信的时间大概是 <em><em>2log_2N</em>(M/B)</em>*，随着节点数目 P 的增加，树形结构的效果会越来越明显。</p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_Tree_Allreduce.jpg" alt="broadcast_and_all_gather"  />
</p>
<p>业界用得最多一种优化的方式是，每次只传一部分，这部分是百度提出的 ring-allreduce 的方案，具体的介绍可以参考这篇博客<a href="https://link.zhihu.com/?target=https%3A//andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/">Bringing HPC Techniques to Deep Learning</a>，这边就不赘述了。整体上就是每次不会像上面这样整份数据传递，而是一部分一部分传，优化后，所有节点需要传输的数据量的传输 <strong>2(N−1)M/N</strong> 比较平均，所需要的时间可以大概是 <strong>2(N−1)M/(NB)</strong>，horovod 也是基于这种 all-reduce 的形式实现的。</p>
<h3 id="实践">实践:<a hidden class="anchor" aria-hidden="true" href="#实践">#</a></h3>
<h4 id="pytorchdistributed">pytorch.distributed<a hidden class="anchor" aria-hidden="true" href="#pytorchdistributed">#</a></h4>
<p>尝试使用 pytorch 自带的分布式工具包 <a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/distributed.html">torch.distributed</a>，进行一些概念性的尝试。</p>
<p>为了方便尝试，我这里提供了一个简单的 demo，大家如果安装了 gpu 版本的 pytorch &gt;= 1.3，应该都可以尝试下面的例子尝试使用多进程模拟分布式（单机上可以跑）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">argparse</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">Process</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;PyTorch MNIST Example&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-m&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--mode&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;one_device&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute mode, distributed/one_device&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--function&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s1">&#39;p2p&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;function to run (p2p/all_reduce/gpu_all_reduce)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-b&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;--backend&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">default</span><span class="o">=</span><span class="s2">&#34;nccl&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;distribute backend (gloo/nccl)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Initialize the distributed environment. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">fn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data before send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Send the tensor to process 1</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Receive tensor from process 0</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data after send/recv&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">backend</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">backend</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;distributed&#34;</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;RANK&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in distribute mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;RANK&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;in one device mode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">function</span> <span class="o">==</span> <span class="s2">&#34;gpu_all_reduce&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">run_multigpu_allreduce</span><span class="p">,</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">function</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">run</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;gloo&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以简单地运行上面的例子：</p>
<p><strong>send/recv:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">before</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">after</span> <span class="n">send</span><span class="o">/</span><span class="n">recv</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上面是演示的是通过 pytorch 的 multiprocessing 包，模拟一次分布式的 send/recv 过程，这里是 rank0 的进程往 rank1 的进程发送一个 tensor，可以看到 rank 1 tensor 初始化为 0，是接收到 rank 0 的tensor 后变为 1 的。（注意：这里特别设置了 backend 为 gloo 是因为 nccl 不支持 point2point 的传输，具体不同 backend 支持什么形式的原语，参考文档backend部分 ）</p>
<p><strong>all_reduce</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">one</span> <span class="n">device</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对应函数 </span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Simple reduce communication. &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># use rank 0 and rank 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里也很浅白，主要就是对两个进程上的 tensor 进行一次 allreduce，可以看到两个 rank 上的结果都为 2了。</p>
<p><strong>gpu_all_reduce</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下：</span>
</span></span><span class="line"><span class="cl"><span class="c1">#in one device mode</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:2&#39;), tensor([1.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [tensor([1.], device=&#39;cuda:0&#39;), tensor([1.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:2&#39;), tensor([4.], device=&#39;cuda:3&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#all_reduce_multigpu [tensor([4.], device=&#39;cuda:0&#39;), tensor([4.], device=&#39;cuda:1&#39;)]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  0  has data tensor[0]: tensor([8.], device=&#39;cuda:0&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:1&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Rank  1  has data tensor[0]: tensor([8.], device=&#39;cuda:2&#39;) , tensor[1]: tensor([4.], device=&#39;cuda:3&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对应函数 </span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_multigpu_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">+</span> <span class="n">dev_idx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data tensor[0]:&#39;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;, tensor[1]:&#34;</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<blockquote>
<p>all_reduce_multigpu: 相当于将多个gpu内的多进程的值进行相加;
all_reduce: 相当于单个gpu内的多进程的值相加</p>
</blockquote>
</blockquote>
<p>这里演示的是尝试对不同进程下多个 gpu (这里是 4 个) 进行 reduce，具体逻辑就是：</p>
<pre><code>- 对不同的进程分别把 tensor 初始化在不同的 gpu 上，rank0 初始化在 0，1 gpu 上，rank 1 在 2，3上。
- 进行一次 all_reduce_multigpu （这个函数跟 all_reduce 不同，是把不同的 node 上不同的gpu 上的tensor 都放到一个 list 中，进行reduce），这时所有 gpu 上的值都是4，作为对比，我们对 tensor_list[0] 的tensor 做一次all_reduce，得到的结果在 gpu 0,2 上的 tensor 进行了all_reduce 结果是 8，在 gpu 1,3 的 tensor 没有任何变化。
</code></pre>
<p><strong>多terminal尝试</strong></p>
<p>在验证分布式逻辑的时候，其实我们不一定需要多台机子才可以，对一些不涉及网络性能的验证，可以尝试在一台机子上开多个 terminal 进行验证。可以使用上面的例子，在多个 terminal 下跑以下命令。</p>
<p><em>terminal0:</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">0</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">0</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em>terminal1:</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">RANK</span><span class="o">=</span><span class="mi">1</span> <span class="n">python3</span> <span class="n">distribute_test</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">f</span> <span class="n">gpu_all_reduce</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出如下</span>
</span></span><span class="line"><span class="cl"><span class="ow">in</span> <span class="n">distribute</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl"><span class="n">all_reduce_multigpu</span> <span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">Rank</span>  <span class="mi">1</span>  <span class="n">has</span> <span class="n">data</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">8.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:2&#39;</span><span class="p">)</span> <span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:3&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里是通过本地机子上的回送地址进行模拟，结果是分别在不同的 terminal 呈现，当然可以用上面的demo，在多台机子上跑，不过需要修改一下 init_process 函数中的 os.environ[&lsquo;MASTER_ADDR&rsquo;] = &lsquo;127.0.0.1&rsquo; 为 rank 0 机子的 IP，这里就不演示了。具体 pytorch distributed 工具相关的内容可以参考<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/intermediate/dist_tuto.html">官方博客</a></p>
<p>练习： 如果大概理解了上面的一些集合通信的原语，可以尝试着用上面 pytorch 提供的 send/recv 尝试去实现一下上面的树状 allreduce。</p>
<h3 id="mpi">MPI<a hidden class="anchor" aria-hidden="true" href="#mpi">#</a></h3>
<p>更深入的尝试，可以尝试了解一下 mpi 的知识，这个<a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/">mpi</a>教程 算是写得比较系统的，大家可以参考一下来练习，特别是对底层不是很了解的同学，可以多看看 <a href="https://link.zhihu.com/?target=https%3A//mpitutorial.com/tutorials/running-an-mpi-cluster-within-a-lan/">Running an MPI cluster within a LAN</a> 的部分，实操一下通过 ssh 跑起一个分布式的 demo。集合通信库的基础大概先到这里，如果要深入的可以再去看看 <a href="https://link.zhihu.com/?target=https%3A//github.com/open-mpi/ompi/blob/98afc838aa53da88cba339f6dcbab256806a5745/ompi/mca/coll/tuned/coll_tuned_allreduce_decision.c">openMPI</a>，和 <a href="https://github.com/NVIDIA/nccl">nccl</a> 的实现。</p>
<h2 id="horovod流程分析">Horovod流程分析<a hidden class="anchor" aria-hidden="true" href="#horovod流程分析">#</a></h2>
<p>下面我会以一个简单的 pytorch horovod 的 demo 尝试去理解一下 horovod 的工作机理，demo 如下（省略了一些不关键的代码段）。为了准确起见，我们是根据 horovod v0.20.3 的版本进行阅读的，如果是其他版本，可能会跟这里的内容有一些出入。</p>
<h3 id="pytorch-demo">pytorch demo<a hidden class="anchor" aria-hidden="true" href="#pytorch-demo">#</a></h3>
<p>一般的 horovod 训练程序都会包含以下几个关键步骤：</p>
<pre><code>1. hvd.init: 对 horovod 
2. 初始化。初始化模型，数据集，优化器，初始化不同 node 的模型权重。
3. 使用 hvd.DistributedOptimizer 包装优化器。
4. 进入训练流程，进行优化迭代。
</code></pre>
<p>我们会着重介绍第 1 和 4 步，因为主要也是1，4步会跟 c++ 后端进行信息交换。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.backends.cudnn</span> <span class="k">as</span> <span class="nn">cudnn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.torch</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">timeit</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">...</span> <span class="c1"># some argparse</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">lr_scaler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl"><span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: wrap optimizer with DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">named_parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">op</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">Adasum</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">use_adasum</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Average</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: broadcast parameters &amp; optimizer state.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_optimizer_state</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set up fixed fake data</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">()</span> <span class="o">%</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cuda</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1">#... some log configuration</span>
</span></span><span class="line"><span class="cl"><span class="n">img_secs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="n">benchmark_step</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_sec</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">num_batches_per_iter</span> <span class="o">/</span> <span class="n">time</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_secs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img_sec</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Results</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后下图是我对 horovod 整体流程的梳理，把一些不是很关键的部分隐藏了，可能有一些细节的地方和实现有出入，不过我待会会有详细的说明。这里先解释一下，下面几个大的部分:</p>
<ul>
<li>main.py： 表示训练脚本，一般是 使用 horovod 提供的函数跟特定的训练框架相互合作完成分布式训练（下文称前端）</li>
<li>C++ interface：是指 horovod python 函数调用 C++ 的接口</li>
<li>GlobalState：在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问，在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）后面会提到，主要会在下图 backgroundThreadLoop 中完成 globalstate 不同元素初始化，比较重要的有 controller 管理总体通信控制流，tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等）。</li>
<li>BackgroundThreadLoop：是训练过程中的后台线程，主要负责跟其他节点的通信，和处理前端过来的通信需求（request），会轮询调用 RunLoopOnce，不断查看 tensor_queue 中有没有需要通信的tensor，如果有跟其他节点同步更新，然后执行通信操作。</li>
</ul>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-07-27_Horovod_and_Openmpi/Horovod_and_Openmpi_Horovod_process.jpg" alt="Horovod Process"  />
</p>
<h3 id="流程分析">流程分析<a hidden class="anchor" aria-hidden="true" href="#流程分析">#</a></h3>
<p>下面使用 mpi_controller 进行 allreduce 操作进行分析。</p>
<p><strong>1.hvd.init()-&gt;InitializeHorovodOnce</strong></p>
<p>首先，hvd.init() 会通过一系列的调用和配置最终调用 horovod/common/http://operations.cc 下的 InitializeHorovodOnce 函数，这个函数会根据加载的<strong>集合通讯库</strong>（<em>mpi</em> 或者 <em>gloo</em>）为 globalstate 创建对应的 controller，然后使用 BackgroundThreadLoop 启动一个后台线程。</p>
<p>horovod/common/http://operations.cc #628</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">InitializeHorovodOnce</span><span class="p">(</span><span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="kt">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ... some envParse
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Enable mpi is it&#39;s used either i[n cpu data transfer or controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">mpi_context</span><span class="p">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 创建一个 MPIController 对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="o">::</span><span class="n">MPI</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">MPIController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">mpi_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SetRanks</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GLOO
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// Reset initialization flag
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 启动后台线程
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">horovod_global</span><span class="p">.</span><span class="n">background_thread</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">BackgroundThreadLoop</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">initialization_done</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">this_thread</span><span class="o">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>2.BackgroundThreadLoop</strong></p>
<p>BackgroundThreadLoop 会为 GlobalState 初始化一系列包括初始化 mpi_context， controller的元素，然后轮询调用 RunLoopOnce，还有一些对 RunLoopOnce 结束后的后处理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">BackgroundThreadLoop</span><span class="p">(</span><span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize mpi context
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">mpi_ctx_manager</span> <span class="o">=</span> <span class="n">MPIContextManager</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// mpi_context 会根据前端和环境变量传过来的信息，创建 mpi 线程，和一些 mpiOps 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">mpi_context</span><span class="p">.</span><span class="n">Initialize</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRanks</span><span class="p">(),</span> <span class="n">mpi_ctx_manager</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// Initialize controller
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 会同步不同 node 的 global_size, local_size, rank, is_coordinator 等信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">Initialize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set background thread affinity
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">parse_and_set_affinity</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">getenv</span><span class="p">(</span><span class="n">HOROVOD_THREAD_AFFINITY</span><span class="p">),</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GPU
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="p">...</span> <span class="c1">// 设置 gpu_context 的 stream 数目等初始化动作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="c1">// 下面是设置 parameter_manager 这里为了节省篇幅直接给出，设置的语句，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 原来这里会读取对应的环境变量的，去设置 parameter_manager。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 后面也会有篇幅介绍 parameter_manager，这里先不展开。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetTensorFusionThresholdBytes</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCycleTimeMs</span><span class="p">(</span><span class="mi">5</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetCacheEnabled</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">response_cache</span><span class="p">.</span><span class="n">set_capacity</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">CacheEnabled</span><span class="p">()</span> <span class="o">*</span> <span class="n">state</span><span class="p">.</span><span class="n">cache_capacity</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetHierarchicalAllgather</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">SetAutoTuning</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 其他一些初始化设置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 设置op_manager，这里主要是注册不同的集合通信库的 ops
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//（ 如：NCCLAllreduce, MPI_GPUAllgather 等）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_manager</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">CreateOperationManager</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 初始化完成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">state</span><span class="p">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Iterate until shutdown.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="p">(</span><span class="n">RunLoopOnce</span><span class="p">(</span><span class="n">state</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Horovod background loop uncaught exception: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// 其他一些后处理函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>3.Optimizer.step()-&gt;DoAllReduce</strong>
这里我们先不急着看 RunLoopOnce 函数，先回到 InitializeHorovodOnce ，因为上面的 initialization_done = True，所以 InitializeHorovodOnce 可以退出了，就是前端的 hvd.init() 可以进行下一步了。这里 main.py 走完前向 loss = model(data,target)，后向逻辑 loss.backward()，调用 optimizer.step() 进行梯度同步。optimizer.step() 会通过一系列的调用和处理（如：compression 等操作）最终会调用 C++ interface 的 DoAllReduce 函数。</p>
<p><em><strong>DoAllReduce</strong></em> 函数会调用 EnqueueTensorAllreduce 函数会把需要 reduce 的 tensor 组装成一个Request 往 GlobalState 的 tensor_queue 里面塞。这里注意每个 tensor 会创建对应 TensorTableEntry，用于保存tensor 的权重，message 主要是一些 元信息 metadata。然后就等后台线程去读取这些allreduce 的请求了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="nf">EnqueueTensorAllreduce</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">OpContext</span><span class="o">&gt;</span> <span class="n">context</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">output</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyEvent</span><span class="o">&gt;</span> <span class="n">ready_event</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">StatusCallback</span> <span class="n">callback</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="n">ReduceOp</span> <span class="n">reduce_op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">prescale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="kt">double</span> <span class="n">postscale_factor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span> <span class="c1">// some config
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Request</span> <span class="n">message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_request_rank</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_name</span><span class="p">(</span><span class="n">name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_tensor_type</span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_prescale_factor</span><span class="p">(</span><span class="n">prescale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">message</span><span class="p">.</span><span class="n">set_postscale_factor</span><span class="p">(</span><span class="n">postscale_factor</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">reduce_op</span> <span class="o">==</span> <span class="n">ReduceOp</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ADASUM</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">set_request_type</span><span class="p">(</span><span class="n">Request</span><span class="o">::</span><span class="n">ALLREDUCE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dims</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">message</span><span class="p">.</span><span class="n">add_tensor_shape</span><span class="p">((</span><span class="kt">int64_t</span><span class="p">)</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">().</span><span class="n">dim_size</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">TensorTableEntry</span> <span class="n">e</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">ready_event</span> <span class="o">=</span> <span class="n">ready_event</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">e</span><span class="p">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="p">.</span><span class="n">shut_down</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">SHUT_DOWN_ERROR</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">status</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">AddToTensorQueue</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">LOG</span><span class="p">(</span><span class="n">TRACE</span><span class="p">,</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">GetRank</span><span class="p">())</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Enqueued &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>4.RunLoopOnce</strong></p>
<p>回到后台线程 BackgroundThreadLoop，后面会轮询调用 RunLoopOnce。 RunLoopOnce会首先调用 ComputeResponseList 函数，其主要工作是同步不同 worker 之间的需要 allreduce 的 tensors，为后面 allreduce 的执行做好准备。</p>
<p>？？？为什么会在执行 tensor 的 allreduce 之前执行这样一步工作呢？而不是直接执行 allreduce 呢？我自己的猜测是，因为分布式训练是运行在不同的机子上的，因为 <u>horovod 没有引入类似参数服务器（parameter server）的节点，而是采取 master-worker</u> 的形式 进行 allreduce的。所以 allreduce 的时候必须确保所有的节点都是走到了同一句 allreduce 上，然后传输的 tensors 也要求是一致的，否则传输的 tensors 有可能没有匹配起来就执行allreduce，导致一些不可预知的错误。另外这部分引入了一些提高性能的 tricks，如对之前 reduce 过的 tensor 通过一个 bitmap 进行缓存，每次调用看一下是不是都是之前的 tensor，如果不是再 update 一下，不需要每次都全量更新。？？？（不是很确定）</p>
<p><strong>ComputeResponseList</strong>具体的流程是(可以对照上面流程图看):</p>
<ul>
<li>从自己进程的 GlobalState 读取 tensor_queue 的信息，如果有新的元素，会通过图中 popMessagesFromQueue pop 出来，然后经过一系列处理缓存到 message_queue_tmp 中。</li>
<li>当 worker 到达了前端 all_reduce 这句的时候，会用 message_queue_tmp 整理成一个 message_list通过流程图中的 SendReadyTensors 函数往主节点( coordinator ) 发送一个请求表明我打算reduce，然后会把准备 reduce 的 tensor 信息通过 message_list 迭代地送过去，最后有一个 Done 的请求</li>
<li>coordinator 会接收通过图中 RecvReadyTensors 这些 requests，然后保存在 ready_to_reduce 中，coordinator 会持续接收这些信息，直到获取的 Done 的数目等于 global_size。</li>
<li>coordinator 会找到所有准备好 reduce 的 tensors，通过 SendFinalTensors 返回一个 response 给所有的 worker，如果信息有误会返回一个 error，发送完成也会发送一个 Done。</li>
<li>worker 会通过 RecvFinalTensors 监听 response 的信息，整理出需要 reduce 的 tensor，当收到 Done，会尝试调用 performation 去进行 reduce 。</li>
<li>coordinator 和 worker 都会把同步的信息整理成一个 responses 的数组给到后面的 PerformOperation 操作。</li>
</ul>
<p>这里说一下mpi是怎么实现的，就是<u>对应的 coordinator 和 worker 会阻塞地到同一条指令</u>：</p>
<p>SendReadyTensors 和 RecvReadyTensors 阻塞到 MPI_Gather，SendFinalTensors 和 RecvFinalTensors 到 MPI_Bcast ，可以这样分辨：<font color=red><em>如果是 coordinator 发送的就是 MPI_Bcast，如果是worker 发送的是 MPI_Gather</font></em>。通信都是先同步需要通信message的大小 length，再同步message，代码如下：</p>
<p>horovod/common/mpi/http://mpi_controller.cc</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">SendReadyTensors</span><span class="p">(</span><span class="n">RequestList</span><span class="o">&amp;</span> <span class="n">message_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">encoded_message</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">RequestList</span><span class="o">::</span><span class="n">SerializeToString</span><span class="p">(</span><span class="n">message_list</span><span class="p">,</span> <span class="n">encoded_message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">encoded_message_length</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">length</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 先 gather 这个 message 的大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gather</span><span class="p">(</span><span class="o">&amp;</span><span class="n">encoded_message_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Gather failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 再 gather 这个 message
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ret_code</span> <span class="o">=</span> <span class="n">MPI_Gatherv</span><span class="p">((</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">encoded_message</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="n">encoded_message_length</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvReadyTensors</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;&amp;</span> <span class="n">ready_to_reduce</span><span class="p">,</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">RequestList</span><span class="o">&gt;&amp;</span> <span class="n">ready_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gather</span><span class="p">(</span><span class="n">MPI_IN_PLACE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Gatherv</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="n">recvcounts</span><span class="p">,</span> <span class="n">displcmnts</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MPIController</span><span class="o">::</span><span class="n">RecvFinalTensors</span><span class="p">(</span><span class="n">ResponseList</span><span class="o">&amp;</span> <span class="n">response_list</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">msg_length</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="o">&amp;</span><span class="n">msg_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">ret_code</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s">&#34;MPI_Broadcast failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">buffer</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">uint8_t</span><span class="p">[</span><span class="n">msg_length</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">  <span class="n">ret_code</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">MPI_Bcast</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">msg_length</span><span class="p">,</span> <span class="n">MPI_BYTE</span><span class="p">,</span> <span class="n">RANK_ZERO</span><span class="p">,</span> <span class="n">mpi_ctx_</span><span class="p">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>5.PerformOperation</strong></p>
<p>从 ComputeResponseList 继续跑 RunLoopOnce， 不同 node 下面会根据前面 ComputeResponseList 返回的 response_list 对每个 response 轮询调用 PerformOperation 完成对应的 reduce 工作。</p>
<p>PerformOperation 流程：</p>
<p><code>horovod/common/http://operations.cc</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">PerformOperation</span><span class="p">(</span><span class="n">Response</span> <span class="n">response</span><span class="p">,</span> <span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;</span> <span class="n">entries</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">timeline</span> <span class="o">=</span> <span class="n">horovod_global</span><span class="p">.</span><span class="n">timeline</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">response_type</span><span class="p">()</span> <span class="o">!=</span> <span class="n">Response</span><span class="o">::</span><span class="n">JOIN</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="p">.</span><span class="n">tensor_queue</span><span class="p">.</span><span class="n">GetTensorEntriesFromResponse</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">entries</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                             <span class="n">state</span><span class="p">.</span><span class="n">joined</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 对数据预处理和 buffer 初始化
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">Status</span> <span class="n">status</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 执行 all_reduce 等操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">try</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">op_manager</span><span class="o">-&gt;</span><span class="n">ExecuteOperation</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">response</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">status</span> <span class="o">=</span> <span class="n">Status</span><span class="o">::</span><span class="n">UnknownError</span><span class="p">(</span><span class="n">ex</span><span class="p">.</span><span class="n">what</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 调用 callback 函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>PerformOperation 会从 horovod_global.tensor_queue 通过函数 <code>GetTensorEntriesFromResponse</code> 取出对应的 TensorEntry</li>
<li>如果还没初始化buffer，调用 horovod_global.fusion_buffer.InitializeBuffer 初始化</li>
<li>然后 status = op_manager-&gt;ExecuteOperation(entries, response) 会调用不同的 op-&gt;Execute(entries, response) 执行reduce 运算</li>
</ul>
<p>下面以 <strong>MPIAllreduce::Execute</strong> 为例：
<code>horovod/common/ops/http://mpi_operations.cc</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">Status</span> <span class="n">MPIAllreduce</span><span class="o">::</span><span class="n">Execute</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TensorTableEntry</span><span class="o">&gt;&amp;</span> <span class="n">entries</span><span class="p">,</span> <span class="k">const</span> <span class="n">Response</span><span class="o">&amp;</span> <span class="n">response</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span> <span class="c1">// 一些变量声明
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 把 tensor copy 到 buffer 中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_IN_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyInFusionBuffer</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">fused_input_data</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span> <span class="n">buffer_len</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">fused_input_data</span> <span class="o">=</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_data</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">buffer_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">size_t</span><span class="p">)</span> <span class="n">first_entry</span><span class="p">.</span><span class="n">output</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Do allreduce
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">sendbuf</span> <span class="o">=</span> <span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">fused_input_data</span> <span class="o">==</span> <span class="n">buffer_data</span>
</span></span><span class="line"><span class="cl">                        <span class="o">?</span> <span class="nl">MPI_IN_PLACE</span> <span class="p">:</span> <span class="n">fused_input_data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">op</span> <span class="o">=</span> <span class="n">MPI_Allreduce</span><span class="p">(</span><span class="n">sendbuf</span><span class="p">,</span> <span class="n">buffer_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="n">num_elements</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPIDataType</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPISumOp</span><span class="p">(</span><span class="n">first_entry</span><span class="p">.</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mpi_context_</span><span class="o">-&gt;</span><span class="n">GetMPICommunicator</span><span class="p">(</span><span class="n">Communicator</span><span class="o">::</span><span class="n">GLOBAL</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">op</span> <span class="o">!=</span> <span class="n">MPI_SUCCESS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&#34;MPI_Allreduce failed, see MPI output for details.&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Copy memory out of the fusion buffer.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 把 allreduce 后的 tensor copy 会 entries
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">entries</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityStartAll</span><span class="p">(</span><span class="n">entries</span><span class="p">,</span> <span class="n">MEMCPY_OUT_FUSION_BUFFER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">MemcpyOutFusionBuffer</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">,</span> <span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">timeline</span><span class="p">.</span><span class="n">ActivityEndAll</span><span class="p">(</span><span class="n">entries</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>然后调用不同 entries 的 callback，这里 callback 一般是给前端作相应的。</li>
</ul>
<p><strong>6.parameter_manager.update</strong></p>
<p>完成上述步骤之后，如果设置了 state.parameter_manager.IsAutoTuning()，RunLoopOnce 还会调用相关的逻辑，调整传输的参数，然后返回 BackgroundThreadLoop 重新调用。_重新调用时会睡一定时间再继续_上述第 3 - 5 步的工作。</p>
<h3 id="其他关键模块">其他关键模块<a hidden class="anchor" aria-hidden="true" href="#其他关键模块">#</a></h3>
<p>上面只是介绍了 horovod 主流程工作原理，不过 horovod 还有其他一些模块协同主流程工作的，下面会对其中的一些我认为可以值得一说的模块说一下。</p>
<p><strong>Parameter_manager:</strong> Parameter_manager 主要是 GlobalState 的一个用于管理一些调节 horovod 性能的参数的管理器，在 BackgroundThreadLoop 中跟其他的 GlobalState 的元素一同初始化，然后会读取下面这些对应的环境变量，然后进行设置。</p>
<p><strong>HOROVOD_FUSION_THRESHOLD</strong>：指传输数据切片的大小，默认是64M，如果切片太大，传输的时候就不能很好地 pipeline 传输，如果太小，一个 tensor 需要传输多次，增加 IO 的 overhead。</p>
<p><strong>HOROVOD_CYCLE_TIME</strong>：指 <u>RunLoopOnce 的睡眠时长</u>，默认是 <strong>5ms</strong>，我自己的猜测（还没进行验证）比较理想的睡眠时间应该是 RunLoopOnce 其余逻辑处理的时间 + HOROVOD_CYCLE_TIME 刚好等于一次前向传播和后向传播所用的时间，因为睡太久前端会在等 RunLoopOnce 睡醒；如果睡太短，不断地跑一次 RunLoopOnce，tensor_queue 也不会有新的元素，只是白跑。</p>
<p><strong>HOROVOD_CACHE_CAPACITY</strong>：指 cache 的大小，这个可能跟 model 层数参数量相关了。</p>
<p><strong>HOROVOD_HIERARCHICAL_ALLGATHER</strong>：是否使用分层的allgather的方式等</p>
<p>Parameter_manager也提供了对这些参数自动调节的功能。通过Parameter_manager.SetAutoTuning进行设置，设置后会在初始的几个batch尝试不同的参数组合进行通信，后面会收敛到一组最优的参数值。</p>
<h3 id="mpicontext">MPIContext<a hidden class="anchor" aria-hidden="true" href="#mpicontext">#</a></h3>
<p>mpi_context 是在加载 C++ 的代码时候就已经创建了，同时创建的还有其他 context（ nccl_context, gpu_context），主要是维护一些节点上 mpi 通信的必要环境信息和设置，如：</p>
<ul>
<li>3 个 MPI communicator，mpi_comm，local_comm，cross_comm 分别负责 horovod mpi 传输，节点内传输，和节点间分层传输（主要用于 hierarchical allreduce）。</li>
<li>mpi_float16_t: horovod 主要以 float16 传输。</li>
<li>mpi_float16_sum: float16 对应的sum 操作。</li>
</ul>
<p>在 horovod 使用 mpi 的时候，都会使用上面的 communicator 进行数据传输。</p>
<h3 id="tensorflow2">Tensorflow2<a hidden class="anchor" aria-hidden="true" href="#tensorflow2">#</a></h3>
<p>TensorFlow2 前端对 horovod 的调用跟 pytorch 类似，只是因为 tensorflow 2 是通过 tape 等级制记录梯度的, 所以会有一些不同。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set up standard model.</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">applications</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">999</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tf.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: (optional) compression algorithm.</span>
</span></span><span class="line"><span class="cl">    <span class="n">compression</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">fp16</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_allreduce</span> <span class="k">else</span> <span class="n">hvd</span><span class="o">.</span><span class="n">Compression</span><span class="o">.</span><span class="n">none</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: use DistributedGradientTape</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Horovod: add Horovod Distributed GradientTape.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tape</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedGradientTape</span><span class="p">(</span><span class="n">tape</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="n">compression</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_iters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">benchmark_step</span><span class="p">(</span><span class="n">first_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>with tf.GradientTape() as tape</code>这一句会调用 <code>horovod/tensorflow/__init__.py</code> 中<code>_DistributedGradientTape</code> 下 <strong>init</strong> 函数注册 allreduce 的句柄（handle）</li>
<li>然后调用 <code>gradients = tape.gradient(loss, model.trainable_variables)</code> 会调用一系列的跳转最后会调用 <code>tensorflow/mpi_ops.py</code> 下的 _allreduce ，进而调用 `MPI_LIB.horovod_allreduce</li>
<li><code>MPI_LIB.horovod_allreduce</code> 在 <code>horovod/tensorflow/http://mpi_ops.cc</code> 中被 <code>HorovodAllreduceOp</code> 所注册，根据 TensorFlow 的 ops流程，会调用 <code>ops.ComputeAsync</code>，到这里会跟 pytorch 类似会调用 <code>EnqueueTensorAllreduce</code> 把对应的 tensor 和 ops 送到 GlobalState 的 tensor_queue 中。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">HorovodAllreduceOp</span> <span class="p">:</span> <span class="n">public</span> <span class="n">AsyncOpKernel</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="n">public</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">explicit</span> <span class="n">HorovodAllreduceOp</span><span class="p">(</span><span class="n">OpKernelConstruction</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">:</span> <span class="n">AsyncOpKernel</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;reduce_op&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">reduce_op_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;prescale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">prescale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;postscale_factor&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">postscale_factor_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="p">(</span><span class="s2">&#34;ignore_name_scope&#34;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ignore_name_scope_</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">void</span> <span class="n">ComputeAsync</span><span class="p">(</span><span class="n">OpKernelContext</span><span class="o">*</span> <span class="n">context</span><span class="p">,</span> <span class="n">DoneCallback</span> <span class="n">done</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">common</span><span class="p">::</span><span class="n">CheckInitialized</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span> <span class="o">//</span> <span class="n">一些变量验证</span><span class="err">，</span><span class="n">初始化</span>
</span></span><span class="line"><span class="cl">    <span class="n">auto</span> <span class="n">enqueue_result</span> <span class="o">=</span> <span class="n">EnqueueTensorAllreduce</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">hvd_context</span><span class="p">,</span> <span class="n">hvd_tensor</span><span class="p">,</span> <span class="n">hvd_output</span><span class="p">,</span> <span class="n">ready_event</span><span class="p">,</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">context</span><span class="p">,</span> <span class="n">done</span><span class="p">](</span><span class="n">const</span> <span class="n">common</span><span class="p">::</span><span class="n">Status</span><span class="o">&amp;</span> <span class="n">status</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">context</span><span class="o">-&gt;</span><span class="n">SetStatus</span><span class="p">(</span><span class="n">ConvertStatus</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">          <span class="n">done</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">prescale_factor_</span><span class="p">,</span> <span class="p">(</span><span class="n">double</span><span class="p">)</span> <span class="n">postscale_factor_</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">OP_REQUIRES_OK_ASYNC</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">ConvertStatus</span><span class="p">(</span><span class="n">enqueue_result</span><span class="p">),</span> <span class="n">done</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">private</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="nb">int</span> <span class="n">reduce_op_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Using</span> <span class="nb">float</span> <span class="n">since</span> <span class="n">TF</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">support</span> <span class="n">double</span> <span class="n">OP</span> <span class="n">attributes</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">prescale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">float</span> <span class="n">postscale_factor_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="nb">bool</span> <span class="n">ignore_name_scope_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h2>
<p>horovod 的流程分析大概就是这样，没有特别复杂，代码的阅读体验也是比较好的，在主流程的关键函数都有比较清晰的注释。对于第三方开发者来说，horovod 本身已经用了很多提高性能的 tricks，可以 custom 优化的地方不多，一些可以动的参数，也已经提供了autotuning，直接使用就可以得到很好的性能。如果尝试优化，可能要从传输上着手，如 BytePS 会尝试使用不同的网络拓扑引入一些 PS 节点提高带宽等，如果有时间我也会聊一下这个。另外上面的分析也有很多是我自己阅读代码时候的一些思考可能不一定准确，如果有不准确或者模糊的地方，也希望大家可以多多斧正。</p>
<p>References:
[1]. <a href="https://zhuanlan.zhihu.com/p/332825987">https://zhuanlan.zhihu.com/p/332825987</a>
[2]. <a href="https://zhuanlan.zhihu.com/p/158584571">https://zhuanlan.zhihu.com/p/158584571</a>
[3]. <a href="https://zhuanlan.zhihu.com/p/79030485">https://zhuanlan.zhihu.com/p/79030485</a>
[4]. <a href="https://github.com/zjykzj/pytorch-distributed">https://github.com/zjykzj/pytorch-distributed</a>
[5]. <a href="https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/">MPI教程</a>
<a href="https://blog.csdn.net/qq_47058489/article/details/125980505">https://blog.csdn.net/qq_47058489/article/details/125980505</a></p>
<p><a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1">https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1</a></p>
<p>[5.] <a href="https://blog.csdn.net/weixin_45385568/article/details/121208161?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-121208161-blog-87971642.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=1">ubuntu20.04 + docker + horovod</a></p>
<h1 id="horovod-and-distributed-training">Horovod and Distributed Training<a hidden class="anchor" aria-hidden="true" href="#horovod-and-distributed-training">#</a></h1>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jianye0428.github.io/en/tags/distributed-training/">distributed training</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jianye0428.github.io/en/posts/tech/2022-08-01_c&#43;&#43;_data_structure/">
    <span class="title"><i class="fas fa-angle-double-left"></i> Prev Page</span>
    <br>
    <span>C&#43;&#43; STL (Standard Template Library) Containers</span>
  </a>
  <a class="next" href="https://jianye0428.github.io/en/posts/notes/2022-07-19_softwareinstallation/">
    <span class="title">Next Page <i class="fas fa-angle-double-right"></i></span>
    <br>
    <span>Software Installation Notes</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on twitter"
        href="https://twitter.com/intent/tweet/?text=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f&amp;hashtags=distributedtraining">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f&amp;title=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;summary=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;source=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f&title=%5bDistributed%20Training%5d%20Horovod_and_Openmpi">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on whatsapp"
        href="https://api.whatsapp.com/send?text=%5bDistributed%20Training%5d%20Horovod_and_Openmpi%20-%20https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share [Distributed Training] Horovod_and_Openmpi on telegram"
        href="https://telegram.me/share/url?text=%5bDistributed%20Training%5d%20Horovod_and_Openmpi&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-07-27_horovod_and_openmpi%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>




<footer class="tc-container" id="comment">
    <div class="tc-title"><p class="c-title">Discussion</p></div>
    <div id="tcomments"></div>
</footer>
<script crossorigin="anonymous" src="/js/twikoo.min.b16100b7cf8a61759eab076a122482054e083087aad37c3be1fe2e293934dc34.js" integrity="sha256-sWEAt8&#43;KYXWeqwdqEiSCBU4IMIeq03w74f4uKTk03DQ="></script>
<script>
    twikoo.init({
        envId: 'https://my-repository-pink.vercel.app/',
        el: '#tcomments',
        region: 'ap-shanghai', 
        
        lang: 'zh-CN', 
    });
</script>

</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
