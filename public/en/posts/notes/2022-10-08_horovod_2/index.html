<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 | Jian&#39;s Blog</title>
<meta name="keywords" content="Horovod">
<meta name="description" content="reference: [1].https://www.cnblogs.com/rossiXYZ/p/14856543.html 0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Hor">
<meta name="author" content="Jian">
<link rel="canonical" href="https://jianye0428.github.io/en/posts/notes/2022-10-08_horovod_2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.1ea9c8832138446635789668415e5c75b8a534b191ee749a44f5ab404c9f27c2.css" integrity="sha256-HqnIgyE4RGY1eJZoQV5cdbilNLGR7nSaRPWrQEyfJ8I=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="apple-touch-icon" href="https://jianye0428.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://jianye0428.github.io/favicon/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://jianye0428.github.io/en/posts/notes/2022-10-08_horovod_2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="baidu-site-verification" content="code-9oLyeix0aK" />
<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4a41bf85d719f0e8c3165fc76904f546";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>



<script defer crossorigin="anonymous" src="/js/katex.min.8f5024e83d2055dd60e021751066111b0057e230db34911dd56242d67f0a4c86.js" integrity="sha256-j1Ak6D0gVd1g4CF1EGYRGwBX4jDbNJEd1WJC1n8KTIY="></script>


<script defer crossorigin="anonymous" src="/js/auto-render.min.b09accad850e4e87b8a2fc8b93fae790def79172b68de72fd777958c52e566ad.js" integrity="sha256-sJrMrYUOToe4ovyLk/rnkN73kXK2jecv13eVjFLlZq0="></script>

<script>
    
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });

    
    window.WebFontConfig = {
        custom: {
            families: ['KaTeX_AMS', 'KaTeX_Caligraphic:n4,n7', 'KaTeX_Fraktur:n4,n7',
            'KaTeX_Main:n4,n7,i4,i7', 'KaTeX_Math:i4,i7', 'KaTeX_Script',
            'KaTeX_SansSerif:n4,n7,i4', 'KaTeX_Size1', 'KaTeX_Size2', 'KaTeX_Size3',
            'KaTeX_Size4', 'KaTeX_Typewriter'],
        },
    };
</script>


<script defer crossorigin="anonymous" src="/js/webfontloader.min.min.d1c6c39d18e2decb5c99dc9efc579098ab37b9654725df3f9c0737bc2dd00760.js" integrity="sha256-0cbDnRji3stcmdye/FeQmKs3uWVHJd8/nAc3vC3QB2A="></script>


 

<script async src="https://www.googletagmanager.com/gtag/js?id=G-C6GDZ56F4S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-C6GDZ56F4S', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入" />
<meta property="og:description" content="reference: [1].https://www.cnblogs.com/rossiXYZ/p/14856543.html 0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Hor" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jianye0428.github.io/en/posts/notes/2022-10-08_horovod_2/" /><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-08T17:25:41&#43;08:00" />
<meta property="article:modified_time" content="2022-10-08T17:25:41&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/>

<meta name="twitter:title" content="深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入"/>
<meta name="twitter:description" content="reference: [1].https://www.cnblogs.com/rossiXYZ/p/14856543.html 0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Hor"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jianye0428.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入",
      "item": "https://jianye0428.github.io/en/posts/notes/2022-10-08_horovod_2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入",
  "name": "深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入",
  "description": "reference: [1].https://www.cnblogs.com/rossiXYZ/p/14856543.html 0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。 本系列将通过源码分析来带领大家了解 Hor",
  "keywords": [
    "Horovod"
  ],
  "articleBody": "reference: [1].https://www.cnblogs.com/rossiXYZ/p/14856543.html\n0 摘要 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。\n本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。\n前一篇参见如下：\n深度学习分布式训练框架 Horovod[1] – 基础知识\n1 Horovod 简介 Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，支持TensorFlow，Keras，PyTorch和MXNet。Horovod 的名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与分布式 TensorFlow 流程使用 Horovod 互相通信的场景很像。\n因为各个机器学习框架对于底层集合通信库（ nccl，openmpi，gloo 等等）的利用水平可能各不相同，使得他们无法充分利用这些底层集合通信库的威力。因而，hovorod 就整合这些框架，提供一个易用高效的解决方案。\nUber的工程师就是根据FaceBook的一篇paper：“Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour”和百度的一篇“Bringing HPC Techniques to Deep Learning” 改进并发布了开源框架Horovod。\nHorovod 相比于百度的工作，并无学术上的贡献。但是 Horovod 扎实的工程实现，使得它受到了更多的关注。它最大的优势在于对 RingAllReduce 进行了更高层次的抽象，使其支持多种不同的框架。同时引入了 Nvidia NCCL，对 GPU 更加友好。\nHorovod依赖于Nvidia的 NCCL2 做 All Reduce，依赖于MPI做进程间通信，简化了同步多 GPU 或多节点分布式训练的开发流程。由于使用了NCCL2，Horovod也可以利用以下功能：NVLINK，RDMA，GPUDirectRDMA，自动检测通信拓扑，能够回退到 PCIe 和 TCP/IP 通信。\n我们需要几个问题来引导分析：\nHovorod 怎么进行数据分割？ Hovorod 怎么进行训练代码分发？ Hovorod 启动时候，python 和 C++ 都做了什么？ 如何确保 Hovorod 启动时候步骤一致； 2 Hovorod 机制概述 2.1 Horovod 机制 Horovod使用数据并行化策略在GPU上分配训练。\n在数据并行化中，作业中的每个GPU都会接收其自己的数据批处理的独立切片，即它的“批处理切片”。 每个GPU都使用自己分配到的数据来独立计算，进行梯度更新。\n假如使用两个GPU，批处理大小为32，则第一个GPU将处理前16条记录的正向传播和向后传播，以及第二个GPU处理后16条记录的正向传播和向后传播。然后，这些梯度更新将在GPU之间平均在一起，最后应用于模型。\n每一个迭代的操作方法如下：\n每个 worker 将维护自己的模型权重副本和自己的数据集副本。\n收到执行信号后，每个工作进程都会从数据集中提取一个不相交的批次，并计算该批次的梯度。\nWorkers 使用ring all-reduce算法来同步彼此的梯度，从而在本地所有节点上计算同样的平均梯度。\n将每个设备上的梯度 tensor 切分成长度大致相等的 num_devices 个分片，后续每一次通信都将给下一个邻居发送一个自己的分片（同时从上一个邻居接受一个新分片）。\nScatterReduce 阶段：通过 num_devices - 1 轮通信和相加，在每个 device 上都计算出一个 tensor 分片的和，即每个 device 将有一个块，其中包含所有device 中该块中所有值的总和；具体如下：\nAllGather 阶段：通过 num_devices - 1 轮通信和覆盖，将上个阶段计算出的每个 tensor 分片的和 广播到其他 device；最终所有节点都拥有所有tensor分片和。具体如下： 在每个设备上合并分片，得到梯度和，然后除以 num_devices，得到平均梯度； 每个 worker 将 梯度更新 应用于其模型的本地副本。\n执行下一个batch。\n3 示例代码 3.1 摘要代码 我们此处给出官网示例代码部分摘要，具体分析参见下面代码中的注释。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 import tensorflow as tf import horovod.tensorflow.keras as hvd # Horovod: initialize Horovod. hvd.init() # 初始化 Horovod，启动相关线程和MPI线程 # Horovod: pin GPU to be used to process local rank (one GPU per process) # 依据 local rank 为不同的进程分配不同的GPU gpus = tf.config.experimental.list_physical_devices('GPU') for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) if gpus: tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU') (mnist_images, mnist_labels), _ = \\ tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank()) # 切分数据 dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32), tf.cast(mnist_labels, tf.int64)) ) dataset = dataset.repeat().shuffle(10000).batch(128) mnist_model = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, [3, 3], activation='relu'), ...... tf.keras.layers.Dense(10, activation='softmax') ]) # Horovod: adjust learning rate based on number of GPUs. scaled_lr = 0.001 * hvd.size() # 根据Worker的数量增加学习率的大小 opt = tf.optimizers.Adam(scaled_lr) # Horovod: add Horovod DistributedOptimizer. # 把常规TensorFlow Optimizer通过Horovod包装起来，进而使用 ring-allreduce 来得到平均梯度 opt = hvd.DistributedOptimizer( opt, backward_passes_per_step=1, average_aggregated_gradients=True) # Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow # uses hvd.DistributedOptimizer() to compute gradients. mnist_model.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'], experimental_run_tf_function=False) callbacks = [ hvd.callbacks.BroadcastGlobalVariablesCallback(0), # 广播初始化，将模型的参数从第一个设备传向其他设备，以保证初始化模型参数的一致性 hvd.callbacks.MetricAverageCallback(), hvd.callbacks.LearningRateWarmupCallback(initial_lr=scaled_lr, warmup_epochs=3, verbose=1), ] # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them. # 只有设备0需要保存模型参数作为checkpoint if hvd.rank() == 0: callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5')) # Horovod: write logs on worker 0. verbose = 1 if hvd.rank() == 0 else 0 # Train the model. # Horovod: adjust number of steps based on number of GPUs. mnist_model.fit(dataset, steps_per_epoch=500 // hvd.size(), callbacks=callbacks, epochs=24, verbose=verbose) 3.2 horovodrun Horovod训练脚本未作为Python脚本启动。 例如，您不能使用python train.py运行此脚本。 需要采用特殊的CLI命令 horovodrun 来启动（训练代码 train.py 需要手动拷贝到各个节点上，且目录相同）：\n1 $ horovodrun -np 4 -H localhost:4 python train.py 4 运行逻辑 我们按照顺序梳理，看看在程序初始化过程背后都做了什么。\n4.1 引入python文件 如下代码会引入各种相关python文件。\n1 2 import tensorflow as tf import horovod.tensorflow.keras as hvd 4.2 初始化 in python python 世界的初始化位于 horovod-master/horovod/mxnet/mpi_ops.py\n4.2.1 引入SO库 4.2.1.1 SO库 horovod/tensorflow/mpi_ops.py 之中会引入SO库。 比如 dist-packages/horovod/tensorflow/mpi_lib.cpython-36m-x86_64-linux-gnu.so。\nSO库 就是 horovod 中 C++ 代码编译出来的结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def _load_library(name): \"\"\"Loads a .so file containing the specified operators. \"\"\" filename = resource_loader.get_path_to_datafile(name) library = load_library.load_op_library(filename) return library # Check possible symbol not found error from tensorflow version mismatch try: MPI_LIB = _load_library('mpi_lib' + get_ext_suffix()) except Exception as e: check_installed_version('tensorflow', tf.__version__, e) raise e else: check_installed_version('tensorflow', tf.__version__) 4.2.2.2 SO作用 引入库的作用是获取到 C++ 的函数，并且用 python 封装一下，这样就可以在 python 世界使用 C++代码了。\n由下文可以看出来，python 的 _allreduce 函数就会把功能转发给 C++，由 MPI_LIB.horovod_allreduce 完成。\n1 2 3 4 5 6 7 8 def _allreduce(tensor, name=None, op=Sum, prescale_factor=1.0, postscale_factor=1.0, ignore_name_scope=False): if name is None and not _executing_eagerly(): name = 'HorovodAllreduce_%s' % _normalize_name(tensor.name) return MPI_LIB.horovod_allreduce(tensor, name=name, reduce_op=op, prescale_factor=prescale_factor, postscale_factor=postscale_factor, ignore_name_scope=ignore_name_scope) 4.2.2 初始化配置 我们摘录了主要部分，就是初始化 _HorovodBasics，然后从 _HorovodBasics 内获取各种函数，变量和配置，比如是否编译了mpi，gloo等等.\n1 2 3 4 5 6 7 8 9 10 11 12 13 from horovod.common.basics import HorovodBasics as _HorovodBasics _basics = _HorovodBasics(__file__, 'mpi_lib') # import basic methods init = _basics.init size = _basics.size local_size = _basics.local_size rank = _basics.rank local_rank = _basics.local_rank mpi_built = _basics.mpi_built gloo_enabled = _basics.gloo_enabled ...... 4.2.3 hvd.init() 初始化 首先需要用 hvd.init() 来初始化，horovod 管理的所有状态都会传到 hvd 对象中。\n1 2 # Horovod: initialize Horovod. hvd.init() 此处调用的是 HorovodBasics 中的函数，我们看看做了什么。\n可以看到，这部分会一直深入到 C++世界，调用了大量的 MPI_LIB_CTYPES 函数，所以我们接下来就要进入到 C++的世界看看。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def init(self, comm=None): \"\"\"A function that initializes Horovod. \"\"\" atexit.register(self.shutdown) if not isinstance(comm, list): mpi_built = self.MPI_LIB_CTYPES.horovod_mpi_built() from mpi4py import MPI if MPI._sizeof(MPI.Comm) == ctypes.sizeof(ctypes.c_int): MPI_Comm = ctypes.c_int else: MPI_Comm = ctypes.c_void_p self.MPI_LIB_CTYPES.horovod_init_comm.argtypes = [MPI_Comm] comm_obj = MPI_Comm.from_address(MPI._addressof(comm)) self.MPI_LIB_CTYPES.horovod_init_comm(comm_obj) else: comm_size = len(comm) self.MPI_LIB_CTYPES.horovod_init( (ctypes.c_int * comm_size)(*comm), ctypes.c_int(comm_size)) 目前逻辑如下图：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Import python files + | | v Import C++ SO files | | | v Create _HorovodBasics + | | v hvd.init() + Python | +------------------------------------------+ C++ | | v 4.3 初始化 in C++ 4.3.1 horovod_init_comm 在初始化的时候，Horovod 会：\n调用 MPI_Comm_dup 获取一个 Communicator，这样就有了和 MPI 协调的基础。 然后调用 InitializeHorovodOnce。 1 2 3 4 void horovod_init_comm(MPI_Comm comm) { MPI_Comm_dup(comm, \u0026mpi_context.mpi_comm); InitializeHorovodOnce(nullptr, 0); } 4.3.2 InitializeHorovodOnce InitializeHorovodOnce 是初始化的主要工作，主要是：\n依据是否编译了 mpi 或者 gloo，对各自的 context 进行处理，为 globalstate 创建对应的 controller； 启动了后台线程 BackgroundThreadLoop 用来在各个worker之间协调； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 void horovod_init(const int* ranks, int nranks) { InitializeHorovodOnce(ranks, nranks); } void InitializeHorovodOnce(const int* ranks, int nranks) { // Ensure background thread is only started once. if (!horovod_global.initialize_flag.test_and_set()) { horovod_global.control_operation = ParseControllerOpsFromEnv(); horovod_global.cpu_operation = ParseCPUOpsFromEnv(); #if HAVE_MPI // 依据是否编译了MPI进行处理 // Enable mpi is it's used either in cpu data transfer or controller if (horovod_global.cpu_operation == LibType::MPI || horovod_global.control_operation == LibType::MPI) { mpi_context.Enable(); } if (horovod_global.control_operation == LibType::MPI){ // 创建一个 MPIController 对象 horovod_global.controller.reset(new MPIController( horovod_global.response_cache, horovod_global.tensor_queue, horovod_global.timeline, horovod_global.parameter_manager, horovod_global.group_table, mpi_context)); horovod_global.controller-\u003eSetRanks(ranks, nranks); } #endif #if HAVE_GLOO // 依据是否编译了 GLOO 进行处理 // Enable gloo is it's used either in cpu data transfer or controller if (horovod_global.cpu_operation == LibType::GLOO || horovod_global.control_operation == LibType::GLOO) { gloo_context.Enable(); } if (horovod_global.control_operation == LibType::GLOO) { horovod_global.controller.reset(new GlooController( horovod_global.response_cache, horovod_global.tensor_queue, horovod_global.timeline, horovod_global.parameter_manager, horovod_global.group_table, gloo_context)); } #endif // Reset initialization flag // 启动后台线程 horovod_global.initialization_done = false; horovod_global.background_thread = std::thread( BackgroundThreadLoop, std::ref(horovod_global)); } // Wait to ensure that the background thread has finished initializing MPI. while (!horovod_global.initialization_done) { std::this_thread::sleep_for(std::chrono::milliseconds(1)); } } 4.3.3 HorovodGlobalState 在 C++ 世界，HorovodGlobalState 起到了集中管理各种全局变量的作用。\nHorovodGlobalState 在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问。HorovodGlobalState 在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）。\nHorovod 主要会在backgroundThreadLoop 中完成 HorovodGlobalState 不同元素初始化，比较重要的有：\ncontroller 管理总体通信控制流； tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等)； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // All the Horovod state that must be stored globally per-process. HorovodGlobalState horovod_global; #if HAVE_MPI MPIContext mpi_context; #endif #if HAVE_GLOO GlooContext gloo_context; #endif .... std::unique_ptr\u003cOperationManager\u003e op_manager; HorovodGlobalState 摘要如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 struct HorovodGlobalState { // Background thread running MPI communication. std::thread background_thread; // 后台线程，用来在各个worker之间协调 ParameterManager parameter_manager; // 维护后台总体参数配置 // Encapsulates the fusion buffers, handles resizing and auto-tuning of buffer // size. FusionBufferManager fusion_buffer; // 融合tensor，以便缩减通信开销 std::shared_ptr\u003cController\u003e controller; //管理总体通信控制流 TensorQueue tensor_queue; //处理从前端过来的通信需求（allreduce，broadcast 等） // Pointer to shared buffer for allgather void* shared_buffer = nullptr; // LRU cache of Responses ResponseCache response_cache; // Information on registered groups. GroupTable group_table; ~HorovodGlobalState() { // Make sure that the destructor of the background thread is safe to // call. If a thread is still joinable (not detached or complete) its // destructor cannot be called. if (background_thread.joinable()) { shut_down = true; background_thread.join(); } } }; 目前具体逻辑如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 Import python files + | | v Import C++ SO files | | | v Create _HorovodBasics + | | v hvd.init() + Python | +-------------------------------------------------------------------------------------------------------------+ | c++ | v +-----------------------------+ | HorovodGlobalState | horovod_init_comm | | + +------------------+ | | | | horovod_global +---------\u003e | TensorQueue | | | | | | v | | | background_thread | | mpi_context | | | InitializeHorovodOnce +------------\u003e | | | ParameterManager | + | | | | | | gloo_context | | FusionBufferManager | | | | | | | | | | Controller | v | op_manager | | | background_threa | | | ResponseCache | +------------------+ | | | shared_buffer | +-----------------------------+ 如图：\n至此，horovod 已经初始化完成，用户代码可以使用了。\n4.3 hvd 概念 在用户代码中，接下来是rank概念。\n1 2 3 hvd.local_rank() hvd.rank() 我们介绍下几个相关概念：\nHorovod为设备上的每个GPU启动了该训练脚本的一个副本。local rank就是分配给某一台计算机上每个执行训练的唯一编号（也可以认为是进程号或者GPU设备的ID号），范围是 0 到 n-1，其中 n 是该计算机上GPU设备的数量。 rank 可以认为是代表分布式任务里的一个执行训练的唯一全局编号（用于进程间通讯）。Rank 0 在Horovod中通常具有特殊的意义：它是负责此同步的设备。 在百度的实现中，不同 Rank 的角色是不一样的，Rank 0 会充当 coordinator 的角色。它会协调来自其他 Rank 的 MPI 请求，是一个工程上的考量。这一设计也被后来的 Horovod 采用。 Rank 0 也用来把参数广播到其他进程 \u0026 存储 checkpoint。 world_size：进程总数量，会等到所有world_size个进程就绪之后才会开始训练。 hvd.init 这部分的目的就是让并行进程们可以知道自己被分配的 rank / local rank 等信息，于是后续可以根据 local rank（所在节点上的第几张 GPU 卡） 来设置所需的显存分配。\n4.5 数据处理 接下来是数据处理。\n1 2 3 4 5 dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32), tf.cast(mnist_labels, tf.int64)) ) dataset = dataset.repeat().shuffle(10000).batch(128) 这里有几点需要说明：\n首先，训练的数据需要放置在任何节点都能访问的地方。\n其次，Horovod 需要对数据进行分片处理，需要在不同机器上按Rank进行切分，以保证每个GPU进程训练的数据集是不一样的。\n数据集本体需要出于数据并行性的需求而被拆分为多个分片，Horovod的不同工作节点都将分别读取自己的数据集分片。\n从 PyTorch 示例脚本看得更加清楚。\n1 2 3 4 5 # Horovod: use DistributedSampler to partition the training data. train_sampler = torch.utils.data.distributed.DistributedSampler( train_dataset, num_replicas=hvd.size(), rank=hvd.rank()) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=args.batch_size, sampler=train_sampler, **kwargs) DataLoader的采样器组件从要绘制的数据集中返回可迭代的索引。 PyTorch中的默认采样器是顺序的，返回序列0, 1, 2, …, n 。 Horovod使用其DistributedSampler覆盖了此行为，该DistributedSampler处理跨计算机的数据集分区。 DistributedSampler本身接受两个参数作为输入： hvd.size() (GPU的总数，例如16)和hvd.rank() (从总体列表中分配给该设备的ID，例如0…15)。\nPytorch使用的是数据分布式训练，每个进程实际上是独立加载数据的，所以需要加载相同数据集后用一定的规则根据rank来顺序切割获取不同的数据子集，DistributedSampler就是用来确保dataloader只会load到整个数据集的一个特定子集的做法(实际上不用Pytorch提供的DistributedSampler工具，自己做加载数据后切分word_size个子集按rank顺序拿到子集效果也是一样)。\n同时为了能够按顺序划分数据子集，拿到不同部分数据，所以数据集不能够进行随机打散，所以用了参数 'shuffle': False。\n4.6 广播初始化变量 以下代码完成广播初始化的功能。\n1 hvd.callbacks.BroadcastGlobalVariablesCallback(0) 这句代码保证的是 rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。\n下面就介绍下 Horvod 之中广播的使用。\n4.6.1 广播定义 广播的具体实现是：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BroadcastGlobalVariablesCallbackImpl(object): def __init__(self, backend, root_rank, device='', *args): super(BroadcastGlobalVariablesCallbackImpl, self).__init__(*args) self.backend = backend self.root_rank = root_rank self.device = device self.broadcast_done = False def on_batch_end(self, batch, logs=None): if self.broadcast_done: return with tf.device(self.device): if hvd._executing_eagerly() and hasattr(self.model, 'variables'): # TensorFlow 2.0 or TensorFlow eager hvd.broadcast_variables(self.model.variables, root_rank=self.root_rank) hvd.broadcast_variables(self.model.optimizer.variables(), root_rank=self.root_rank) else: bcast_op = hvd.broadcast_global_variables(self.root_rank) self.backend.get_session().run(bcast_op) self.broadcast_done = True 4.6.2 broadcast_variables broadcast_variables 调用了 _make_broadcast_group_fn 完成功能，可以看到对于 执行图 的每个变量，调用了 broadcast。\n1 2 3 4 5 6 7 8 9 10 def broadcast_variables(variables, root_rank): \"\"\"Broadcasts variables from root rank to all other processes. Arguments: variables: variables for broadcast root_rank: rank of the process from which global variables will be broadcasted to all other processes. \"\"\" broadcast_group = _make_broadcast_group_fn() return broadcast_group(variables, root_rank) 以及\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @_cache def _make_broadcast_group_fn(): if _executing_eagerly(): # Eager mode will parallelize independent control flow def broadcast_group(variables, root_rank): for var in variables: var.assign(broadcast(var, root_rank)) return _make_subgraph(broadcast_group) else: # Graph mode requires an Op def broadcast_group(variables, root_rank): return tf.group(*[var.assign(broadcast(var, root_rank)) for var in variables]) return broadcast_group 4.6.3 调用 MPI broadcast 就是调用了 MPI 函数真正完成了功能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def broadcast(tensor, root_rank, name=None, ignore_name_scope=False): \"\"\"An op which broadcasts the input tensor on root rank to the same input tensor on all other Horovod processes. The broadcast operation is keyed by the name of the op. The tensor type and shape must be the same on all Horovod processes for a given name. The broadcast will not start until all processes are ready to send and receive the tensor. Returns: A tensor of the same shape and type as `tensor`, with the value broadcasted from root rank. \"\"\" if name is None and not _executing_eagerly(): name = 'HorovodBroadcast_%s' % _normalize_name(tensor.name) return MPI_LIB.horovod_broadcast(tensor, name=name, root_rank=root_rank, ignore_name_scope=ignore_name_scope) 4.6.4 同步参数 在后台进程中，会根据情况定期同步参数。\n1 2 3 4 5 6 7 8 9 10 11 12 bool RunLoopOnce(HorovodGlobalState\u0026 state) { // 业务逻辑功能 if (state.parameter_manager.IsAutoTuning()) { bool should_sync = state.parameter_manager.Update(tensor_names, total_tensor_size); // 看看是否需要同步，如果需要，就同步。 if (should_sync) { state.controller-\u003eSynchronizeParameters(); } } ...... } 同步参数代码也是调用了 Bcast 功能完成。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void Controller::SynchronizeParameters() { ParameterManager::Params param; if (is_coordinator_) { // rank 0 执行操作 param = parameter_manager_.GetParams(); } void* buffer = (void*)(\u0026param); size_t param_size = sizeof(param); Bcast(buffer, param_size, 0, Communicator::GLOBAL); if (!is_coordinator_) { // worker 执行操作 parameter_manager_.SetParams(param); } } 4.7 DistributedOptimizer 最后需要配置DistributedOptimizer，这就是关键点之一。\n1 2 3 # Horovod: add Horovod DistributedOptimizer. opt = hvd.DistributedOptimizer( opt, backward_passes_per_step=1, average_aggregated_gradients=True) TF Optimizer 是模型训练的关键API，可以获取到每个OP的梯度并用来更新权重。HVD 在原始 TF Optimizer的基础上包装了hvd.DistributedOptimizer。\nDistributedOptimizer包装器将原始优化器作为输入，将梯度计算委托给它。 即DistributedOptimizer会调用原始优化器进行梯度计算。这样，在集群中每台机器都会用原始优化器得到自己的梯度（Local Gradient）。\nHorovod DistributedOptimizer接下来会使用all-reduce或all-gather来完成全局梯度归并，然后将这些平均梯度应用于所有设备。\n我们梳理下其中的调用关系：\nhvd.DistributedOptimizer继承 keras Optimizer，在计算时候，依然由传入的原始优化器做计算。 在得到计算的梯度之后，调用 hvd.allreduce 或者 hvd.allgather 来计算。 最后实施这些平均之后的梯度。从而实现整个集群的梯度归并操作。 具体后文会详细介绍。\n4.8 未来可能 Horovod 目前架构的基础是：机器学习的模型参数在一张 GPU 上可以存下。\n未来是否可以把模型分片结合进来，是一个很大的看点。\n另外，如果模型的全连接层较多，则全连接层的强耦合性结合 allreduce 类似 bsp 的同步机制，还是会让网络通信时间成为瓶颈。因此，在 ring-allreduce 环境下，同步协议的改造，比如利用 SSP 来替换 BSP，或者利用梯度压缩来加快 allreduce 进程也是值得探索的方向。\n5 总结 针对文初提出的几个问题，我们现在回答如下：\nHovorod 怎么进行数据分割？ 答案：有的框架可以自动做数据分割。如果框架不提供，则需要用户自己进行数据分割，以保证每个GPU进程训练的数据集是不一样的。 Hovorod 怎么进行模型分发？ 用户需要手动拷贝训练代码到各个节点上。 Hovorod 启动时候，python 和 C++ 都做了什么？ 答案：python 会引入 C++库，初始化各种变量和配置。C++部分会对 MPI，GLOO上下文进行初始化，启动后台进程处理内部通信。 如何确保 Hovorod 启动时候步骤一致； 答案： rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。 下一篇文章将深入到python世界看看。\nreference: [1].https://www.cnblogs.com/rossiXYZ/p/14856543.html\n",
  "wordCount" : "6571",
  "inLanguage": "en",
  "datePublished": "2022-10-08T17:25:41+08:00",
  "dateModified": "2022-10-08T17:25:41+08:00",
  "author":[{
    "@type": "Person",
    "name": "Jian"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jianye0428.github.io/en/posts/notes/2022-10-08_horovod_2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jian's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jianye0428.github.io/favicon/jian_icon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jianye0428.github.io/en/" accesskey="h" title="Jian&#39;s Blog (Alt + H)">
                <img src="https://jianye0428.github.io/favicon/jian_icon.png" alt="logo" aria-label="logo"
                    height="30">Jian&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://jianye0428.github.io/cn/" title="Chinese"
                            aria-label="Chinese">Chinese</a>
                    </li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jianye0428.github.io/en/myresume/" title="My Resume">
                    <span>My Resume</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/tags/" title="🔖Tags">
                    <span>🔖Tags</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/archives" title="🙋🏻‍♂️Archive">
                    <span>🙋🏻‍♂️Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/search/" title="🔍Search (Alt &#43; /)" accesskey=/>
                    <span>🔍Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jianye0428.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://jianye0428.github.io/en/posts/">Posts</a></div>
    <h1 class="post-title">
      深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入
    </h1>
    <div class="post-meta"><span title='2022-10-08 17:25:41 +0800 CST'>2022-10-08</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;Jian&nbsp;|&nbsp;<a href="https://github.com/jianye0428/myblog/tree/main/content/posts/notes/2022-10-08_Horovod_2.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#0-%e6%91%98%e8%a6%81" aria-label="0 摘要">0 摘要</a></li>
                    <li>
                        <a href="#1-horovod-%e7%ae%80%e4%bb%8b" aria-label="1 Horovod 简介">1 Horovod 简介</a></li>
                    <li>
                        <a href="#2-hovorod-%e6%9c%ba%e5%88%b6%e6%a6%82%e8%bf%b0" aria-label="2 Hovorod 机制概述">2 Hovorod 机制概述</a><ul>
                            
                    <li>
                        <a href="#21-horovod-%e6%9c%ba%e5%88%b6" aria-label="2.1 Horovod 机制">2.1 Horovod 机制</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-%e7%a4%ba%e4%be%8b%e4%bb%a3%e7%a0%81" aria-label="3 示例代码">3 示例代码</a><ul>
                            
                    <li>
                        <a href="#31--%e6%91%98%e8%a6%81%e4%bb%a3%e7%a0%81" aria-label="3.1  摘要代码">3.1  摘要代码</a></li>
                    <li>
                        <a href="#32-horovodrun" aria-label="3.2 horovodrun">3.2 horovodrun</a></li></ul>
                    </li>
                    <li>
                        <a href="#4-%e8%bf%90%e8%a1%8c%e9%80%bb%e8%be%91" aria-label="4 运行逻辑">4 运行逻辑</a><ul>
                            
                    <li>
                        <a href="#41-%e5%bc%95%e5%85%a5python%e6%96%87%e4%bb%b6" aria-label="4.1 引入python文件">4.1 引入python文件</a></li>
                    <li>
                        <a href="#42--%e5%88%9d%e5%a7%8b%e5%8c%96-in-python" aria-label="4.2  初始化 in python">4.2  初始化 in python</a><ul>
                            
                    <li>
                        <a href="#421-%e5%bc%95%e5%85%a5so%e5%ba%93" aria-label="4.2.1 引入SO库">4.2.1 引入SO库</a><ul>
                            
                    <li>
                        <a href="#4211-so%e5%ba%93" aria-label="4.2.1.1 SO库">4.2.1.1 SO库</a></li>
                    <li>
                        <a href="#4222-so%e4%bd%9c%e7%94%a8" aria-label="4.2.2.2 SO作用">4.2.2.2 SO作用</a></li></ul>
                    </li>
                    <li>
                        <a href="#422-%e5%88%9d%e5%a7%8b%e5%8c%96%e9%85%8d%e7%bd%ae" aria-label="4.2.2 初始化配置">4.2.2 初始化配置</a></li>
                    <li>
                        <a href="#423-hvdinit-%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="4.2.3 hvd.init() 初始化">4.2.3 hvd.init() 初始化</a></li></ul>
                    </li>
                    <li>
                        <a href="#43-%e5%88%9d%e5%a7%8b%e5%8c%96-in-c" aria-label="4.3 初始化 in C&#43;&#43;">4.3 初始化 in C++</a><ul>
                            
                    <li>
                        <a href="#431-horovod_init_comm" aria-label="4.3.1 horovod_init_comm">4.3.1 horovod_init_comm</a></li>
                    <li>
                        <a href="#432-initializehorovodonce" aria-label="4.3.2 InitializeHorovodOnce">4.3.2 InitializeHorovodOnce</a></li>
                    <li>
                        <a href="#433-horovodglobalstate" aria-label="4.3.3 HorovodGlobalState">4.3.3 HorovodGlobalState</a></li></ul>
                    </li>
                    <li>
                        <a href="#43-hvd-%e6%a6%82%e5%bf%b5" aria-label="4.3 hvd 概念">4.3 hvd 概念</a></li>
                    <li>
                        <a href="#45--%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" aria-label="4.5  数据处理">4.5  数据处理</a></li>
                    <li>
                        <a href="#46-%e5%b9%bf%e6%92%ad%e5%88%9d%e5%a7%8b%e5%8c%96%e5%8f%98%e9%87%8f" aria-label="4.6 广播初始化变量">4.6 广播初始化变量</a><ul>
                            
                    <li>
                        <a href="#461-%e5%b9%bf%e6%92%ad%e5%ae%9a%e4%b9%89" aria-label="4.6.1 广播定义">4.6.1 广播定义</a></li>
                    <li>
                        <a href="#462-broadcast_variables" aria-label="4.6.2 broadcast_variables">4.6.2 broadcast_variables</a></li>
                    <li>
                        <a href="#463-%e8%b0%83%e7%94%a8-mpi" aria-label="4.6.3 调用 MPI">4.6.3 调用 MPI</a></li>
                    <li>
                        <a href="#464-%e5%90%8c%e6%ad%a5%e5%8f%82%e6%95%b0" aria-label="4.6.4 同步参数">4.6.4 同步参数</a></li></ul>
                    </li>
                    <li>
                        <a href="#47-distributedoptimizer" aria-label="4.7 DistributedOptimizer">4.7 DistributedOptimizer</a></li>
                    <li>
                        <a href="#48-%e6%9c%aa%e6%9d%a5%e5%8f%af%e8%83%bd" aria-label="4.8 未来可能">4.8 未来可能</a></li></ul>
                    </li>
                    <li>
                        <a href="#5-%e6%80%bb%e7%bb%93" aria-label="5 总结">5 总结</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>reference:
[1].https://www.cnblogs.com/rossiXYZ/p/14856543.html</p>
<h2 id="0-摘要">0 摘要<a hidden class="anchor" aria-hidden="true" href="#0-摘要">#</a></h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，在业界得到了广泛应用。</p>
<p>本系列将通过源码分析来带领大家了解 Horovod。系列大约有15 ～ 18 篇，本文是系列第二篇，从用户角度切入 Horovod。</p>
<p>前一篇参见如下：</p>
<p><a href="http://localhost:1313/posts/notes/2022-10-08_horovod_1/">深度学习分布式训练框架 Horovod[1] &ndash; 基础知识</a></p>
<h2 id="1-horovod-简介">1 Horovod 简介<a hidden class="anchor" aria-hidden="true" href="#1-horovod-简介">#</a></h2>
<p>Horovod 是Uber于2017年发布的一个易于使用的高性能的分布式训练框架，支持TensorFlow，Keras，PyTorch和MXNet。Horovod 的名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与分布式 TensorFlow 流程使用 Horovod 互相通信的场景很像。</p>
<p>因为各个机器学习框架对于底层集合通信库（ nccl，openmpi，gloo 等等）的利用水平可能各不相同，使得他们无法充分利用这些底层集合通信库的威力。因而，hovorod 就整合这些框架，提供一个易用高效的解决方案。</p>
<p>Uber的工程师就是根据FaceBook的一篇paper：“<a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a>”和百度的一篇“<a href="https://research.baidu.com/bringing-hpc-techniques-deep-learning/">Bringing HPC Techniques to Deep Learning</a>” 改进并发布了开源框架Horovod。</p>
<p>Horovod 相比于百度的工作，并无学术上的贡献。但是 Horovod 扎实的工程实现，使得它受到了更多的关注。它最大的优势在于对 RingAllReduce 进行了更高层次的抽象，使其支持多种不同的框架。同时引入了 Nvidia NCCL，对 GPU 更加友好。</p>
<p>Horovod依赖于Nvidia的 NCCL2 做 All Reduce，依赖于MPI做进程间通信，简化了同步多 GPU 或多节点分布式训练的开发流程。由于使用了NCCL2，Horovod也可以利用以下功能：NVLINK，RDMA，GPUDirectRDMA，自动检测通信拓扑，能够回退到 PCIe 和 TCP/IP 通信。</p>
<p>我们需要几个问题来引导分析：</p>
<ul>
<li>Hovorod 怎么进行数据分割？</li>
<li>Hovorod 怎么进行训练代码分发？</li>
<li>Hovorod 启动时候，python 和 C++ 都做了什么？</li>
<li>如何确保 Hovorod 启动时候步骤一致；</li>
</ul>
<h2 id="2-hovorod-机制概述">2 Hovorod 机制概述<a hidden class="anchor" aria-hidden="true" href="#2-hovorod-机制概述">#</a></h2>
<h3 id="21-horovod-机制">2.1 Horovod 机制<a hidden class="anchor" aria-hidden="true" href="#21-horovod-机制">#</a></h3>
<p>Horovod使用<strong>数据并行化</strong>策略在GPU上分配训练。</p>
<p>在数据并行化中，作业中的每个GPU都会接收其自己的数据批处理的独立切片，即它的“批处理切片”。 每个GPU都使用自己分配到的数据来独立计算，进行梯度更新。</p>
<p>假如使用两个GPU，批处理大小为32，则第一个GPU将处理前16条记录的正向传播和向后传播，以及第二个GPU处理后16条记录的正向传播和向后传播。然后，这些梯度更新将在GPU之间平均在一起，最后应用于模型。</p>
<p>每一个迭代的操作方法如下：</p>
<ol>
<li>
<p>每个 worker 将维护自己的模型权重副本和自己的数据集副本。</p>
</li>
<li>
<p>收到执行信号后，每个工作进程都会从数据集中提取一个不相交的批次，并计算该批次的梯度。</p>
</li>
<li>
<p>Workers 使用ring all-reduce算法来同步彼此的梯度，从而在本地所有节点上计算同样的平均梯度。</p>
<ol>
<li>
<p>将每个设备上的梯度 tensor 切分成长度大致相等的 num_devices 个分片，后续每一次通信都将给下一个邻居发送一个自己的分片（同时从上一个邻居接受一个新分片）。</p>
</li>
<li>
<p>ScatterReduce 阶段：通过 num_devices - 1 轮通信和相加，在每个 device 上都计算出一个 tensor 分片的和，即每个 device 将有一个块，其中包含所有device 中该块中所有值的总和；具体如下：</p>
</li>
</ol>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-10-08_Horovod_2/Horovod_2_scatter_reduce.png#center" alt="Scatter Reduce"  />
</p>
<ol start="3">
<li>AllGather 阶段：通过 num_devices - 1 轮通信和覆盖，将上个阶段计算出的每个 tensor 分片的和 广播到其他 device；最终所有节点都拥有所有tensor分片和。具体如下：
<img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-10-08_Horovod_2/Horovod_2_allgather.png#center" alt="Allgather"  />
</li>
<li>在每个设备上合并分片，得到梯度和，然后除以 num_devices，得到平均梯度；</li>
</ol>
</li>
<li>
<p>每个 worker 将 梯度更新 应用于其模型的本地副本。</p>
</li>
<li>
<p>执行下一个batch。</p>
</li>
</ol>
<h2 id="3-示例代码">3 示例代码<a hidden class="anchor" aria-hidden="true" href="#3-示例代码">#</a></h2>
<h3 id="31--摘要代码">3.1  摘要代码<a hidden class="anchor" aria-hidden="true" href="#31--摘要代码">#</a></h3>
<p>我们此处给出官网示例代码部分摘要，具体分析参见下面代码中的注释。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.tensorflow.keras</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: initialize Horovod.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span> <span class="c1"># 初始化 Horovod，启动相关线程和MPI线程</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 依据 local rank 为不同的进程分配不同的GPU</span>
</span></span><span class="line"><span class="cl"><span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">mnist_images</span><span class="p">,</span> <span class="n">mnist_labels</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;mnist-</span><span class="si">%d</span><span class="s1">.npz&#39;</span> <span class="o">%</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 切分数据  </span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_images</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">             <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="o">......</span>
</span></span><span class="line"><span class="cl">    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: adjust learning rate based on number of GPUs.</span>
</span></span><span class="line"><span class="cl"><span class="n">scaled_lr</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="c1"># 根据Worker的数量增加学习率的大小</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">scaled_lr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 把常规TensorFlow Optimizer通过Horovod包装起来，进而使用 ring-allreduce 来得到平均梯度</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="p">,</span> <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow</span>
</span></span><span class="line"><span class="cl"><span class="c1"># uses hvd.DistributedOptimizer() to compute gradients.</span>
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">experimental_run_tf_function</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># 广播初始化，将模型的参数从第一个设备传向其他设备，以保证初始化模型参数的一致性</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">MetricAverageCallback</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateWarmupCallback</span><span class="p">(</span><span class="n">initial_lr</span><span class="o">=</span><span class="n">scaled_lr</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them. # 只有设备0需要保存模型参数作为checkpoint</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">&#39;./checkpoint-</span><span class="si">{epoch}</span><span class="s1">.h5&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: write logs on worker 0.</span>
</span></span><span class="line"><span class="cl"><span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the model.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Horovod: adjust number of steps based on number of GPUs.</span>
</span></span><span class="line"><span class="cl"><span class="n">mnist_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">500</span> <span class="o">//</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="32-horovodrun">3.2 horovodrun<a hidden class="anchor" aria-hidden="true" href="#32-horovodrun">#</a></h3>
<p>Horovod训练脚本未作为Python脚本启动。 例如，您不能使用<code>python train.py</code>运行此脚本。 需要采用特殊的CLI命令 <code>horovodrun</code> 来启动（训练代码 train.py 需要手动拷贝到各个节点上，且目录相同）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ horovodrun -np 4 -H localhost:4 python train.py
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="4-运行逻辑">4 运行逻辑<a hidden class="anchor" aria-hidden="true" href="#4-运行逻辑">#</a></h2>
<p>我们按照顺序梳理，看看在程序初始化过程背后都做了什么。</p>
<h3 id="41-引入python文件">4.1 引入python文件<a hidden class="anchor" aria-hidden="true" href="#41-引入python文件">#</a></h3>
<p>如下代码会引入各种相关python文件。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">horovod.tensorflow.keras</span> <span class="k">as</span> <span class="nn">hvd</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="42--初始化-in-python">4.2  初始化 in python<a hidden class="anchor" aria-hidden="true" href="#42--初始化-in-python">#</a></h3>
<p>python 世界的初始化位于 <code>horovod-master/horovod/mxnet/mpi_ops.py</code></p>
<h4 id="421-引入so库">4.2.1 引入SO库<a hidden class="anchor" aria-hidden="true" href="#421-引入so库">#</a></h4>
<h5 id="4211-so库">4.2.1.1 SO库<a hidden class="anchor" aria-hidden="true" href="#4211-so库">#</a></h5>
<p><code>horovod/tensorflow/mpi_ops.py</code> 之中会引入SO库。
比如 <code>dist-packages/horovod/tensorflow/mpi_lib.cpython-36m-x86_64-linux-gnu.so</code>。</p>
<p>SO库 就是 horovod 中 C++ 代码编译出来的结果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_load_library</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Loads a .so file containing the specified operators.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">filename</span> <span class="o">=</span> <span class="n">resource_loader</span><span class="o">.</span><span class="n">get_path_to_datafile</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">library</span> <span class="o">=</span> <span class="n">load_library</span><span class="o">.</span><span class="n">load_op_library</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">library</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check possible symbol not found error from tensorflow version mismatch</span>
</span></span><span class="line"><span class="cl"><span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">MPI_LIB</span> <span class="o">=</span> <span class="n">_load_library</span><span class="p">(</span><span class="s1">&#39;mpi_lib&#39;</span> <span class="o">+</span> <span class="n">get_ext_suffix</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">check_installed_version</span><span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">raise</span> <span class="n">e</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">check_installed_version</span><span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="4222-so作用">4.2.2.2 SO作用<a hidden class="anchor" aria-hidden="true" href="#4222-so作用">#</a></h5>
<p>引入库的作用是获取到 C++ 的函数，并且用 python 封装一下，这样就可以在 python 世界使用 C++代码了。</p>
<p>由下文可以看出来，python 的 _allreduce 函数就会把功能转发给 C++，由 <code>MPI_LIB.horovod_allreduce</code> 完成。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">Sum</span><span class="p">,</span> <span class="n">prescale_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">postscale_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">               <span class="n">ignore_name_scope</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;HorovodAllreduce_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">_normalize_name</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">MPI_LIB</span><span class="o">.</span><span class="n">horovod_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">reduce_op</span><span class="o">=</span><span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">prescale_factor</span><span class="o">=</span><span class="n">prescale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">postscale_factor</span><span class="o">=</span><span class="n">postscale_factor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ignore_name_scope</span><span class="o">=</span><span class="n">ignore_name_scope</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="422-初始化配置">4.2.2 初始化配置<a hidden class="anchor" aria-hidden="true" href="#422-初始化配置">#</a></h4>
<p>我们摘录了主要部分，就是初始化 _HorovodBasics，然后从 _HorovodBasics 内获取各种函数，变量和配置，比如是否编译了mpi，gloo等等.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">horovod.common.basics</span> <span class="kn">import</span> <span class="n">HorovodBasics</span> <span class="k">as</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">_basics</span> <span class="o">=</span> <span class="n">_HorovodBasics</span><span class="p">(</span><span class="vm">__file__</span><span class="p">,</span> <span class="s1">&#39;mpi_lib&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># import basic methods</span>
</span></span><span class="line"><span class="cl"><span class="n">init</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">init</span>
</span></span><span class="line"><span class="cl"><span class="n">size</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl"><span class="n">local_size</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">local_size</span>
</span></span><span class="line"><span class="cl"><span class="n">rank</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">rank</span>
</span></span><span class="line"><span class="cl"><span class="n">local_rank</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">local_rank</span>
</span></span><span class="line"><span class="cl"><span class="n">mpi_built</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">mpi_built</span>
</span></span><span class="line"><span class="cl"><span class="n">gloo_enabled</span> <span class="o">=</span> <span class="n">_basics</span><span class="o">.</span><span class="n">gloo_enabled</span>
</span></span><span class="line"><span class="cl"><span class="o">......</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="423-hvdinit-初始化">4.2.3 hvd.init() 初始化<a hidden class="anchor" aria-hidden="true" href="#423-hvdinit-初始化">#</a></h4>
<p>首先需要用 <code>hvd.init()</code> 来初始化，horovod 管理的所有状态都会传到 hvd 对象中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: initialize Horovod.</span>
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>此处调用的是 HorovodBasics 中的函数，我们看看做了什么。</p>
<p>可以看到，这部分会一直深入到 C++世界，调用了大量的 MPI_LIB_CTYPES 函数，所以我们接下来就要进入到 C++的世界看看。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">comm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;A function that initializes Horovod.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">mpi_built</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_mpi_built</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">MPI</span><span class="o">.</span><span class="n">_sizeof</span><span class="p">(</span><span class="n">MPI</span><span class="o">.</span><span class="n">Comm</span><span class="p">)</span> <span class="o">==</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">sizeof</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">MPI_Comm</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">MPI_Comm</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init_comm</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">MPI_Comm</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">comm_obj</span> <span class="o">=</span> <span class="n">MPI_Comm</span><span class="o">.</span><span class="n">from_address</span><span class="p">(</span><span class="n">MPI</span><span class="o">.</span><span class="n">_addressof</span><span class="p">(</span><span class="n">comm</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init_comm</span><span class="p">(</span><span class="n">comm_obj</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">comm_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">comm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">MPI_LIB_CTYPES</span><span class="o">.</span><span class="n">horovod_init</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span> <span class="o">*</span> <span class="n">comm_size</span><span class="p">)(</span><span class="o">*</span><span class="n">comm</span><span class="p">),</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int</span><span class="p">(</span><span class="n">comm_size</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>目前逻辑如下图：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-JAVA" data-lang="JAVA"><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">python</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">C</span><span class="o">++</span> <span class="n">SO</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Create</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="na">init</span><span class="o">()</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">Python</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+------------------------------------------+</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span><span class="o">++</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="43-初始化-in-c">4.3 初始化 in C++<a hidden class="anchor" aria-hidden="true" href="#43-初始化-in-c">#</a></h3>
<h4 id="431-horovod_init_comm">4.3.1 horovod_init_comm<a hidden class="anchor" aria-hidden="true" href="#431-horovod_init_comm">#</a></h4>
<p>在初始化的时候，Horovod 会：</p>
<ul>
<li>调用 <code>MPI_Comm_dup</code> 获取一个 Communicator，这样就有了和 MPI 协调的基础。</li>
<li>然后调用 <code>InitializeHorovodOnce</code>。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">void</span> <span class="n">horovod_init_comm</span><span class="p">(</span><span class="n">MPI_Comm</span> <span class="n">comm</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">MPI_Comm_dup</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mpi_context</span><span class="o">.</span><span class="n">mpi_comm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">nullptr</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="432-initializehorovodonce">4.3.2 InitializeHorovodOnce<a hidden class="anchor" aria-hidden="true" href="#432-initializehorovodonce">#</a></h4>
<p>InitializeHorovodOnce 是初始化的主要工作，主要是：</p>
<ul>
<li>依据是否编译了 mpi 或者 gloo，对各自的 context 进行处理，为 globalstate 创建对应的 controller；</li>
<li>启动了后台线程 BackgroundThreadLoop 用来在各个worker之间协调；</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">void</span> <span class="n">horovod_init</span><span class="p">(</span><span class="n">const</span> <span class="nb">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="nb">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">void</span> <span class="n">InitializeHorovodOnce</span><span class="p">(</span><span class="n">const</span> <span class="nb">int</span><span class="o">*</span> <span class="n">ranks</span><span class="p">,</span> <span class="nb">int</span> <span class="n">nranks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Ensure</span> <span class="n">background</span> <span class="n">thread</span> <span class="ow">is</span> <span class="n">only</span> <span class="n">started</span> <span class="n">once</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="err">!</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">initialize_flag</span><span class="o">.</span><span class="n">test_and_set</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">=</span> <span class="n">ParseControllerOpsFromEnv</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">=</span> <span class="n">ParseCPUOpsFromEnv</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1">#if HAVE_MPI // 依据是否编译了MPI进行处理</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Enable</span> <span class="n">mpi</span> <span class="ow">is</span> <span class="n">it</span><span class="s1">&#39;s used either in cpu data transfer or controller</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">mpi_context</span><span class="o">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">MPI</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">      <span class="o">//</span> <span class="n">创建一个</span> <span class="n">MPIController</span> <span class="n">对象</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">new</span> <span class="n">MPIController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">group_table</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">mpi_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SetRanks</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">nranks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">#endif</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#if HAVE_GLOO // 依据是否编译了 GLOO 进行处理</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Enable</span> <span class="n">gloo</span> <span class="ow">is</span> <span class="n">it</span><span class="s1">&#39;s used either in cpu data transfer or controller</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">cpu_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">        <span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">gloo_context</span><span class="o">.</span><span class="n">Enable</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">control_operation</span> <span class="o">==</span> <span class="n">LibType</span><span class="p">::</span><span class="n">GLOO</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">horovod_global</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">new</span> <span class="n">GlooController</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">response_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">tensor_queue</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">timeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">horovod_global</span><span class="o">.</span><span class="n">parameter_manager</span><span class="p">,</span> <span class="n">horovod_global</span><span class="o">.</span><span class="n">group_table</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="n">gloo_context</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">#endif</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">Reset</span> <span class="n">initialization</span> <span class="n">flag</span>
</span></span><span class="line"><span class="cl">    <span class="o">//</span> <span class="n">启动后台线程</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">initialization_done</span> <span class="o">=</span> <span class="n">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">horovod_global</span><span class="o">.</span><span class="n">background_thread</span> <span class="o">=</span> <span class="n">std</span><span class="p">::</span><span class="n">thread</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">BackgroundThreadLoop</span><span class="p">,</span> <span class="n">std</span><span class="p">::</span><span class="n">ref</span><span class="p">(</span><span class="n">horovod_global</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="o">//</span> <span class="n">Wait</span> <span class="n">to</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">background</span> <span class="n">thread</span> <span class="n">has</span> <span class="n">finished</span> <span class="n">initializing</span> <span class="n">MPI</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">  <span class="k">while</span> <span class="p">(</span><span class="err">!</span><span class="n">horovod_global</span><span class="o">.</span><span class="n">initialization_done</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="p">::</span><span class="n">this_thread</span><span class="p">::</span><span class="n">sleep_for</span><span class="p">(</span><span class="n">std</span><span class="p">::</span><span class="n">chrono</span><span class="p">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="433-horovodglobalstate">4.3.3 HorovodGlobalState<a hidden class="anchor" aria-hidden="true" href="#433-horovodglobalstate">#</a></h4>
<p>在 C++ 世界，HorovodGlobalState 起到了<font color=red>集中管理各种全局变量</font>的作用。</p>
<p>HorovodGlobalState 在 horovod 中是一个全局变量，其中的元素可以供不同的线程访问。HorovodGlobalState 在加载 C++ 的代码时候就已经创建了，同时创建的还有各种 context（mpi_context, nccl_context, gpu_context）。</p>
<p>Horovod 主要会在backgroundThreadLoop 中完成 HorovodGlobalState 不同元素初始化，比较重要的有：</p>
<ul>
<li>controller 管理总体通信控制流；</li>
<li>tensor_queue 会处理从前端过来的通信需求（allreduce，broadcast 等)；</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// All the Horovod state that must be stored globally per-process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">HorovodGlobalState</span> <span class="n">horovod_global</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_MPI
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">MPIContext</span> <span class="n">mpi_context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#if HAVE_GLOO
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">GlooContext</span> <span class="n">gloo_context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="p">....</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">OperationManager</span><span class="o">&gt;</span> <span class="n">op_manager</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>HorovodGlobalState 摘要如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">HorovodGlobalState</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Background thread running MPI communication.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">background_thread</span><span class="p">;</span> <span class="c1">// 后台线程，用来在各个worker之间协调
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">ParameterManager</span> <span class="n">parameter_manager</span><span class="p">;</span> <span class="c1">// 维护后台总体参数配置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Encapsulates the fusion buffers, handles resizing and auto-tuning of buffer
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// size.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">FusionBufferManager</span> <span class="n">fusion_buffer</span><span class="p">;</span> <span class="c1">// 融合tensor，以便缩减通信开销
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Controller</span><span class="o">&gt;</span> <span class="n">controller</span><span class="p">;</span> <span class="c1">//管理总体通信控制流
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">TensorQueue</span> <span class="n">tensor_queue</span><span class="p">;</span> <span class="c1">//处理从前端过来的通信需求（allreduce，broadcast 等）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Pointer to shared buffer for allgather
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">void</span><span class="o">*</span> <span class="n">shared_buffer</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// LRU cache of Responses
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ResponseCache</span> <span class="n">response_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Information on registered groups.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">GroupTable</span> <span class="n">group_table</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">HorovodGlobalState</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Make sure that the destructor of the background thread is safe to
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// call. If a thread is still joinable (not detached or complete) its
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// destructor cannot be called.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">background_thread</span><span class="p">.</span><span class="n">joinable</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">shut_down</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">background_thread</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>目前具体逻辑如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">python</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Import</span> <span class="n">C</span><span class="o">++</span> <span class="n">SO</span> <span class="n">files</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">           <span class="n">Create</span> <span class="n">_HorovodBasics</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="na">init</span><span class="o">()</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>
</span></span><span class="line"><span class="cl"><span class="n">Python</span>              <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="o">+-------------------------------------------------------------------------------------------------------------+</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">++</span>                 <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                                                          <span class="o">+-----------------------------+</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">|</span>  <span class="n">HorovodGlobalState</span>         <span class="o">|</span>
</span></span><span class="line"><span class="cl">              <span class="n">horovod_init_comm</span>                                                <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                             <span class="o">+------------------+</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span> <span class="n">horovod_global</span> <span class="o">+---------&gt;</span> <span class="o">|</span>        <span class="n">TensorQueue</span>          <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">background_thread</span>    <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                  <span class="o">|</span> <span class="n">mpi_context</span>      <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">           <span class="n">InitializeHorovodOnce</span>   <span class="o">+------------&gt;</span> <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">ParameterManager</span>     <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">+</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span> <span class="n">gloo_context</span>     <span class="o">|</span>         <span class="o">|</span>        <span class="n">FusionBufferManager</span>  <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="o">|</span>                             <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">Controller</span>           <span class="o">|</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v</span>                             <span class="o">|</span> <span class="n">op_manager</span>       <span class="o">|</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">             <span class="n">background_threa</span>                     <span class="o">|</span>                  <span class="o">|</span>         <span class="o">|</span>        <span class="n">ResponseCache</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                  <span class="o">+------------------+</span>         <span class="o">|</span>                             <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">|</span>        <span class="n">shared_buffer</span>        <span class="o">|</span>
</span></span><span class="line"><span class="cl">                                                                               <span class="o">+-----------------------------+</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如图：</p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-10-08_Horovod_2/Horovod_2_init_logic.png#center" alt="HOROVOD INIT LOGIC"  />
</p>
<p>至此，horovod 已经初始化完成，用户代码可以使用了。</p>
<h3 id="43-hvd-概念">4.3 hvd 概念<a hidden class="anchor" aria-hidden="true" href="#43-hvd-概念">#</a></h3>
<p>在用户代码中，接下来是rank概念。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们介绍下几个相关概念：</p>
<ul>
<li>Horovod为设备上的每个GPU启动了该训练脚本的一个副本。<strong>local rank</strong>就是分配给某一台计算机上每个执行训练的唯一编号（也可以认为是进程号或者GPU设备的ID号），范围是 0 到 n-1，其中 n 是该计算机上GPU设备的数量。</li>
<li>rank 可以认为是代表分布式任务里的一个执行训练的唯一全局编号（<font color=red>用于进程间通讯</font>）。Rank 0 在Horovod中通常具有特殊的意义：<strong>它是负责此同步的设备</strong>。
<ul>
<li>在百度的实现中，不同 Rank 的角色是不一样的，Rank 0 会充当 coordinator 的角色。它会协调来自其他 Rank 的 MPI 请求，是一个工程上的考量。这一设计也被后来的 Horovod 采用。</li>
<li>Rank 0 也用来把参数广播到其他进程 &amp; 存储 checkpoint。</li>
<li>world_size：进程总数量，会等到所有world_size个进程就绪之后才会开始训练。</li>
</ul>
</li>
</ul>
<p>hvd.init 这部分的目的就是让<strong>并行进程</strong>们可以知道自己被分配的 rank / local rank 等信息，于是后续可以根据 local rank（所在节点上的第几张 GPU 卡） 来设置所需的显存分配。</p>
<h3 id="45--数据处理">4.5  数据处理<a hidden class="anchor" aria-hidden="true" href="#45--数据处理">#</a></h3>
<p>接下来是数据处理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_images</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">             <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mnist_labels</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里有几点需要说明：</p>
<ul>
<li>
<p>首先，训练的数据需要放置在任何节点都能访问的地方。</p>
</li>
<li>
<p>其次，Horovod 需要对数据进行分片处理，需要在不同机器上按Rank进行切分，以保证每个GPU进程训练的数据集是不一样的。</p>
</li>
<li>
<p>数据集本体需要出于数据并行性的需求而被拆分为多个分片，Horovod的不同工作节点都将分别读取自己的数据集分片。</p>
</li>
</ul>
<p>从 PyTorch 示例脚本看得更加清楚。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: use DistributedSampler to partition the training data.</span>
</span></span><span class="line"><span class="cl"><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">rank</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p><code>DataLoader</code>的采样器组件从要绘制的数据集中返回可迭代的索引。 PyTorch中的默认采样器是顺序的，返回序列<code>0, 1, 2, …, n</code> 。 Horovod使用其<code>DistributedSampler</code>覆盖了此行为，该DistributedSampler处理跨计算机的数据集分区。 <code>DistributedSampler</code>本身接受两个参数作为输入： <code>hvd.size()</code> (GPU的总数，例如16)和<code>hvd.rank()</code> (从总体列表中分配给该设备的ID，例如0…15)。</p>
</li>
<li>
<p>Pytorch使用的是<strong>数据分布式训练</strong>，每个进程实际上是独立加载数据的，所以需要加载相同数据集后用一定的规则根据rank来顺序切割获取不同的数据子集，DistributedSampler就是用来确保dataloader只会load到整个数据集的一个特定子集的做法(实际上不用Pytorch提供的DistributedSampler工具，自己做加载数据后切分word_size个子集按rank顺序拿到子集效果也是一样)。</p>
</li>
<li>
<p>同时为了能够按顺序划分数据子集，拿到不同部分数据，所以数据集不能够进行随机打散，所以用了参数 <code>'shuffle': False</code>。</p>
</li>
</ul>
<h3 id="46-广播初始化变量">4.6 广播初始化变量<a hidden class="anchor" aria-hidden="true" href="#46-广播初始化变量">#</a></h3>
<p>以下代码完成广播初始化的功能。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hvd</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">BroadcastGlobalVariablesCallback</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这句代码保证的是 rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</p>
<p>下面就介绍下 Horvod 之中广播的使用。</p>
<h4 id="461-广播定义">4.6.1 广播定义<a hidden class="anchor" aria-hidden="true" href="#461-广播定义">#</a></h4>
<p>广播的具体实现是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BroadcastGlobalVariablesCallbackImpl</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">BroadcastGlobalVariablesCallbackImpl</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span> <span class="o">=</span> <span class="n">root_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">hvd</span><span class="o">.</span><span class="n">_executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;variables&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># TensorFlow 2.0 or TensorFlow eager</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">variables</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">root_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">root_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">bcast_op</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_global_variables</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">bcast_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_done</span> <span class="o">=</span> <span class="kc">True</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="462-broadcast_variables">4.6.2 broadcast_variables<a hidden class="anchor" aria-hidden="true" href="#462-broadcast_variables">#</a></h4>
<p>broadcast_variables 调用了 _make_broadcast_group_fn 完成功能，可以看到对于 执行图 的每个变量，调用了 broadcast。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">broadcast_variables</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Broadcasts variables from root rank to all other processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">        variables: variables for broadcast
</span></span></span><span class="line"><span class="cl"><span class="s2">        root_rank: rank of the process from which global variables will be broadcasted
</span></span></span><span class="line"><span class="cl"><span class="s2">                   to all other processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">broadcast_group</span> <span class="o">=</span> <span class="n">_make_broadcast_group_fn</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>以及</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@_cache</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_make_broadcast_group_fn</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Eager mode will parallelize independent control flow</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_make_subgraph</span><span class="p">(</span><span class="n">broadcast_group</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Graph mode requires an Op</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">broadcast_group</span><span class="p">(</span><span class="n">variables</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">var</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                              <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variables</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">broadcast_group</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="463-调用-mpi">4.6.3 调用 MPI<a hidden class="anchor" aria-hidden="true" href="#463-调用-mpi">#</a></h4>
<p>broadcast 就是调用了 MPI 函数真正完成了功能。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">root_rank</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_name_scope</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;An op which broadcasts the input tensor on root rank to the same input tensor
</span></span></span><span class="line"><span class="cl"><span class="s2">    on all other Horovod processes.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The broadcast operation is keyed by the name of the op. The tensor type and
</span></span></span><span class="line"><span class="cl"><span class="s2">    shape must be the same on all Horovod processes for a given name. The broadcast
</span></span></span><span class="line"><span class="cl"><span class="s2">    will not start until all processes are ready to send and receive the tensor.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">      A tensor of the same shape and type as `tensor`, with the value broadcasted
</span></span></span><span class="line"><span class="cl"><span class="s2">      from root rank.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_executing_eagerly</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;HorovodBroadcast_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">_normalize_name</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">MPI_LIB</span><span class="o">.</span><span class="n">horovod_broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="n">root_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                     <span class="n">ignore_name_scope</span><span class="o">=</span><span class="n">ignore_name_scope</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="464-同步参数">4.6.4 同步参数<a hidden class="anchor" aria-hidden="true" href="#464-同步参数">#</a></h4>
<p>在后台进程中，会<strong>根据情况定期同步参数</strong>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">bool</span> <span class="nf">RunLoopOnce</span><span class="p">(</span><span class="n">HorovodGlobalState</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// 业务逻辑功能
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">IsAutoTuning</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">should_sync</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">state</span><span class="p">.</span><span class="n">parameter_manager</span><span class="p">.</span><span class="n">Update</span><span class="p">(</span><span class="n">tensor_names</span><span class="p">,</span> <span class="n">total_tensor_size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 看看是否需要同步，如果需要，就同步。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">should_sync</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">state</span><span class="p">.</span><span class="n">controller</span><span class="o">-&gt;</span><span class="n">SynchronizeParameters</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">......</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>同步参数代码也是调用了 Bcast 功能完成。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Controller</span><span class="o">::</span><span class="n">SynchronizeParameters</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ParameterManager</span><span class="o">::</span><span class="n">Params</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">is_coordinator_</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// rank 0 执行操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">param</span> <span class="o">=</span> <span class="n">parameter_manager_</span><span class="p">.</span><span class="n">GetParams</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">void</span><span class="o">*</span> <span class="n">buffer</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">size_t</span> <span class="n">param_size</span> <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">Bcast</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">param_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Communicator</span><span class="o">::</span><span class="n">GLOBAL</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_coordinator_</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// worker 执行操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">parameter_manager_</span><span class="p">.</span><span class="n">SetParams</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="47-distributedoptimizer">4.7 DistributedOptimizer<a hidden class="anchor" aria-hidden="true" href="#47-distributedoptimizer">#</a></h3>
<p>最后需要配置DistributedOptimizer，这就是关键点之一。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Horovod: add Horovod DistributedOptimizer.</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="p">,</span> <span class="n">backward_passes_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average_aggregated_gradients</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>TF Optimizer 是模型训练的关键API，可以获取到每个OP的梯度并用来更新权重。HVD 在原始 TF Optimizer的基础上包装了hvd.DistributedOptimizer。</p>
<p>DistributedOptimizer包装器将原始优化器作为输入，将梯度计算委托给它。 即DistributedOptimizer会调用原始优化器进行梯度计算。这样，在集群中每台机器都会用原始优化器得到自己的梯度（Local Gradient）。</p>
<p><code>Horovod DistributedOptimizer</code>接下来会使用all-reduce或all-gather来完成全局梯度归并，然后将这些平均梯度应用于所有设备。</p>
<p>我们梳理下其中的调用关系：</p>
<ul>
<li>hvd.DistributedOptimizer继承 keras Optimizer，在计算时候，依然由传入的原始优化器做计算。</li>
<li>在得到计算的梯度之后，调用 hvd.allreduce 或者 hvd.allgather 来计算。</li>
<li>最后实施这些平均之后的梯度。从而实现整个集群的梯度归并操作。</li>
</ul>
<p>具体后文会详细介绍。</p>
<h3 id="48-未来可能">4.8 未来可能<a hidden class="anchor" aria-hidden="true" href="#48-未来可能">#</a></h3>
<p>Horovod 目前架构的基础是：机器学习的模型参数在一张 GPU 上可以存下。</p>
<p><strong>未来是否可以把模型分片结合进来，是一个很大的看点。</strong></p>
<p>另外，如果模型的全连接层较多，则全连接层的强耦合性结合 allreduce 类似 bsp 的同步机制，还是会让网络通信时间成为瓶颈。因此，在 ring-allreduce 环境下，同步协议的改造，比如利用 SSP 来替换 BSP，或者利用梯度压缩来加快 allreduce 进程也是值得探索的方向。</p>
<h2 id="5-总结">5 总结<a hidden class="anchor" aria-hidden="true" href="#5-总结">#</a></h2>
<p>针对文初提出的几个问题，我们现在回答如下：</p>
<ul>
<li>Hovorod 怎么进行数据分割？
答案：有的框架可以自动做数据分割。如果框架不提供，则需要用户自己进行数据分割，以保证每个GPU进程训练的数据集是不一样的。</li>
<li>Hovorod 怎么进行模型分发？
用户需要手动拷贝训练代码到各个节点上。</li>
<li>Hovorod 启动时候，python 和 C++ 都做了什么？
答案：python 会引入 C++库，初始化各种变量和配置。C++部分会对 MPI，GLOO上下文进行初始化，启动后台进程处理内部通信。</li>
<li>如何确保 Hovorod 启动时候步骤一致；
答案： rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，即变量从第一个流程向其他流程传播，以实现参数一致性初始化。</li>
</ul>
<p>下一篇文章将深入到python世界看看。</p>
<p>reference:
[1].https://www.cnblogs.com/rossiXYZ/p/14856543.html</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jianye0428.github.io/en/tags/horovod/">Horovod</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jianye0428.github.io/en/posts/notes/2022-10-09_horovod_3/">
    <span class="title"><i class="fas fa-angle-double-left"></i> Prev Page</span>
    <br>
    <span>深度学习分布式训练框架 horovod(3) --- Horovodrun背后做了什么</span>
  </a>
  <a class="next" href="https://jianye0428.github.io/en/posts/notes/2022-10-08_horovod_1/">
    <span class="title">Next Page <i class="fas fa-angle-double-right"></i></span>
    <br>
    <span>深度学习分布式训练框架 Horovod[1] -- 基础知识</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 on twitter"
        href="https://twitter.com/intent/tweet/?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6%20Horovod%5b2%5d%20--%20%e4%bb%8e%e4%bd%bf%e7%94%a8%e8%80%85%e8%a7%92%e5%ba%a6%e5%88%87%e5%85%a5&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-10-08_horovod_2%2f&amp;hashtags=Horovod">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-10-08_horovod_2%2f&amp;title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6%20Horovod%5b2%5d%20--%20%e4%bb%8e%e4%bd%bf%e7%94%a8%e8%80%85%e8%a7%92%e5%ba%a6%e5%88%87%e5%85%a5&amp;summary=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6%20Horovod%5b2%5d%20--%20%e4%bb%8e%e4%bd%bf%e7%94%a8%e8%80%85%e8%a7%92%e5%ba%a6%e5%88%87%e5%85%a5&amp;source=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-10-08_horovod_2%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-10-08_horovod_2%2f&title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6%20Horovod%5b2%5d%20--%20%e4%bb%8e%e4%bd%bf%e7%94%a8%e8%80%85%e8%a7%92%e5%ba%a6%e5%88%87%e5%85%a5">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-10-08_horovod_2%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 on whatsapp"
        href="https://api.whatsapp.com/send?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6%20Horovod%5b2%5d%20--%20%e4%bb%8e%e4%bd%bf%e7%94%a8%e8%80%85%e8%a7%92%e5%ba%a6%e5%88%87%e5%85%a5%20-%20https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-10-08_horovod_2%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 深度学习分布式训练框架 Horovod[2] -- 从使用者角度切入 on telegram"
        href="https://telegram.me/share/url?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83%e6%a1%86%e6%9e%b6%20Horovod%5b2%5d%20--%20%e4%bb%8e%e4%bd%bf%e7%94%a8%e8%80%85%e8%a7%92%e5%ba%a6%e5%88%87%e5%85%a5&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-10-08_horovod_2%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>




<footer class="tc-container" id="comment">
    <div class="tc-title"><p class="c-title">Discussion</p></div>
    <div id="tcomments"></div>
</footer>
<script crossorigin="anonymous" src="/js/twikoo.min.b16100b7cf8a61759eab076a122482054e083087aad37c3be1fe2e293934dc34.js" integrity="sha256-sWEAt8&#43;KYXWeqwdqEiSCBU4IMIeq03w74f4uKTk03DQ="></script>
<script>
    twikoo.init({
        envId: 'https://my-repository-pink.vercel.app/',
        el: '#tcomments',
        region: 'ap-shanghai', 
        
        lang: 'zh-CN', 
    });
</script>

</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
