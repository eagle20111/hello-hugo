<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>PyTorch Notes | Jian&#39;s Blog</title>
<meta name="keywords" content="PyTorch">
<meta name="description" content="Torch åŸºæœ¬å‡½æ•° 1. torch.einsum() torch.einsum(equation, *operands)-&gt;Tensor:çˆ±å› æ–¯å¦æ±‚å’Œ ref1: ç®—å­éƒ¨ç½²: https://blog.csdn.net/HW140701/article/details/120654252 ref2: ä¾‹å­: https://zhuanlan.zhihu.com/p/361209187 ä¸‰æ¡åŸºæœ¬è§„åˆ™: è§„åˆ™ä¸€: equation ç®­å¤´å·¦è¾¹ï¼Œåœ¨ä¸åŒè¾“å…¥">
<meta name="author" content="Jian">
<link rel="canonical" href="https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.1ea9c8832138446635789668415e5c75b8a534b191ee749a44f5ab404c9f27c2.css" integrity="sha256-HqnIgyE4RGY1eJZoQV5cdbilNLGR7nSaRPWrQEyfJ8I=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="apple-touch-icon" href="https://jianye0428.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://jianye0428.github.io/favicon/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="baidu-site-verification" content="code-9oLyeix0aK" />
<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4a41bf85d719f0e8c3165fc76904f546";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>



<script defer crossorigin="anonymous" src="/js/katex.min.8f5024e83d2055dd60e021751066111b0057e230db34911dd56242d67f0a4c86.js" integrity="sha256-j1Ak6D0gVd1g4CF1EGYRGwBX4jDbNJEd1WJC1n8KTIY="></script>


<script defer crossorigin="anonymous" src="/js/auto-render.min.b09accad850e4e87b8a2fc8b93fae790def79172b68de72fd777958c52e566ad.js" integrity="sha256-sJrMrYUOToe4ovyLk/rnkN73kXK2jecv13eVjFLlZq0="></script>

<script>
    
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });

    
    window.WebFontConfig = {
        custom: {
            families: ['KaTeX_AMS', 'KaTeX_Caligraphic:n4,n7', 'KaTeX_Fraktur:n4,n7',
            'KaTeX_Main:n4,n7,i4,i7', 'KaTeX_Math:i4,i7', 'KaTeX_Script',
            'KaTeX_SansSerif:n4,n7,i4', 'KaTeX_Size1', 'KaTeX_Size2', 'KaTeX_Size3',
            'KaTeX_Size4', 'KaTeX_Typewriter'],
        },
    };
</script>


<script defer crossorigin="anonymous" src="/js/webfontloader.min.min.d1c6c39d18e2decb5c99dc9efc579098ab37b9654725df3f9c0737bc2dd00760.js" integrity="sha256-0cbDnRji3stcmdye/FeQmKs3uWVHJd8/nAc3vC3QB2A="></script>


 

<script async src="https://www.googletagmanager.com/gtag/js?id=G-C6GDZ56F4S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-C6GDZ56F4S', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="PyTorch Notes" />
<meta property="og:description" content="Torch åŸºæœ¬å‡½æ•° 1. torch.einsum() torch.einsum(equation, *operands)-&gt;Tensor:çˆ±å› æ–¯å¦æ±‚å’Œ ref1: ç®—å­éƒ¨ç½²: https://blog.csdn.net/HW140701/article/details/120654252 ref2: ä¾‹å­: https://zhuanlan.zhihu.com/p/361209187 ä¸‰æ¡åŸºæœ¬è§„åˆ™: è§„åˆ™ä¸€: equation ç®­å¤´å·¦è¾¹ï¼Œåœ¨ä¸åŒè¾“å…¥" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/" /><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-09T19:14:27&#43;08:00" />
<meta property="article:modified_time" content="2022-06-09T19:14:27&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/>

<meta name="twitter:title" content="PyTorch Notes"/>
<meta name="twitter:description" content="Torch åŸºæœ¬å‡½æ•° 1. torch.einsum() torch.einsum(equation, *operands)-&gt;Tensor:çˆ±å› æ–¯å¦æ±‚å’Œ ref1: ç®—å­éƒ¨ç½²: https://blog.csdn.net/HW140701/article/details/120654252 ref2: ä¾‹å­: https://zhuanlan.zhihu.com/p/361209187 ä¸‰æ¡åŸºæœ¬è§„åˆ™: è§„åˆ™ä¸€: equation ç®­å¤´å·¦è¾¹ï¼Œåœ¨ä¸åŒè¾“å…¥"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jianye0428.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "PyTorch Notes",
      "item": "https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "PyTorch Notes",
  "name": "PyTorch Notes",
  "description": "Torch åŸºæœ¬å‡½æ•° 1. torch.einsum() torch.einsum(equation, *operands)-\u0026gt;Tensor:çˆ±å› æ–¯å¦æ±‚å’Œ ref1: ç®—å­éƒ¨ç½²: https://blog.csdn.net/HW140701/article/details/120654252 ref2: ä¾‹å­: https://zhuanlan.zhihu.com/p/361209187 ä¸‰æ¡åŸºæœ¬è§„åˆ™: è§„åˆ™ä¸€: equation ç®­å¤´å·¦è¾¹ï¼Œåœ¨ä¸åŒè¾“å…¥",
  "keywords": [
    "PyTorch"
  ],
  "articleBody": "Torch åŸºæœ¬å‡½æ•° 1. torch.einsum() torch.einsum(equation, *operands)-\u003eTensor:çˆ±å› æ–¯å¦æ±‚å’Œ ref1: ç®—å­éƒ¨ç½²: https://blog.csdn.net/HW140701/article/details/120654252 ref2: ä¾‹å­: https://zhuanlan.zhihu.com/p/361209187\nä¸‰æ¡åŸºæœ¬è§„åˆ™:\nè§„åˆ™ä¸€: equation ç®­å¤´å·¦è¾¹ï¼Œåœ¨ä¸åŒè¾“å…¥ä¹‹é—´é‡å¤å‡ºç°çš„ç´¢å¼•è¡¨ç¤ºï¼ŒæŠŠè¾“å…¥å¼ é‡æ²¿ç€è¯¥ç»´åº¦åšä¹˜æ³•æ“ä½œï¼Œæ¯”å¦‚è¿˜æ˜¯ä»¥ä¸Šé¢çŸ©é˜µä¹˜æ³•ä¸ºä¾‹ï¼Œ â€œik,kj-\u003eijâ€ï¼Œk åœ¨è¾“å…¥ä¸­é‡å¤å‡ºç°ï¼Œæ‰€ä»¥å°±æ˜¯æŠŠ a å’Œ b æ²¿ç€ k è¿™ä¸ªç»´åº¦ä½œç›¸ä¹˜æ“ä½œï¼› è§„åˆ™äºŒ: åªå‡ºç°åœ¨ equation ç®­å¤´å·¦è¾¹çš„ç´¢å¼•ï¼Œè¡¨ç¤ºä¸­é—´è®¡ç®—ç»“æœéœ€è¦åœ¨è¿™ä¸ªç»´åº¦ä¸Šæ±‚å’Œï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢æåˆ°çš„æ±‚å’Œç´¢å¼•ï¼› è§„åˆ™ä¸‰: equation ç®­å¤´å³è¾¹çš„ç´¢å¼•é¡ºåºå¯ä»¥æ˜¯ä»»æ„çš„ï¼Œæ¯”å¦‚ä¸Šé¢çš„ â€œik,kj-\u003eijâ€ å¦‚æœå†™æˆ â€œik,kj-\u003ejiâ€ï¼Œé‚£ä¹ˆå°±æ˜¯è¿”å›è¾“å‡ºç»“æœçš„è½¬ç½®ï¼Œç”¨æˆ·åªéœ€è¦å®šä¹‰å¥½ç´¢å¼•çš„é¡ºåºï¼Œè½¬ç½®æ“ä½œä¼šåœ¨ einsum å†…éƒ¨å®Œæˆ ç‰¹æ®Šè§„åˆ™:\nequation å¯ä»¥ä¸å†™åŒ…æ‹¬ç®­å¤´åœ¨å†…çš„å³è¾¹éƒ¨åˆ†ï¼Œé‚£ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å‡ºå¼ é‡çš„ç»´åº¦ä¼šæ ¹æ®é»˜è®¤è§„åˆ™æ¨å¯¼ã€‚å°±æ˜¯æŠŠè¾“å…¥ä¸­åªå‡ºç°ä¸€æ¬¡çš„ç´¢å¼•å–å‡ºæ¥ï¼Œç„¶åæŒ‰å­—æ¯è¡¨é¡ºåºæ’åˆ—ï¼Œæ¯”å¦‚ä¸Šé¢çš„çŸ©é˜µä¹˜æ³• â€œik,kj-\u003eijâ€ ä¹Ÿå¯ä»¥ç®€åŒ–ä¸º â€œik,kjâ€ï¼Œæ ¹æ®é»˜è®¤è§„åˆ™ï¼Œè¾“å‡ºå°±æ˜¯ â€œijâ€ ä¸åŸæ¥ä¸€æ ·ï¼› equation ä¸­æ”¯æŒ â€œâ€¦â€ çœç•¥å·ï¼Œç”¨äºè¡¨ç¤ºç”¨æˆ·å¹¶ä¸å…³å¿ƒçš„ç´¢å¼•ã€‚æ¯”å¦‚åªå¯¹ä¸€ä¸ªé«˜ç»´å¼ é‡çš„æœ€åä¸¤ç»´åšè½¬ç½®å¯ä»¥è¿™ä¹ˆå†™ï¼š 1 2 3 a = torch.randn(2,3,5,7,9) # i = 7, j = 9 b = torch.einsum('...ij-\u003e...ji', [a]) 2. torch.permute()/torch.transpose() torch.permute(dim0, dim1, dim2):ç”¨äºè°ƒæ¢ä¸åŒç»´åº¦çš„é¡ºåº torch.transpose(input, dim0, dim1):äº¤æ¢çŸ©é˜µçš„ä¸¤ä¸ªç»´åº¦\n3. torch.rand() torch.rand(dim0, dim1):ç”Ÿæˆdim0 x dim1çš„tensor\n4. torch.size()/torch.shape torch.size():è¿”å›tensorçš„size torch.shape:è¿”å›tensorçš„size\n5. torch.tensordot() ref: tensordot()å’Œeinsum()çš„åŒºåˆ«: https://blog.csdn.net/Eric_1993/article/details/105670381 torch.tensordot(tensor1ï¼Œ tensor2ï¼Œ axes=([dim1,dim2],[dim0, dim1])): å°†axesæŒ‡å®šçš„å­æ•°ç»„è¿›è¡Œç‚¹ä¹˜, axes æŒ‡å®šå…·ä½“çš„ç»´åº¦.\n6. torch.transpose() torch.transpose(tensor, dim0, dim2) â€”\u003e Tensor:åœ¨dim0å’Œdim1æ–¹å‘ä¸Šè½¬ç½®\n###7. torch.index_add_()\nTensor.index_add_(dim, index, source, *, alpha=1) â†’ Tensor\ndemo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u003e\u003e\u003e x = torch.ones(5, 3) \u003e\u003e\u003e t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) \u003e\u003e\u003e index = torch.tensor([0, 4, 2]) \u003e\u003e\u003e x.index_add_(0, index, t) tensor([[ 2., 3., 4.], [ 1., 1., 1.], [ 8., 9., 10.], [ 1., 1., 1.], [ 5., 6., 7.]]) \u003e\u003e\u003e x.index_add_(0, index, t, alpha=-1) tensor([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) Torch NN Module 1 2 3 import torch from torch import nn from torch import functional as F 1. nn.Conv1d() torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\nShape: - Input: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$ - Output: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$, where $$L_{out} = \\frac{L_{in} + 2 \\cdot \\text{padding} - \\text{dilation} \\cdot (\\text{kernel_size} - 1) - 1}{stride}$$\nDemo:\n1 2 3 4 m = nn.Conv1d(16, 33, 3, stride=2) input = torch.randn(20, 16, 50) # B x C x H or N x C x L output = m(input) print(output.shape) # torch.Size([20, 33, 24]) 2. nn.Conv2d() torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\nShape:\nInput: $(N, C_{\\text in}, H_{\\text in}, W_{\\text in})$ or $(C_{\\text in}, H_{\\text in}, W_{\\text in})$ - Output: $(N, C_{\\text out}, H_{\\text out}, W_{\\text out})$ or $(C_{\\text out}, H_{\\text out}, W_{\\text out})$, where $$ H_{out} = \\frac{H_{in} + 2 \\cdot \\text{padding[0]} - \\text{dilation[0]} \\cdot (\\text{kernel_size[0]} - 1) - 1}{stride[0]} + 1 $$ $$ W_{out} = \\frac{W_{in} + 2 \\cdot \\text{padding[1]} - \\text{dilation[1]} \\cdot (\\text{kernel_size[1]} - 1) - 1}{stride[1]} + 1 $$ Demo:\n1 2 3 4 5 6 7 8 # With square kernels and equal stride m = nn.Conv2d(16, 33, 3, stride=2) # non-square kernels and unequal stride and with padding m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) # output.shape: 20 x 33 x 28 x 100 # non-square kernels and unequal stride and with padding and dilation m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) # output.shape: 20 x 33 x 26 x 100 input = torch.randn(20, 16, 50, 100) output = m(input) # 3. nn.functional.interpolate() torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False)\n4. nn.functional.ReLU() $$ \\text{ReLU} = (x)^+ = \\max {(0,x)}$$\ntorch.nn.ReLU(inplace=False)\nä½œç”¨:\nSigmoidçš„å¯¼æ•°åªæœ‰åœ¨0é™„è¿‘çš„æ—¶å€™æœ‰æ¯”è¾ƒå¥½çš„æ¿€æ´»æ€§ï¼Œåœ¨æ­£è´Ÿé¥±å’ŒåŒºçš„æ¢¯åº¦éƒ½æ¥è¿‘äº0ï¼Œæ‰€ä»¥è¿™ä¼šé€ æˆæ¢¯åº¦å¼¥æ•£ï¼Œè€ŒReLUå‡½æ•°åœ¨å¤§äº0çš„éƒ¨åˆ†æ¢¯åº¦ä¸ºå¸¸æ•°ï¼Œæ‰€ä»¥ä¸ä¼šäº§ç”Ÿæ¢¯åº¦å¼¥æ•£ç°è±¡ã€‚\nReLUå‡½æ•°åœ¨è´ŸåŠåŒºçš„å¯¼æ•°ä¸º0 ï¼Œæ‰€ä»¥ä¸€æ—¦ç¥ç»å…ƒæ¿€æ´»å€¼è¿›å…¥è´ŸåŠåŒºï¼Œé‚£ä¹ˆæ¢¯åº¦å°±ä¼šä¸º0ï¼Œè€Œæ­£å€¼ä¸å˜ï¼Œè¿™ç§æ“ä½œè¢«æˆä¸ºå•ä¾§æŠ‘åˆ¶ã€‚ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼šåœ¨è¾“å…¥æ˜¯è´Ÿå€¼çš„æƒ…å†µä¸‹ï¼Œå®ƒä¼šè¾“å‡º0ï¼Œé‚£ä¹ˆç¥ç»å…ƒå°±ä¸ä¼šè¢«æ¿€æ´»ã€‚è¿™æ„å‘³ç€åŒä¸€æ—¶é—´åªæœ‰éƒ¨åˆ†ç¥ç»å…ƒä¼šè¢«æ¿€æ´»ï¼Œä»è€Œä½¿å¾—ç½‘ç»œå¾ˆç¨€ç–ï¼Œè¿›è€Œå¯¹è®¡ç®—æ¥è¯´æ˜¯éå¸¸æœ‰æ•ˆç‡çš„ã€‚ï¼‰æ­£å› ä¸ºæœ‰äº†è¿™å•ä¾§æŠ‘åˆ¶ï¼Œæ‰ä½¿å¾—ç¥ç»ç½‘ç»œä¸­çš„ç¥ç»å…ƒä¹Ÿå…·æœ‰äº†ç¨€ç–æ¿€æ´»æ€§ã€‚å°¤å…¶ä½“ç°åœ¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹(å¦‚CNN)ä¸­ï¼Œå½“æ¨¡å‹å¢åŠ Nå±‚ä¹‹åï¼Œç†è®ºä¸ŠReLUç¥ç»å…ƒçš„æ¿€æ´»ç‡å°†é™ä½2çš„Næ¬¡æ–¹å€ã€‚\nreluå‡½æ•°çš„å¯¼æ•°è®¡ç®—æ›´å¿«ï¼Œç¨‹åºå®ç°å°±æ˜¯ä¸€ä¸ªif-elseè¯­å¥ï¼Œè€Œsigmoidå‡½æ•°è¦è¿›è¡Œæµ®ç‚¹å››åˆ™è¿ç®—ã€‚\nShape:\nInput: $(âˆ—)$, where $*$ means any number of dimensions. Output: $(âˆ—)$, same shape as the input. Demo:\n1 2 3 4 5 6 7 m = nn.ReLU() input = torch.randn(2) output = m(input) # An implementation of CReLU - https://arxiv.org/abs/1603.05201 m = nn.ReLU() input = torch.randn(2).unsqueeze(0) output = torch.cat((m(input),m(-input))) 5. nn.MaxPool2d() torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\nShape:\nInput: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$ Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$ where,\n$$ H_{out} = \\frac{H_{in} + 2 * \\text{padding}[0] - \\text{dilation}[0] * (\\text{kernel_size}[0]-1) - 1}{\\text{stride}[0]} + 1$$\n$$ W_{out} = \\frac{W_{in} + 2 * \\text{padding}[1] - \\text{dilation}[1] * (\\text{kernel_size}[1]-1) - 1}{\\text{stride}[1]} + 1$$\ndemo:\n1 2 3 4 5 6 # pool of square window of size=3, stride=2 m = nn.MaxPool2d(3, stride=2) # pool of non-square window m = nn.MaxPool2d((3, 2), stride=(2, 1)) input = torch.randn(20, 16, 50, 32) output = m(input) # 20 16 24 31 6. nn.AvgPool2d() 1 torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) Shape:\nInput: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$ Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$ where,\n$$ H_{out} = \\frac{H_{in} + 2 * \\text{padding}[0] - (\\text{kernel_size}[0])}{\\text{stride}[0]} + 1$$\n$$ W_{out} = \\frac{W_{in} + 2 * \\text{padding}[1] - (\\text{kernel_size}[1])}{\\text{stride}[1]} + 1$$\ndemo:\n1 2 3 4 5 6 # pool of square window of size=3, stride=2 m = nn.AvgPool2d(3, stride=2) # pool of non-square window m = nn.AvgPool2d((3, 2), stride=(2, 1)) input = torch.randn(20, 16, 50, 32) output = m(input) # 20 16, 24 31 torch.cuda ref link: https://zhuanlan.zhihu.com/p/76908135\ntorch.cuda.current_device(): è¿”å›å½“å‰é€‰æ‹©çš„è®¾å¤‡çš„ç´¢å¼•\ntorch.cuda.current_stream(): è¿”å›å‚æ•°è®¾å¤‡çš„å½“å‰çš„Stream\ntorch.cuda.default_stream(): è¿”å›å½“å‰å‚æ•°è®¾å¤‡çš„Stream\nCLASS torch.cuda.device: å¯ä»¥æ”¹å˜é€‰æ‹©çš„è®¾å¤‡çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ Parametersï¼šdevice (torch.device or int) â€“ device index to select. Itâ€™s a no-op if this argument is a negative integer or None.\ntorch.cuda.device_count(): è¿”å›å¯ä½¿ç”¨GPUçš„æ•°é‡\nCLASS torch.cuda.device_of(obj) Context-manager å°†å‚æ•°å¯¹è±¡çš„è®¾å¤‡æ”¹æˆå½“å‰çš„è®¾å¤‡ã€‚ä½ å¯ä»¥ä½¿ç”¨å¼ é‡æˆ–è€…å­˜å‚¨ä½œä¸ºå‚æ•°ã€‚å¦‚æœä¼ å…¥çš„å¯¹è±¡æ²¡æœ‰åˆ†é…åœ¨GPUä¸Šï¼Œè¿™ä¸ªæ“ä½œæ˜¯æ— æ•ˆçš„ã€‚\ntorch.cuda.empty_cache() é‡Šæ”¾caching allocatorå½“å‰æŒæœ‰çš„æ‰€æœ‰æœªå ç”¨çš„cached memoryï¼Œä½¿å…¶å¯ä»¥ç”¨åœ¨å…¶ä»–GPUåº”ç”¨ä¸”å¯ä»¥åœ¨ nvidia-smiå¯è§†åŒ–ã€‚\næ³¨æ„ï¼šempty_cache() å¹¶ä¸ä¼šå¢åŠ PyTorchå¯ä»¥ä½¿ç”¨çš„GPUæ˜¾å­˜çš„å¤§å°ã€‚ æŸ¥çœ‹ Memory management æ¥è·å–æ›´å¤šçš„GPUæ˜¾å­˜ç®¡ç†çš„ä¿¡æ¯ã€‚\ntorch.cuda.get_device_capability(device=None) Gets the cuda capability of a device.\nParametersï¼šdevice (torch.device or int, optional) â€“ device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given bycurrent_device(), if device is None (default).\nReturnsï¼šthe major and minor cuda capability of the device\nReturn type ï¼š tuple(int, int)\ntorch.cuda.get_device_name(device=None)\ntorch.cuda.init() åˆå§‹åŒ–PyTorchçš„CUDAçŠ¶æ€ã€‚å¦‚æœä½ é€šè¿‡C APIä¸PyTorchè¿›è¡Œäº¤äº’ï¼Œä½ å¯èƒ½éœ€è¦æ˜¾å¼è°ƒç”¨è¿™ä¸ªæ–¹æ³•ã€‚åªæœ‰CUDAçš„åˆå§‹åŒ–å®Œæˆï¼ŒCUDAçš„åŠŸèƒ½æ‰ä¼šç»‘å®šåˆ°Pythonã€‚ç”¨æˆ·ä¸€èˆ¬ä¸åº”è¯¥éœ€è¦è¿™ä¸ªï¼Œå› ä¸ºæ‰€æœ‰PyTorchçš„CUDAæ–¹æ³•éƒ½ä¼šè‡ªåŠ¨åœ¨éœ€è¦çš„æ—¶å€™åˆå§‹åŒ–CUDAã€‚å¦‚æœCUDAçš„çŠ¶æ€å·²ç»åˆå§‹åŒ–äº†ï¼Œå°†ä¸èµ·ä»»ä½•ä½œç”¨ã€‚\n[torch.cuda.is_available()]\ntorch.cuda.max_memory_allocated(device=None) Returns the maximum GPU memory occupied by tensors in bytes for a given device.\ntorch.cuda.max_memory_cached(device=None)\ntorch.cuda.memory_allocated(device=None) Parametersï¼šdevice (torch.device or int, optional) â€“ selected device. Returns statistic for the current device, given by current_device(), if device is None (default).\ntorch.cuda.memory_cached(devide=None)\n[``]\n",
  "wordCount" : "2394",
  "inLanguage": "en",
  "datePublished": "2022-06-09T19:14:27+08:00",
  "dateModified": "2022-06-09T19:14:27+08:00",
  "author":[{
    "@type": "Person",
    "name": "Jian"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jian's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jianye0428.github.io/favicon/jian_icon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jianye0428.github.io/en/" accesskey="h" title="Jian&#39;s Blog (Alt + H)">
                <img src="https://jianye0428.github.io/favicon/jian_icon.png" alt="logo" aria-label="logo"
                    height="30">Jian&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://jianye0428.github.io/cn/" title="Chinese"
                            aria-label="Chinese">Chinese</a>
                    </li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jianye0428.github.io/en/myresume/" title="My Resume">
                    <span>My Resume</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/tags/" title="ğŸ”–Tags">
                    <span>ğŸ”–Tags</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/archives" title="ğŸ™‹ğŸ»â€â™‚ï¸Archive">
                    <span>ğŸ™‹ğŸ»â€â™‚ï¸Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/search/" title="ğŸ”Search (Alt &#43; /)" accesskey=/>
                    <span>ğŸ”Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jianye0428.github.io/en/">Home</a>&nbsp;Â»&nbsp;<a href="https://jianye0428.github.io/en/posts/">Posts</a></div>
    <h1 class="post-title">
      PyTorch Notes
    </h1>
    <div class="post-meta"><span title='2022-06-09 19:14:27 +0800 CST'>2022-06-09</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;Jian&nbsp;|&nbsp;<a href="https://github.com/jianye0428/myblog/tree/main/content/posts/notes/2022-06-09_Pytorch.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#torch-%e5%9f%ba%e6%9c%ac%e5%87%bd%e6%95%b0" aria-label="Torch åŸºæœ¬å‡½æ•°">Torch åŸºæœ¬å‡½æ•°</a><ul>
                            
                    <li>
                        <a href="#1-torcheinsum" aria-label="1. torch.einsum()">1. <strong><code>torch.einsum()</code></strong></a></li>
                    <li>
                        <a href="#2-torchpermutetorchtranspose" aria-label="2. torch.permute()/torch.transpose()">2. <strong><code>torch.permute()/torch.transpose()</code></strong></a></li>
                    <li>
                        <a href="#3-torchrand" aria-label="3. torch.rand()">3. <strong><code>torch.rand()</code></strong></a></li>
                    <li>
                        <a href="#4-torchsizetorchshape" aria-label="4. torch.size()/torch.shape">4. <strong><code>torch.size()/torch.shape</code></strong></a></li>
                    <li>
                        <a href="#5-torchtensordot" aria-label="5. torch.tensordot()">5. <strong><code>torch.tensordot()</code></strong></a></li>
                    <li>
                        <a href="#6-torchtranspose" aria-label="6. torch.transpose()">6. <strong><code>torch.transpose()</code></strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#torch-nn-module" aria-label="Torch NN Module">Torch NN Module</a><ul>
                            
                    <li>
                        <a href="#1-nnconv1d" aria-label="1. nn.Conv1d()">1. <strong><code>nn.Conv1d()</code></strong></a></li>
                    <li>
                        <a href="#2-nnconv2d" aria-label="2. nn.Conv2d()">2. <strong><code>nn.Conv2d()</code></strong></a></li>
                    <li>
                        <a href="#3-nnfunctionalinterpolate" aria-label="3. nn.functional.interpolate()">3. <strong><code>nn.functional.interpolate()</code></strong></a></li>
                    <li>
                        <a href="#4-nnfunctionalrelu" aria-label="4. nn.functional.ReLU()">4. <strong><code>nn.functional.ReLU()</code></strong></a></li>
                    <li>
                        <a href="#5-nnmaxpool2d" aria-label="5. nn.MaxPool2d()">5. <strong><code>nn.MaxPool2d()</code></strong></a></li>
                    <li>
                        <a href="#6-nnavgpool2d" aria-label="6. nn.AvgPool2d()">6. <strong><code>nn.AvgPool2d()</code></strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#torchcuda" aria-label="torch.cuda">torch.cuda</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="torch-åŸºæœ¬å‡½æ•°">Torch åŸºæœ¬å‡½æ•°<a hidden class="anchor" aria-hidden="true" href="#torch-åŸºæœ¬å‡½æ•°">#</a></h2>
<h3 id="1-torcheinsum">1. <strong><code>torch.einsum()</code></strong><a hidden class="anchor" aria-hidden="true" href="#1-torcheinsum">#</a></h3>
<p><code>torch.einsum(equation, *operands)-&gt;Tensor</code>:çˆ±å› æ–¯å¦æ±‚å’Œ
ref1: ç®—å­éƒ¨ç½²: <a href="https://blog.csdn.net/HW140701/article/details/120654252">https://blog.csdn.net/HW140701/article/details/120654252</a>
ref2: ä¾‹å­: <a href="https://zhuanlan.zhihu.com/p/361209187">https://zhuanlan.zhihu.com/p/361209187</a></p>
<p><strong>ä¸‰æ¡åŸºæœ¬è§„åˆ™:</strong></p>
<ul>
<li><strong>è§„åˆ™ä¸€:</strong> equation ç®­å¤´å·¦è¾¹ï¼Œåœ¨ä¸åŒè¾“å…¥ä¹‹é—´<font color=red>é‡å¤å‡ºç°çš„ç´¢å¼•</font>è¡¨ç¤ºï¼ŒæŠŠè¾“å…¥å¼ é‡æ²¿ç€è¯¥ç»´åº¦åšä¹˜æ³•æ“ä½œï¼Œæ¯”å¦‚è¿˜æ˜¯ä»¥ä¸Šé¢çŸ©é˜µä¹˜æ³•ä¸ºä¾‹ï¼Œ &ldquo;ik,kj-&gt;ij&rdquo;ï¼Œk åœ¨è¾“å…¥ä¸­é‡å¤å‡ºç°ï¼Œæ‰€ä»¥å°±æ˜¯æŠŠ a å’Œ b æ²¿ç€ k è¿™ä¸ªç»´åº¦ä½œç›¸ä¹˜æ“ä½œï¼›</li>
<li><strong>è§„åˆ™äºŒ:</strong> åªå‡ºç°åœ¨ equation ç®­å¤´å·¦è¾¹çš„ç´¢å¼•ï¼Œè¡¨ç¤ºä¸­é—´è®¡ç®—ç»“æœéœ€è¦åœ¨è¿™ä¸ªç»´åº¦ä¸Šæ±‚å’Œï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢æåˆ°çš„æ±‚å’Œç´¢å¼•ï¼›</li>
<li><strong>è§„åˆ™ä¸‰:</strong> equation ç®­å¤´å³è¾¹çš„ç´¢å¼•é¡ºåºå¯ä»¥æ˜¯ä»»æ„çš„ï¼Œæ¯”å¦‚ä¸Šé¢çš„ &ldquo;ik,kj-&gt;ij&rdquo; å¦‚æœå†™æˆ &ldquo;ik,kj-&gt;ji&rdquo;ï¼Œé‚£ä¹ˆå°±æ˜¯è¿”å›è¾“å‡ºç»“æœçš„è½¬ç½®ï¼Œç”¨æˆ·åªéœ€è¦å®šä¹‰å¥½ç´¢å¼•çš„é¡ºåºï¼Œè½¬ç½®æ“ä½œä¼šåœ¨ einsum å†…éƒ¨å®Œæˆ</li>
</ul>
<p><strong>ç‰¹æ®Šè§„åˆ™:</strong></p>
<ul>
<li>equation å¯ä»¥ä¸å†™åŒ…æ‹¬ç®­å¤´åœ¨å†…çš„å³è¾¹éƒ¨åˆ†ï¼Œé‚£ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å‡ºå¼ é‡çš„ç»´åº¦ä¼šæ ¹æ®é»˜è®¤è§„åˆ™æ¨å¯¼ã€‚å°±æ˜¯æŠŠè¾“å…¥ä¸­åªå‡ºç°ä¸€æ¬¡çš„ç´¢å¼•å–å‡ºæ¥ï¼Œç„¶åæŒ‰å­—æ¯è¡¨é¡ºåºæ’åˆ—ï¼Œæ¯”å¦‚ä¸Šé¢çš„çŸ©é˜µä¹˜æ³• &ldquo;ik,kj-&gt;ij&rdquo; ä¹Ÿå¯ä»¥ç®€åŒ–ä¸º &ldquo;ik,kj&rdquo;ï¼Œæ ¹æ®é»˜è®¤è§„åˆ™ï¼Œè¾“å‡ºå°±æ˜¯ &ldquo;ij&rdquo; ä¸åŸæ¥ä¸€æ ·ï¼›</li>
<li>equation ä¸­æ”¯æŒ &ldquo;&hellip;&rdquo; çœç•¥å·ï¼Œç”¨äºè¡¨ç¤ºç”¨æˆ·å¹¶ä¸å…³å¿ƒçš„ç´¢å¼•ã€‚æ¯”å¦‚åªå¯¹ä¸€ä¸ªé«˜ç»´å¼ é‡çš„æœ€åä¸¤ç»´åšè½¬ç½®å¯ä»¥è¿™ä¹ˆå†™ï¼š
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">a</span> <span class="o">=</span> torch.randn<span class="o">(</span>2,3,5,7,9<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># i = 7, j = 9</span>
</span></span><span class="line"><span class="cl"><span class="nv">b</span> <span class="o">=</span> torch.einsum<span class="o">(</span><span class="s1">&#39;...ij-&gt;...ji&#39;</span>, <span class="o">[</span>a<span class="o">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="2-torchpermutetorchtranspose">2. <strong><code>torch.permute()/torch.transpose()</code></strong><a hidden class="anchor" aria-hidden="true" href="#2-torchpermutetorchtranspose">#</a></h3>
<p><code>torch.permute(dim0, dim1, dim2)</code>:ç”¨äºè°ƒæ¢ä¸åŒç»´åº¦çš„é¡ºåº
<code>torch.transpose(input, dim0, dim1)</code>:äº¤æ¢çŸ©é˜µçš„ä¸¤ä¸ªç»´åº¦</p>
<h3 id="3-torchrand">3. <strong><code>torch.rand()</code></strong><a hidden class="anchor" aria-hidden="true" href="#3-torchrand">#</a></h3>
<p><code>torch.rand(dim0, dim1)</code>:ç”Ÿæˆdim0 x dim1çš„tensor</p>
<h3 id="4-torchsizetorchshape">4. <strong><code>torch.size()/torch.shape</code></strong><a hidden class="anchor" aria-hidden="true" href="#4-torchsizetorchshape">#</a></h3>
<p><code>torch.size()</code>:è¿”å›tensorçš„size
<code>torch.shape</code>:è¿”å›tensorçš„size</p>
<h3 id="5-torchtensordot">5. <strong><code>torch.tensordot()</code></strong><a hidden class="anchor" aria-hidden="true" href="#5-torchtensordot">#</a></h3>
<p>ref: tensordot()å’Œeinsum()çš„åŒºåˆ«: <a href="https://blog.csdn.net/Eric_1993/article/details/105670381">https://blog.csdn.net/Eric_1993/article/details/105670381</a>
<code>torch.tensordot(tensor1ï¼Œ tensor2ï¼Œ axes=([dim1,dim2],[dim0, dim1]))</code>: å°†axesæŒ‡å®šçš„å­æ•°ç»„è¿›è¡Œç‚¹ä¹˜, axes æŒ‡å®šå…·ä½“çš„ç»´åº¦.</p>
<h3 id="6-torchtranspose">6. <strong><code>torch.transpose()</code></strong><a hidden class="anchor" aria-hidden="true" href="#6-torchtranspose">#</a></h3>
<p><code>torch.transpose(tensor, dim0, dim2) â€”&gt; Tensor</code>:åœ¨dim0å’Œdim1æ–¹å‘ä¸Šè½¬ç½®</p>
<p>###7. <strong><code>torch.index_add_()</code></strong></p>
<p><code> Tensor.index_add_(dim, index, source, *, alpha=1) â†’ Tensor</code></p>
<p>demo:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span><span class="p">([[</span>  <span class="mf">2.</span><span class="p">,</span>   <span class="mf">3.</span><span class="p">,</span>   <span class="mf">4.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">8.</span><span class="p">,</span>   <span class="mf">9.</span><span class="p">,</span>  <span class="mf">10.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">5.</span><span class="p">,</span>   <span class="mf">6.</span><span class="p">,</span>   <span class="mf">7.</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span><span class="p">([[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="torch-nn-module">Torch NN Module<a hidden class="anchor" aria-hidden="true" href="#torch-nn-module">#</a></h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="1-nnconv1d">1. <strong><code>nn.Conv1d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#1-nnconv1d">#</a></h3>
<p><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code></p>
<p><strong>Shape:</strong>
- Input: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$
- Output: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$, where
$$L_{out} = \frac{L_{in} + 2 \cdot \text{padding} - \text{dilation} \cdot (\text{kernel_size} - 1) - 1}{stride}$$</p>
<p><strong>Demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span> <span class="c1"># B x C x H or N x C x L</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># torch.Size([20, 33, 24])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="2-nnconv2d">2. <strong><code>nn.Conv2d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#2-nnconv2d">#</a></h3>
<p><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code></p>
<p><strong>Shape:</strong></p>
<ul>
<li>Input: $(N, C_{\text in}, H_{\text in}, W_{\text in})$ or $(C_{\text in}, H_{\text in}, W_{\text in})$
- Output: $(N, C_{\text out}, H_{\text out}, W_{\text out})$ or $(C_{\text out}, H_{\text out}, W_{\text out})$, where
$$
H_{out} = \frac{H_{in} + 2 \cdot \text{padding[0]} - \text{dilation[0]} \cdot (\text{kernel_size[0]} - 1) - 1}{stride[0]} + 1
$$
$$
W_{out} = \frac{W_{in} + 2 \cdot \text{padding[1]} - \text{dilation[1]} \cdot (\text{kernel_size[1]} - 1) - 1}{stride[1]} + 1
$$</li>
</ul>
<p><strong>Demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># With square kernels and equal stride</span>
</span></span><span class="line"><span class="cl">  <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># non-square kernels and unequal stride and with padding</span>
</span></span><span class="line"><span class="cl">  <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># output.shape: 20 x 33 x 28 x 100</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
</span></span><span class="line"><span class="cl">  <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># output.shape: 20 x 33 x 26 x 100</span>
</span></span><span class="line"><span class="cl">  <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># </span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="3-nnfunctionalinterpolate">3. <strong><code>nn.functional.interpolate()</code></strong><a hidden class="anchor" aria-hidden="true" href="#3-nnfunctionalinterpolate">#</a></h3>
<p><code>torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False)</code></p>
<h3 id="4-nnfunctionalrelu">4. <strong><code>nn.functional.ReLU()</code></strong><a hidden class="anchor" aria-hidden="true" href="#4-nnfunctionalrelu">#</a></h3>
<p>$$ \text{ReLU} = (x)^+ = \max {(0,x)}$$</p>
<p><code>torch.nn.ReLU(inplace=False)</code></p>
<p><strong>ä½œç”¨:</strong></p>
<ul>
<li>
<p>Sigmoidçš„å¯¼æ•°åªæœ‰åœ¨0é™„è¿‘çš„æ—¶å€™æœ‰æ¯”è¾ƒå¥½çš„æ¿€æ´»æ€§ï¼Œåœ¨æ­£è´Ÿé¥±å’ŒåŒºçš„æ¢¯åº¦éƒ½æ¥è¿‘äº0ï¼Œæ‰€ä»¥è¿™ä¼šé€ æˆæ¢¯åº¦å¼¥æ•£ï¼Œè€ŒReLUå‡½æ•°åœ¨å¤§äº0çš„éƒ¨åˆ†æ¢¯åº¦ä¸ºå¸¸æ•°ï¼Œæ‰€ä»¥ä¸ä¼šäº§ç”Ÿæ¢¯åº¦å¼¥æ•£ç°è±¡ã€‚</p>
</li>
<li>
<p>ReLUå‡½æ•°åœ¨è´ŸåŠåŒºçš„å¯¼æ•°ä¸º0 ï¼Œæ‰€ä»¥ä¸€æ—¦ç¥ç»å…ƒæ¿€æ´»å€¼è¿›å…¥è´ŸåŠåŒºï¼Œé‚£ä¹ˆæ¢¯åº¦å°±ä¼šä¸º0ï¼Œè€Œæ­£å€¼ä¸å˜ï¼Œè¿™ç§æ“ä½œè¢«æˆä¸ºå•ä¾§æŠ‘åˆ¶ã€‚ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼š<strong>åœ¨è¾“å…¥æ˜¯è´Ÿå€¼çš„æƒ…å†µä¸‹ï¼Œå®ƒä¼šè¾“å‡º0ï¼Œé‚£ä¹ˆç¥ç»å…ƒå°±ä¸ä¼šè¢«æ¿€æ´»ã€‚è¿™æ„å‘³ç€åŒä¸€æ—¶é—´åªæœ‰éƒ¨åˆ†ç¥ç»å…ƒä¼šè¢«æ¿€æ´»ï¼Œä»è€Œä½¿å¾—ç½‘ç»œå¾ˆç¨€ç–ï¼Œè¿›è€Œå¯¹è®¡ç®—æ¥è¯´æ˜¯éå¸¸æœ‰æ•ˆç‡çš„ã€‚</strong>ï¼‰<u>æ­£å› ä¸ºæœ‰äº†è¿™å•ä¾§æŠ‘åˆ¶ï¼Œæ‰ä½¿å¾—ç¥ç»ç½‘ç»œä¸­çš„ç¥ç»å…ƒä¹Ÿå…·æœ‰äº†ç¨€ç–æ¿€æ´»æ€§ã€‚</u>å°¤å…¶ä½“ç°åœ¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹(å¦‚CNN)ä¸­ï¼Œå½“æ¨¡å‹å¢åŠ Nå±‚ä¹‹åï¼Œç†è®ºä¸ŠReLUç¥ç»å…ƒçš„æ¿€æ´»ç‡å°†é™ä½2çš„Næ¬¡æ–¹å€ã€‚</p>
</li>
<li>
<p>reluå‡½æ•°çš„å¯¼æ•°<strong>è®¡ç®—æ›´å¿«</strong>ï¼Œç¨‹åºå®ç°å°±æ˜¯ä¸€ä¸ªif-elseè¯­å¥ï¼Œè€Œsigmoidå‡½æ•°è¦è¿›è¡Œæµ®ç‚¹å››åˆ™è¿ç®—ã€‚</p>
</li>
</ul>
<p><strong>Shape:</strong></p>
<ul>
<li>Input: $(âˆ—)$, where $*$ means any number of dimensions.</li>
<li>Output: $(âˆ—)$, same shape as the input.</li>
</ul>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-06-09_PyTorch/ReLU.png" alt="æ‰§è¡Œecho $PATHçš„ç»“æœ"  />
</p>
<p><strong>Demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># An implementation of CReLU - https://arxiv.org/abs/1603.05201</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span><span class="n">m</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="5-nnmaxpool2d">5. <strong><code>nn.MaxPool2d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#5-nnmaxpool2d">#</a></h3>
<p><code>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></p>
<p><strong>Shape:</strong></p>
<ul>
<li>Input: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$</li>
<li>Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$</li>
</ul>
<p>where,</p>
<p>$$ H_{out} = \frac{H_{in} + 2 * \text{padding}[0] - \text{dilation}[0] * (\text{kernel_size}[0]-1) - 1}{\text{stride}[0]} + 1$$</p>
<p>$$ W_{out} = \frac{W_{in} + 2 * \text{padding}[1] - \text{dilation}[1] * (\text{kernel_size}[1]-1) - 1}{\text{stride}[1]} + 1$$</p>
<p><strong>demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># pool of square window of size=3, stride=2</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># pool of non-square window</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># 20 16 24 31</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="6-nnavgpool2d">6. <strong><code>nn.AvgPool2d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#6-nnavgpool2d">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Shape:</strong></p>
<ul>
<li>Input: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$</li>
<li>Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$</li>
</ul>
<p>where,</p>
<p>$$ H_{out} = \frac{H_{in} + 2 * \text{padding}[0] -  (\text{kernel_size}[0])}{\text{stride}[0]} + 1$$</p>
<p>$$ W_{out} = \frac{W_{in} + 2 * \text{padding}[1] - (\text{kernel_size}[1])}{\text{stride}[1]} + 1$$</p>
<p><strong>demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># pool of square window of size=3, stride=2</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># pool of non-square window</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># 20 16, 24 31</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="torchcuda">torch.cuda<a hidden class="anchor" aria-hidden="true" href="#torchcuda">#</a></h2>
<p>ref link: <a href="https://zhuanlan.zhihu.com/p/76908135">https://zhuanlan.zhihu.com/p/76908135</a></p>
<ol>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_device"><code>torch.cuda.current_device()</code></a>: è¿”å›å½“å‰é€‰æ‹©çš„è®¾å¤‡çš„ç´¢å¼•</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_stream"><code>torch.cuda.current_stream()</code></a>: è¿”å›å‚æ•°è®¾å¤‡çš„å½“å‰çš„<a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.Stream">Stream</a></p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_stream"><code>torch.cuda.default_stream()</code></a>: è¿”å›å½“å‰å‚æ•°è®¾å¤‡çš„<a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.Stream">Stream</a></p>
</li>
<li>
<p><em>CLASS</em> <a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#device"><code>torch.cuda.device</code></a>: å¯ä»¥æ”¹å˜é€‰æ‹©çš„è®¾å¤‡çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
Parametersï¼šdevice (torch.device or int) â€“ device index to select. Itâ€™s a no-op if this argument is a negative integer or None.</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#device_count"><code>torch.cuda.device_count()</code></a>: è¿”å›å¯ä½¿ç”¨GPUçš„æ•°é‡</p>
</li>
<li>
<p><em>CLASS</em> <a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#device_of"><code>torch.cuda.device_of(obj)</code></a>
Context-manager å°†å‚æ•°å¯¹è±¡çš„è®¾å¤‡æ”¹æˆå½“å‰çš„è®¾å¤‡ã€‚ä½ å¯ä»¥ä½¿ç”¨å¼ é‡æˆ–è€…å­˜å‚¨ä½œä¸ºå‚æ•°ã€‚å¦‚æœä¼ å…¥çš„å¯¹è±¡æ²¡æœ‰åˆ†é…åœ¨GPUä¸Šï¼Œè¿™ä¸ªæ“ä½œæ˜¯æ— æ•ˆçš„ã€‚</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#empty_cache"><code>torch.cuda.empty_cache()</code></a>
é‡Šæ”¾caching allocatorå½“å‰æŒæœ‰çš„æ‰€æœ‰æœªå ç”¨çš„cached memoryï¼Œä½¿å…¶å¯ä»¥ç”¨åœ¨å…¶ä»–GPUåº”ç”¨ä¸”å¯ä»¥åœ¨ nvidia-smiå¯è§†åŒ–ã€‚</p>
<blockquote>
<blockquote>
<p>æ³¨æ„ï¼š<a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.empty_cache">empty_cache()</a> å¹¶ä¸ä¼šå¢åŠ PyTorchå¯ä»¥ä½¿ç”¨çš„GPUæ˜¾å­˜çš„å¤§å°ã€‚ æŸ¥çœ‹ <a href="https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management">Memory management</a> æ¥è·å–æ›´å¤šçš„GPUæ˜¾å­˜ç®¡ç†çš„ä¿¡æ¯ã€‚</p>
</blockquote>
</blockquote>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#get_device_capability"><code>torch.cuda.get_device_capability(device=None)</code></a>
Gets the cuda capability of a device.</p>
<p>Parametersï¼šdevice (torch.device or int, optional) â€“ device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given bycurrent_device(), if device is None (default).</p>
<p>Returnsï¼šthe major and minor cuda capability of the device</p>
<p>Return type ï¼š tuple(int, int)</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#get_device_name"><code>torch.cuda.get_device_name(device=None)</code></a></p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#init"><code>torch.cuda.init()</code></a>
åˆå§‹åŒ–PyTorchçš„CUDAçŠ¶æ€ã€‚å¦‚æœä½ é€šè¿‡C APIä¸PyTorchè¿›è¡Œäº¤äº’ï¼Œä½ å¯èƒ½éœ€è¦æ˜¾å¼è°ƒç”¨è¿™ä¸ªæ–¹æ³•ã€‚åªæœ‰CUDAçš„åˆå§‹åŒ–å®Œæˆï¼ŒCUDAçš„åŠŸèƒ½æ‰ä¼šç»‘å®šåˆ°Pythonã€‚ç”¨æˆ·ä¸€èˆ¬ä¸åº”è¯¥éœ€è¦è¿™ä¸ªï¼Œå› ä¸ºæ‰€æœ‰PyTorchçš„CUDAæ–¹æ³•éƒ½ä¼šè‡ªåŠ¨åœ¨éœ€è¦çš„æ—¶å€™åˆå§‹åŒ–CUDAã€‚å¦‚æœCUDAçš„çŠ¶æ€å·²ç»åˆå§‹åŒ–äº†ï¼Œå°†ä¸èµ·ä»»ä½•ä½œç”¨ã€‚</p>
</li>
<li>
<p>[<code>torch.cuda.is_available()</code>]</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#max_memory_allocated"><code>torch.cuda.max_memory_allocated(device=None)</code></a>
Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#max_memory_cached"><code>torch.cuda.max_memory_cached(device=None)</code></a></p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#memory_allocated"><code>torch.cuda.memory_allocated(device=None)</code></a>
Parametersï¼šdevice (torch.device or int, optional) â€“ selected device. Returns statistic for the current device, given by current_device(), if device is None (default).</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#memory_cached"><code>torch.cuda.memory_cached(devide=None)</code></a></p>
</li>
<li>
<p>[``]</p>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jianye0428.github.io/en/tags/pytorch/">PyTorch</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jianye0428.github.io/en/posts/tech/2022-06-12_social_nce/">
    <span class="title"><i class="fas fa-angle-double-left"></i> Prev Page</span>
    <br>
    <span>Social_NCE: Contrastive Learning of Socially-aware Motion Representation</span>
  </a>
  <a class="next" href="https://jianye0428.github.io/en/posts/tech/2022-06-08_social_stgcnn/">
    <span class="title">Next Page <i class="fas fa-angle-double-right"></i></span>
    <br>
    <span>Social_STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on twitter"
        href="https://twitter.com/intent/tweet/?text=PyTorch%20Notes&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f&amp;hashtags=PyTorch">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f&amp;title=PyTorch%20Notes&amp;summary=PyTorch%20Notes&amp;source=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f&title=PyTorch%20Notes">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on whatsapp"
        href="https://api.whatsapp.com/send?text=PyTorch%20Notes%20-%20https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on telegram"
        href="https://telegram.me/share/url?text=PyTorch%20Notes&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>




<footer class="tc-container" id="comment">
    <div class="tc-title"><p class="c-title">Discussion</p></div>
    <div id="tcomments"></div>
</footer>
<script crossorigin="anonymous" src="/js/twikoo.min.b16100b7cf8a61759eab076a122482054e083087aad37c3be1fe2e293934dc34.js" integrity="sha256-sWEAt8&#43;KYXWeqwdqEiSCBU4IMIeq03w74f4uKTk03DQ="></script>
<script>
    twikoo.init({
        envId: 'https://my-repository-pink.vercel.app/',
        el: '#tcomments',
        region: 'ap-shanghai', 
        
        lang: 'zh-CN', 
    });
</script>

</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
