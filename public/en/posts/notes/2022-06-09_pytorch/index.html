<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>PyTorch Notes | Jian&#39;s Blog</title>
<meta name="keywords" content="PyTorch">
<meta name="description" content="Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)-&gt;Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入">
<meta name="author" content="Jian">
<link rel="canonical" href="https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.1ea9c8832138446635789668415e5c75b8a534b191ee749a44f5ab404c9f27c2.css" integrity="sha256-HqnIgyE4RGY1eJZoQV5cdbilNLGR7nSaRPWrQEyfJ8I=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="apple-touch-icon" href="https://jianye0428.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://jianye0428.github.io/favicon/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="baidu-site-verification" content="code-9oLyeix0aK" />
<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4a41bf85d719f0e8c3165fc76904f546";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>



<script defer crossorigin="anonymous" src="/js/katex.min.8f5024e83d2055dd60e021751066111b0057e230db34911dd56242d67f0a4c86.js" integrity="sha256-j1Ak6D0gVd1g4CF1EGYRGwBX4jDbNJEd1WJC1n8KTIY="></script>


<script defer crossorigin="anonymous" src="/js/auto-render.min.b09accad850e4e87b8a2fc8b93fae790def79172b68de72fd777958c52e566ad.js" integrity="sha256-sJrMrYUOToe4ovyLk/rnkN73kXK2jecv13eVjFLlZq0="></script>

<script>
    
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });

    
    window.WebFontConfig = {
        custom: {
            families: ['KaTeX_AMS', 'KaTeX_Caligraphic:n4,n7', 'KaTeX_Fraktur:n4,n7',
            'KaTeX_Main:n4,n7,i4,i7', 'KaTeX_Math:i4,i7', 'KaTeX_Script',
            'KaTeX_SansSerif:n4,n7,i4', 'KaTeX_Size1', 'KaTeX_Size2', 'KaTeX_Size3',
            'KaTeX_Size4', 'KaTeX_Typewriter'],
        },
    };
</script>


<script defer crossorigin="anonymous" src="/js/webfontloader.min.min.d1c6c39d18e2decb5c99dc9efc579098ab37b9654725df3f9c0737bc2dd00760.js" integrity="sha256-0cbDnRji3stcmdye/FeQmKs3uWVHJd8/nAc3vC3QB2A="></script>


 

<script async src="https://www.googletagmanager.com/gtag/js?id=G-C6GDZ56F4S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-C6GDZ56F4S', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="PyTorch Notes" />
<meta property="og:description" content="Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)-&gt;Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/" /><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-09T19:14:27&#43;08:00" />
<meta property="article:modified_time" content="2022-06-09T19:14:27&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/>

<meta name="twitter:title" content="PyTorch Notes"/>
<meta name="twitter:description" content="Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)-&gt;Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jianye0428.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "PyTorch Notes",
      "item": "https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "PyTorch Notes",
  "name": "PyTorch Notes",
  "description": "Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)-\u0026gt;Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187 三条基本规则: 规则一: equation 箭头左边，在不同输入",
  "keywords": [
    "PyTorch"
  ],
  "articleBody": "Torch 基本函数 1. torch.einsum() torch.einsum(equation, *operands)-\u003eTensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187\n三条基本规则:\n规则一: equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， “ik,kj-\u003eij”，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作； 规则二: 只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引； 规则三: equation 箭头右边的索引顺序可以是任意的，比如上面的 “ik,kj-\u003eij” 如果写成 “ik,kj-\u003eji”，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成 特殊规则:\nequation 可以不写包括箭头在内的右边部分，那么在这种情况下，输出张量的维度会根据默认规则推导。就是把输入中只出现一次的索引取出来，然后按字母表顺序排列，比如上面的矩阵乘法 “ik,kj-\u003eij” 也可以简化为 “ik,kj”，根据默认规则，输出就是 “ij” 与原来一样； equation 中支持 “…” 省略号，用于表示用户并不关心的索引。比如只对一个高维张量的最后两维做转置可以这么写： 1 2 3 a = torch.randn(2,3,5,7,9) # i = 7, j = 9 b = torch.einsum('...ij-\u003e...ji', [a]) 2. torch.permute()/torch.transpose() torch.permute(dim0, dim1, dim2):用于调换不同维度的顺序 torch.transpose(input, dim0, dim1):交换矩阵的两个维度\n3. torch.rand() torch.rand(dim0, dim1):生成dim0 x dim1的tensor\n4. torch.size()/torch.shape torch.size():返回tensor的size torch.shape:返回tensor的size\n5. torch.tensordot() ref: tensordot()和einsum()的区别: https://blog.csdn.net/Eric_1993/article/details/105670381 torch.tensordot(tensor1， tensor2， axes=([dim1,dim2],[dim0, dim1])): 将axes指定的子数组进行点乘, axes 指定具体的维度.\n6. torch.transpose() torch.transpose(tensor, dim0, dim2) —\u003e Tensor:在dim0和dim1方向上转置\n###7. torch.index_add_()\nTensor.index_add_(dim, index, source, *, alpha=1) → Tensor\ndemo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u003e\u003e\u003e x = torch.ones(5, 3) \u003e\u003e\u003e t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) \u003e\u003e\u003e index = torch.tensor([0, 4, 2]) \u003e\u003e\u003e x.index_add_(0, index, t) tensor([[ 2., 3., 4.], [ 1., 1., 1.], [ 8., 9., 10.], [ 1., 1., 1.], [ 5., 6., 7.]]) \u003e\u003e\u003e x.index_add_(0, index, t, alpha=-1) tensor([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) Torch NN Module 1 2 3 import torch from torch import nn from torch import functional as F 1. nn.Conv1d() torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\nShape: - Input: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$ - Output: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$, where $$L_{out} = \\frac{L_{in} + 2 \\cdot \\text{padding} - \\text{dilation} \\cdot (\\text{kernel_size} - 1) - 1}{stride}$$\nDemo:\n1 2 3 4 m = nn.Conv1d(16, 33, 3, stride=2) input = torch.randn(20, 16, 50) # B x C x H or N x C x L output = m(input) print(output.shape) # torch.Size([20, 33, 24]) 2. nn.Conv2d() torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\nShape:\nInput: $(N, C_{\\text in}, H_{\\text in}, W_{\\text in})$ or $(C_{\\text in}, H_{\\text in}, W_{\\text in})$ - Output: $(N, C_{\\text out}, H_{\\text out}, W_{\\text out})$ or $(C_{\\text out}, H_{\\text out}, W_{\\text out})$, where $$ H_{out} = \\frac{H_{in} + 2 \\cdot \\text{padding[0]} - \\text{dilation[0]} \\cdot (\\text{kernel_size[0]} - 1) - 1}{stride[0]} + 1 $$ $$ W_{out} = \\frac{W_{in} + 2 \\cdot \\text{padding[1]} - \\text{dilation[1]} \\cdot (\\text{kernel_size[1]} - 1) - 1}{stride[1]} + 1 $$ Demo:\n1 2 3 4 5 6 7 8 # With square kernels and equal stride m = nn.Conv2d(16, 33, 3, stride=2) # non-square kernels and unequal stride and with padding m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) # output.shape: 20 x 33 x 28 x 100 # non-square kernels and unequal stride and with padding and dilation m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) # output.shape: 20 x 33 x 26 x 100 input = torch.randn(20, 16, 50, 100) output = m(input) # 3. nn.functional.interpolate() torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False)\n4. nn.functional.ReLU() $$ \\text{ReLU} = (x)^+ = \\max {(0,x)}$$\ntorch.nn.ReLU(inplace=False)\n作用:\nSigmoid的导数只有在0附近的时候有比较好的激活性，在正负饱和区的梯度都接近于0，所以这会造成梯度弥散，而ReLU函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象。\nReLU函数在负半区的导数为0 ，所以一旦神经元激活值进入负半区，那么梯度就会为0，而正值不变，这种操作被成为单侧抑制。（也就是说：在输入是负值的情况下，它会输出0，那么神经元就不会被激活。这意味着同一时间只有部分神经元会被激活，从而使得网络很稀疏，进而对计算来说是非常有效率的。）正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。\nrelu函数的导数计算更快，程序实现就是一个if-else语句，而sigmoid函数要进行浮点四则运算。\nShape:\nInput: $(∗)$, where $*$ means any number of dimensions. Output: $(∗)$, same shape as the input. Demo:\n1 2 3 4 5 6 7 m = nn.ReLU() input = torch.randn(2) output = m(input) # An implementation of CReLU - https://arxiv.org/abs/1603.05201 m = nn.ReLU() input = torch.randn(2).unsqueeze(0) output = torch.cat((m(input),m(-input))) 5. nn.MaxPool2d() torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\nShape:\nInput: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$ Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$ where,\n$$ H_{out} = \\frac{H_{in} + 2 * \\text{padding}[0] - \\text{dilation}[0] * (\\text{kernel_size}[0]-1) - 1}{\\text{stride}[0]} + 1$$\n$$ W_{out} = \\frac{W_{in} + 2 * \\text{padding}[1] - \\text{dilation}[1] * (\\text{kernel_size}[1]-1) - 1}{\\text{stride}[1]} + 1$$\ndemo:\n1 2 3 4 5 6 # pool of square window of size=3, stride=2 m = nn.MaxPool2d(3, stride=2) # pool of non-square window m = nn.MaxPool2d((3, 2), stride=(2, 1)) input = torch.randn(20, 16, 50, 32) output = m(input) # 20 16 24 31 6. nn.AvgPool2d() 1 torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) Shape:\nInput: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$ Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$ where,\n$$ H_{out} = \\frac{H_{in} + 2 * \\text{padding}[0] - (\\text{kernel_size}[0])}{\\text{stride}[0]} + 1$$\n$$ W_{out} = \\frac{W_{in} + 2 * \\text{padding}[1] - (\\text{kernel_size}[1])}{\\text{stride}[1]} + 1$$\ndemo:\n1 2 3 4 5 6 # pool of square window of size=3, stride=2 m = nn.AvgPool2d(3, stride=2) # pool of non-square window m = nn.AvgPool2d((3, 2), stride=(2, 1)) input = torch.randn(20, 16, 50, 32) output = m(input) # 20 16, 24 31 torch.cuda ref link: https://zhuanlan.zhihu.com/p/76908135\ntorch.cuda.current_device(): 返回当前选择的设备的索引\ntorch.cuda.current_stream(): 返回参数设备的当前的Stream\ntorch.cuda.default_stream(): 返回当前参数设备的Stream\nCLASS torch.cuda.device: 可以改变选择的设备的上下文管理器 Parameters：device (torch.device or int) – device index to select. It’s a no-op if this argument is a negative integer or None.\ntorch.cuda.device_count(): 返回可使用GPU的数量\nCLASS torch.cuda.device_of(obj) Context-manager 将参数对象的设备改成当前的设备。你可以使用张量或者存储作为参数。如果传入的对象没有分配在GPU上，这个操作是无效的。\ntorch.cuda.empty_cache() 释放caching allocator当前持有的所有未占用的cached memory，使其可以用在其他GPU应用且可以在 nvidia-smi可视化。\n注意：empty_cache() 并不会增加PyTorch可以使用的GPU显存的大小。 查看 Memory management 来获取更多的GPU显存管理的信息。\ntorch.cuda.get_device_capability(device=None) Gets the cuda capability of a device.\nParameters：device (torch.device or int, optional) – device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given bycurrent_device(), if device is None (default).\nReturns：the major and minor cuda capability of the device\nReturn type ： tuple(int, int)\ntorch.cuda.get_device_name(device=None)\ntorch.cuda.init() 初始化PyTorch的CUDA状态。如果你通过C API与PyTorch进行交互，你可能需要显式调用这个方法。只有CUDA的初始化完成，CUDA的功能才会绑定到Python。用户一般不应该需要这个，因为所有PyTorch的CUDA方法都会自动在需要的时候初始化CUDA。如果CUDA的状态已经初始化了，将不起任何作用。\n[torch.cuda.is_available()]\ntorch.cuda.max_memory_allocated(device=None) Returns the maximum GPU memory occupied by tensors in bytes for a given device.\ntorch.cuda.max_memory_cached(device=None)\ntorch.cuda.memory_allocated(device=None) Parameters：device (torch.device or int, optional) – selected device. Returns statistic for the current device, given by current_device(), if device is None (default).\ntorch.cuda.memory_cached(devide=None)\n[``]\n",
  "wordCount" : "2394",
  "inLanguage": "en",
  "datePublished": "2022-06-09T19:14:27+08:00",
  "dateModified": "2022-06-09T19:14:27+08:00",
  "author":[{
    "@type": "Person",
    "name": "Jian"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jianye0428.github.io/en/posts/notes/2022-06-09_pytorch/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jian's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jianye0428.github.io/favicon/jian_icon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jianye0428.github.io/en/" accesskey="h" title="Jian&#39;s Blog (Alt + H)">
                <img src="https://jianye0428.github.io/favicon/jian_icon.png" alt="logo" aria-label="logo"
                    height="30">Jian&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://jianye0428.github.io/cn/" title="Chinese"
                            aria-label="Chinese">Chinese</a>
                    </li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jianye0428.github.io/en/myresume/" title="My Resume">
                    <span>My Resume</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/tags/" title="🔖Tags">
                    <span>🔖Tags</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/archives" title="🙋🏻‍♂️Archive">
                    <span>🙋🏻‍♂️Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/search/" title="🔍Search (Alt &#43; /)" accesskey=/>
                    <span>🔍Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jianye0428.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://jianye0428.github.io/en/posts/">Posts</a></div>
    <h1 class="post-title">
      PyTorch Notes
    </h1>
    <div class="post-meta"><span title='2022-06-09 19:14:27 +0800 CST'>2022-06-09</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Jian&nbsp;|&nbsp;<a href="https://github.com/jianye0428/myblog/tree/main/content/posts/notes/2022-06-09_Pytorch.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#torch-%e5%9f%ba%e6%9c%ac%e5%87%bd%e6%95%b0" aria-label="Torch 基本函数">Torch 基本函数</a><ul>
                            
                    <li>
                        <a href="#1-torcheinsum" aria-label="1. torch.einsum()">1. <strong><code>torch.einsum()</code></strong></a></li>
                    <li>
                        <a href="#2-torchpermutetorchtranspose" aria-label="2. torch.permute()/torch.transpose()">2. <strong><code>torch.permute()/torch.transpose()</code></strong></a></li>
                    <li>
                        <a href="#3-torchrand" aria-label="3. torch.rand()">3. <strong><code>torch.rand()</code></strong></a></li>
                    <li>
                        <a href="#4-torchsizetorchshape" aria-label="4. torch.size()/torch.shape">4. <strong><code>torch.size()/torch.shape</code></strong></a></li>
                    <li>
                        <a href="#5-torchtensordot" aria-label="5. torch.tensordot()">5. <strong><code>torch.tensordot()</code></strong></a></li>
                    <li>
                        <a href="#6-torchtranspose" aria-label="6. torch.transpose()">6. <strong><code>torch.transpose()</code></strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#torch-nn-module" aria-label="Torch NN Module">Torch NN Module</a><ul>
                            
                    <li>
                        <a href="#1-nnconv1d" aria-label="1. nn.Conv1d()">1. <strong><code>nn.Conv1d()</code></strong></a></li>
                    <li>
                        <a href="#2-nnconv2d" aria-label="2. nn.Conv2d()">2. <strong><code>nn.Conv2d()</code></strong></a></li>
                    <li>
                        <a href="#3-nnfunctionalinterpolate" aria-label="3. nn.functional.interpolate()">3. <strong><code>nn.functional.interpolate()</code></strong></a></li>
                    <li>
                        <a href="#4-nnfunctionalrelu" aria-label="4. nn.functional.ReLU()">4. <strong><code>nn.functional.ReLU()</code></strong></a></li>
                    <li>
                        <a href="#5-nnmaxpool2d" aria-label="5. nn.MaxPool2d()">5. <strong><code>nn.MaxPool2d()</code></strong></a></li>
                    <li>
                        <a href="#6-nnavgpool2d" aria-label="6. nn.AvgPool2d()">6. <strong><code>nn.AvgPool2d()</code></strong></a></li></ul>
                    </li>
                    <li>
                        <a href="#torchcuda" aria-label="torch.cuda">torch.cuda</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="torch-基本函数">Torch 基本函数<a hidden class="anchor" aria-hidden="true" href="#torch-基本函数">#</a></h2>
<h3 id="1-torcheinsum">1. <strong><code>torch.einsum()</code></strong><a hidden class="anchor" aria-hidden="true" href="#1-torcheinsum">#</a></h3>
<p><code>torch.einsum(equation, *operands)-&gt;Tensor</code>:爱因斯坦求和
ref1: 算子部署: <a href="https://blog.csdn.net/HW140701/article/details/120654252">https://blog.csdn.net/HW140701/article/details/120654252</a>
ref2: 例子: <a href="https://zhuanlan.zhihu.com/p/361209187">https://zhuanlan.zhihu.com/p/361209187</a></p>
<p><strong>三条基本规则:</strong></p>
<ul>
<li><strong>规则一:</strong> equation 箭头左边，在不同输入之间<font color=red>重复出现的索引</font>表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， &ldquo;ik,kj-&gt;ij&rdquo;，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作；</li>
<li><strong>规则二:</strong> 只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引；</li>
<li><strong>规则三:</strong> equation 箭头右边的索引顺序可以是任意的，比如上面的 &ldquo;ik,kj-&gt;ij&rdquo; 如果写成 &ldquo;ik,kj-&gt;ji&rdquo;，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成</li>
</ul>
<p><strong>特殊规则:</strong></p>
<ul>
<li>equation 可以不写包括箭头在内的右边部分，那么在这种情况下，输出张量的维度会根据默认规则推导。就是把输入中只出现一次的索引取出来，然后按字母表顺序排列，比如上面的矩阵乘法 &ldquo;ik,kj-&gt;ij&rdquo; 也可以简化为 &ldquo;ik,kj&rdquo;，根据默认规则，输出就是 &ldquo;ij&rdquo; 与原来一样；</li>
<li>equation 中支持 &ldquo;&hellip;&rdquo; 省略号，用于表示用户并不关心的索引。比如只对一个高维张量的最后两维做转置可以这么写：
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">a</span> <span class="o">=</span> torch.randn<span class="o">(</span>2,3,5,7,9<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># i = 7, j = 9</span>
</span></span><span class="line"><span class="cl"><span class="nv">b</span> <span class="o">=</span> torch.einsum<span class="o">(</span><span class="s1">&#39;...ij-&gt;...ji&#39;</span>, <span class="o">[</span>a<span class="o">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="2-torchpermutetorchtranspose">2. <strong><code>torch.permute()/torch.transpose()</code></strong><a hidden class="anchor" aria-hidden="true" href="#2-torchpermutetorchtranspose">#</a></h3>
<p><code>torch.permute(dim0, dim1, dim2)</code>:用于调换不同维度的顺序
<code>torch.transpose(input, dim0, dim1)</code>:交换矩阵的两个维度</p>
<h3 id="3-torchrand">3. <strong><code>torch.rand()</code></strong><a hidden class="anchor" aria-hidden="true" href="#3-torchrand">#</a></h3>
<p><code>torch.rand(dim0, dim1)</code>:生成dim0 x dim1的tensor</p>
<h3 id="4-torchsizetorchshape">4. <strong><code>torch.size()/torch.shape</code></strong><a hidden class="anchor" aria-hidden="true" href="#4-torchsizetorchshape">#</a></h3>
<p><code>torch.size()</code>:返回tensor的size
<code>torch.shape</code>:返回tensor的size</p>
<h3 id="5-torchtensordot">5. <strong><code>torch.tensordot()</code></strong><a hidden class="anchor" aria-hidden="true" href="#5-torchtensordot">#</a></h3>
<p>ref: tensordot()和einsum()的区别: <a href="https://blog.csdn.net/Eric_1993/article/details/105670381">https://blog.csdn.net/Eric_1993/article/details/105670381</a>
<code>torch.tensordot(tensor1， tensor2， axes=([dim1,dim2],[dim0, dim1]))</code>: 将axes指定的子数组进行点乘, axes 指定具体的维度.</p>
<h3 id="6-torchtranspose">6. <strong><code>torch.transpose()</code></strong><a hidden class="anchor" aria-hidden="true" href="#6-torchtranspose">#</a></h3>
<p><code>torch.transpose(tensor, dim0, dim2) —&gt; Tensor</code>:在dim0和dim1方向上转置</p>
<p>###7. <strong><code>torch.index_add_()</code></strong></p>
<p><code> Tensor.index_add_(dim, index, source, *, alpha=1) → Tensor</code></p>
<p>demo:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span><span class="p">([[</span>  <span class="mf">2.</span><span class="p">,</span>   <span class="mf">3.</span><span class="p">,</span>   <span class="mf">4.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">8.</span><span class="p">,</span>   <span class="mf">9.</span><span class="p">,</span>  <span class="mf">10.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">5.</span><span class="p">,</span>   <span class="mf">6.</span><span class="p">,</span>   <span class="mf">7.</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span><span class="p">([[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="torch-nn-module">Torch NN Module<a hidden class="anchor" aria-hidden="true" href="#torch-nn-module">#</a></h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="1-nnconv1d">1. <strong><code>nn.Conv1d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#1-nnconv1d">#</a></h3>
<p><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code></p>
<p><strong>Shape:</strong>
- Input: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$
- Output: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$, where
$$L_{out} = \frac{L_{in} + 2 \cdot \text{padding} - \text{dilation} \cdot (\text{kernel_size} - 1) - 1}{stride}$$</p>
<p><strong>Demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span> <span class="c1"># B x C x H or N x C x L</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># torch.Size([20, 33, 24])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="2-nnconv2d">2. <strong><code>nn.Conv2d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#2-nnconv2d">#</a></h3>
<p><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code></p>
<p><strong>Shape:</strong></p>
<ul>
<li>Input: $(N, C_{\text in}, H_{\text in}, W_{\text in})$ or $(C_{\text in}, H_{\text in}, W_{\text in})$
- Output: $(N, C_{\text out}, H_{\text out}, W_{\text out})$ or $(C_{\text out}, H_{\text out}, W_{\text out})$, where
$$
H_{out} = \frac{H_{in} + 2 \cdot \text{padding[0]} - \text{dilation[0]} \cdot (\text{kernel_size[0]} - 1) - 1}{stride[0]} + 1
$$
$$
W_{out} = \frac{W_{in} + 2 \cdot \text{padding[1]} - \text{dilation[1]} \cdot (\text{kernel_size[1]} - 1) - 1}{stride[1]} + 1
$$</li>
</ul>
<p><strong>Demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="c1"># With square kernels and equal stride</span>
</span></span><span class="line"><span class="cl">  <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># non-square kernels and unequal stride and with padding</span>
</span></span><span class="line"><span class="cl">  <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># output.shape: 20 x 33 x 28 x 100</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
</span></span><span class="line"><span class="cl">  <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># output.shape: 20 x 33 x 26 x 100</span>
</span></span><span class="line"><span class="cl">  <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># </span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="3-nnfunctionalinterpolate">3. <strong><code>nn.functional.interpolate()</code></strong><a hidden class="anchor" aria-hidden="true" href="#3-nnfunctionalinterpolate">#</a></h3>
<p><code>torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False)</code></p>
<h3 id="4-nnfunctionalrelu">4. <strong><code>nn.functional.ReLU()</code></strong><a hidden class="anchor" aria-hidden="true" href="#4-nnfunctionalrelu">#</a></h3>
<p>$$ \text{ReLU} = (x)^+ = \max {(0,x)}$$</p>
<p><code>torch.nn.ReLU(inplace=False)</code></p>
<p><strong>作用:</strong></p>
<ul>
<li>
<p>Sigmoid的导数只有在0附近的时候有比较好的激活性，在正负饱和区的梯度都接近于0，所以这会造成梯度弥散，而ReLU函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象。</p>
</li>
<li>
<p>ReLU函数在负半区的导数为0 ，所以一旦神经元激活值进入负半区，那么梯度就会为0，而正值不变，这种操作被成为单侧抑制。（也就是说：<strong>在输入是负值的情况下，它会输出0，那么神经元就不会被激活。这意味着同一时间只有部分神经元会被激活，从而使得网络很稀疏，进而对计算来说是非常有效率的。</strong>）<u>正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</u>尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。</p>
</li>
<li>
<p>relu函数的导数<strong>计算更快</strong>，程序实现就是一个if-else语句，而sigmoid函数要进行浮点四则运算。</p>
</li>
</ul>
<p><strong>Shape:</strong></p>
<ul>
<li>Input: $(∗)$, where $*$ means any number of dimensions.</li>
<li>Output: $(∗)$, same shape as the input.</li>
</ul>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/notes/2022-06-09_PyTorch/ReLU.png" alt="执行echo $PATH的结果"  />
</p>
<p><strong>Demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># An implementation of CReLU - https://arxiv.org/abs/1603.05201</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span><span class="n">m</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="5-nnmaxpool2d">5. <strong><code>nn.MaxPool2d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#5-nnmaxpool2d">#</a></h3>
<p><code>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></p>
<p><strong>Shape:</strong></p>
<ul>
<li>Input: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$</li>
<li>Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$</li>
</ul>
<p>where,</p>
<p>$$ H_{out} = \frac{H_{in} + 2 * \text{padding}[0] - \text{dilation}[0] * (\text{kernel_size}[0]-1) - 1}{\text{stride}[0]} + 1$$</p>
<p>$$ W_{out} = \frac{W_{in} + 2 * \text{padding}[1] - \text{dilation}[1] * (\text{kernel_size}[1]-1) - 1}{\text{stride}[1]} + 1$$</p>
<p><strong>demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># pool of square window of size=3, stride=2</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># pool of non-square window</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># 20 16 24 31</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="6-nnavgpool2d">6. <strong><code>nn.AvgPool2d()</code></strong><a hidden class="anchor" aria-hidden="true" href="#6-nnavgpool2d">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Shape:</strong></p>
<ul>
<li>Input: $(N, C, H_{in}, W_{in})$ or $(C, H_{in}, W_{in})$</li>
<li>Output: $(N, C, H_{out}, W_{out})$ or $(C, H_{out}, W_{out})$</li>
</ul>
<p>where,</p>
<p>$$ H_{out} = \frac{H_{in} + 2 * \text{padding}[0] -  (\text{kernel_size}[0])}{\text{stride}[0]} + 1$$</p>
<p>$$ W_{out} = \frac{W_{in} + 2 * \text{padding}[1] - (\text{kernel_size}[1])}{\text{stride}[1]} + 1$$</p>
<p><strong>demo:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># pool of square window of size=3, stride=2</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># pool of non-square window</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># 20 16, 24 31</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="torchcuda">torch.cuda<a hidden class="anchor" aria-hidden="true" href="#torchcuda">#</a></h2>
<p>ref link: <a href="https://zhuanlan.zhihu.com/p/76908135">https://zhuanlan.zhihu.com/p/76908135</a></p>
<ol>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_device"><code>torch.cuda.current_device()</code></a>: 返回当前选择的设备的索引</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_stream"><code>torch.cuda.current_stream()</code></a>: 返回参数设备的当前的<a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.Stream">Stream</a></p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#current_stream"><code>torch.cuda.default_stream()</code></a>: 返回当前参数设备的<a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.Stream">Stream</a></p>
</li>
<li>
<p><em>CLASS</em> <a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#device"><code>torch.cuda.device</code></a>: 可以改变选择的设备的上下文管理器
Parameters：device (torch.device or int) – device index to select. It’s a no-op if this argument is a negative integer or None.</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#device_count"><code>torch.cuda.device_count()</code></a>: 返回可使用GPU的数量</p>
</li>
<li>
<p><em>CLASS</em> <a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#device_of"><code>torch.cuda.device_of(obj)</code></a>
Context-manager 将参数对象的设备改成当前的设备。你可以使用张量或者存储作为参数。如果传入的对象没有分配在GPU上，这个操作是无效的。</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#empty_cache"><code>torch.cuda.empty_cache()</code></a>
释放caching allocator当前持有的所有未占用的cached memory，使其可以用在其他GPU应用且可以在 nvidia-smi可视化。</p>
<blockquote>
<blockquote>
<p>注意：<a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.empty_cache">empty_cache()</a> 并不会增加PyTorch可以使用的GPU显存的大小。 查看 <a href="https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management">Memory management</a> 来获取更多的GPU显存管理的信息。</p>
</blockquote>
</blockquote>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#get_device_capability"><code>torch.cuda.get_device_capability(device=None)</code></a>
Gets the cuda capability of a device.</p>
<p>Parameters：device (torch.device or int, optional) – device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given bycurrent_device(), if device is None (default).</p>
<p>Returns：the major and minor cuda capability of the device</p>
<p>Return type ： tuple(int, int)</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#get_device_name"><code>torch.cuda.get_device_name(device=None)</code></a></p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#init"><code>torch.cuda.init()</code></a>
初始化PyTorch的CUDA状态。如果你通过C API与PyTorch进行交互，你可能需要显式调用这个方法。只有CUDA的初始化完成，CUDA的功能才会绑定到Python。用户一般不应该需要这个，因为所有PyTorch的CUDA方法都会自动在需要的时候初始化CUDA。如果CUDA的状态已经初始化了，将不起任何作用。</p>
</li>
<li>
<p>[<code>torch.cuda.is_available()</code>]</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#max_memory_allocated"><code>torch.cuda.max_memory_allocated(device=None)</code></a>
Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#max_memory_cached"><code>torch.cuda.max_memory_cached(device=None)</code></a></p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#memory_allocated"><code>torch.cuda.memory_allocated(device=None)</code></a>
Parameters：device (torch.device or int, optional) – selected device. Returns statistic for the current device, given by current_device(), if device is None (default).</p>
</li>
<li>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/cuda.html#memory_cached"><code>torch.cuda.memory_cached(devide=None)</code></a></p>
</li>
<li>
<p>[``]</p>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jianye0428.github.io/en/tags/pytorch/">PyTorch</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jianye0428.github.io/en/posts/tech/2022-06-12_social_nce/">
    <span class="title"><i class="fas fa-angle-double-left"></i> Prev Page</span>
    <br>
    <span>Social_NCE: Contrastive Learning of Socially-aware Motion Representation</span>
  </a>
  <a class="next" href="https://jianye0428.github.io/en/posts/tech/2022-06-08_social_stgcnn/">
    <span class="title">Next Page <i class="fas fa-angle-double-right"></i></span>
    <br>
    <span>Social_STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on twitter"
        href="https://twitter.com/intent/tweet/?text=PyTorch%20Notes&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f&amp;hashtags=PyTorch">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f&amp;title=PyTorch%20Notes&amp;summary=PyTorch%20Notes&amp;source=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f&title=PyTorch%20Notes">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on whatsapp"
        href="https://api.whatsapp.com/send?text=PyTorch%20Notes%20-%20https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share PyTorch Notes on telegram"
        href="https://telegram.me/share/url?text=PyTorch%20Notes&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2fnotes%2f2022-06-09_pytorch%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>




<footer class="tc-container" id="comment">
    <div class="tc-title"><p class="c-title">Discussion</p></div>
    <div id="tcomments"></div>
</footer>
<script crossorigin="anonymous" src="/js/twikoo.min.b16100b7cf8a61759eab076a122482054e083087aad37c3be1fe2e293934dc34.js" integrity="sha256-sWEAt8&#43;KYXWeqwdqEiSCBU4IMIeq03w74f4uKTk03DQ="></script>
<script>
    twikoo.init({
        envId: 'https://my-repository-pink.vercel.app/',
        el: '#tcomments',
        region: 'ap-shanghai', 
        
        lang: 'zh-CN', 
    });
</script>

</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
