<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LaneGCN: Learning Lane Graph Representations for Motion Forecasting | Jian&#39;s Blog</title>
<meta name="keywords" content="prediction">
<meta name="description" content="paper link: https://arxiv.org/abs/2007.13732 PPT: https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf Architechture Lane Graph &#43; Actor Map: construct lane graph from vectorized map data to preserve the map structure and can avoid information loss 构建矢量化地图信息，避免地图信息丢失 LaneGCN: extends graph convolutions with multiple adjacency matrices and along-lane dilation to capture complex topology and long range dependencies of the lane graph. exploit">
<meta name="author" content="Jian">
<link rel="canonical" href="https://jianye0428.github.io/en/posts/tech/2022-06-13_lanegcn/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.1ea9c8832138446635789668415e5c75b8a534b191ee749a44f5ab404c9f27c2.css" integrity="sha256-HqnIgyE4RGY1eJZoQV5cdbilNLGR7nSaRPWrQEyfJ8I=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jianye0428.github.io/favicon/jian_icon.png">
<link rel="apple-touch-icon" href="https://jianye0428.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://jianye0428.github.io/favicon/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://jianye0428.github.io/en/posts/tech/2022-06-13_lanegcn/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="baidu-site-verification" content="code-9oLyeix0aK" />
<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4a41bf85d719f0e8c3165fc76904f546";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>



<script defer crossorigin="anonymous" src="/js/katex.min.8f5024e83d2055dd60e021751066111b0057e230db34911dd56242d67f0a4c86.js" integrity="sha256-j1Ak6D0gVd1g4CF1EGYRGwBX4jDbNJEd1WJC1n8KTIY="></script>


<script defer crossorigin="anonymous" src="/js/auto-render.min.b09accad850e4e87b8a2fc8b93fae790def79172b68de72fd777958c52e566ad.js" integrity="sha256-sJrMrYUOToe4ovyLk/rnkN73kXK2jecv13eVjFLlZq0="></script>

<script>
    
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });

    
    window.WebFontConfig = {
        custom: {
            families: ['KaTeX_AMS', 'KaTeX_Caligraphic:n4,n7', 'KaTeX_Fraktur:n4,n7',
            'KaTeX_Main:n4,n7,i4,i7', 'KaTeX_Math:i4,i7', 'KaTeX_Script',
            'KaTeX_SansSerif:n4,n7,i4', 'KaTeX_Size1', 'KaTeX_Size2', 'KaTeX_Size3',
            'KaTeX_Size4', 'KaTeX_Typewriter'],
        },
    };
</script>


<script defer crossorigin="anonymous" src="/js/webfontloader.min.min.d1c6c39d18e2decb5c99dc9efc579098ab37b9654725df3f9c0737bc2dd00760.js" integrity="sha256-0cbDnRji3stcmdye/FeQmKs3uWVHJd8/nAc3vC3QB2A="></script>


 

<script async src="https://www.googletagmanager.com/gtag/js?id=G-C6GDZ56F4S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-C6GDZ56F4S', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="LaneGCN: Learning Lane Graph Representations for Motion Forecasting" />
<meta property="og:description" content="paper link: https://arxiv.org/abs/2007.13732 PPT: https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf Architechture Lane Graph &#43; Actor Map: construct lane graph from vectorized map data to preserve the map structure and can avoid information loss 构建矢量化地图信息，避免地图信息丢失 LaneGCN: extends graph convolutions with multiple adjacency matrices and along-lane dilation to capture complex topology and long range dependencies of the lane graph. exploit" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jianye0428.github.io/en/posts/tech/2022-06-13_lanegcn/" /><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-13T16:01:19&#43;08:00" />
<meta property="article:modified_time" content="2022-06-13T16:01:19&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"/>

<meta name="twitter:title" content="LaneGCN: Learning Lane Graph Representations for Motion Forecasting"/>
<meta name="twitter:description" content="paper link: https://arxiv.org/abs/2007.13732 PPT: https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf Architechture Lane Graph &#43; Actor Map: construct lane graph from vectorized map data to preserve the map structure and can avoid information loss 构建矢量化地图信息，避免地图信息丢失 LaneGCN: extends graph convolutions with multiple adjacency matrices and along-lane dilation to capture complex topology and long range dependencies of the lane graph. exploit"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jianye0428.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LaneGCN: Learning Lane Graph Representations for Motion Forecasting",
      "item": "https://jianye0428.github.io/en/posts/tech/2022-06-13_lanegcn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LaneGCN: Learning Lane Graph Representations for Motion Forecasting",
  "name": "LaneGCN: Learning Lane Graph Representations for Motion Forecasting",
  "description": "paper link: https://arxiv.org/abs/2007.13732 PPT: https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf Architechture Lane Graph + Actor Map: construct lane graph from vectorized map data to preserve the map structure and can avoid information loss 构建矢量化地图信息，避免地图信息丢失 LaneGCN: extends graph convolutions with multiple adjacency matrices and along-lane dilation to capture complex topology and long range dependencies of the lane graph. exploit",
  "keywords": [
    "prediction"
  ],
  "articleBody": "paper link: https://arxiv.org/abs/2007.13732\nPPT: https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf\nArchitechture Lane Graph + Actor Map:\nconstruct lane graph from vectorized map data to preserve the map structure and can avoid information loss 构建矢量化地图信息，避免地图信息丢失\nLaneGCN:\nextends graph convolutions with multiple adjacency matrices and along-lane dilation\nto capture complex topology and long range dependencies of the lane graph. exploit a fusion network consisting of four types of interactions: actor-to-lane, lane-to-actor, actor-to-actor, lane-to-lane.\npresent both actors and lanes as nodes in the graph and use a 1D CNN and LaneGCN to extract the features for the actor and lane nodes respectively, and then exploit spatial attention and another LaneGCN to model four types of interactions. Difference between VectorNet and LaneGCN:\nVecotrNet uses vanilla graph networks with undirected full connections; LaneGCN uses connected lane graph folllowing the map topology and propose task specific multi-type and dilated graph operators. VectorNet uses polyline-level nodes for interactions; LaneGCN uses polyline segments as map nodes to capture higher resolution. Lane Graph Representations for Motion Forecasting ActorNet: Extracting Traffic Participant Representations Each Trajctory is represented as a sequence of displacement ${ \\bigtriangleup{p_{-(T-1)},…,\\bigtriangleup{p_{-1}}, \\bigtriangleup{p_0}}}$, where $\\bigtriangleup{p_t}$ is the 2D displacement from time step $t-1$ to t, and T is the trajectory size.\nFor trajectories with sizes smaller than $T$ , we pad them with zeros. We add a binary $1 × T$ mask to indicate if the element at each step is padded or not and concatenate it with the trajectory tensor, resulting in an input tensor of size $3 × T$.\n1D CNN is used to process the trajectory input for its effectiveness in extracting multi-scale features and efficiency in parallel computing. The output of ActorNet is a temporal feature map, whose element at $t = 0$ is used as the actor feature. The network has 3 groups/scales of 1D convolutions.\nEach group consists of 2 residual blocks, with the stride of the first block as 2. We then use a Feature Pyramid Network (FPN) to fuse the multi-scale features, and apply another residual block to obtain the output tensor. For all layers, the convolution kernel size is 3 and the number of output channels is 128. Layer normalization and the Rectified Linear Unit (ReLU) are used after each convolution.\nMapNet: Extracting Structured Map Representation General Architecture:\npart 1: building a lane graph from vectorized map data; part 2: applying our novel LaneGCN to the lane graph to output the map features. Map Data:\nIn this paper, we adopt a simple form of vectorized map data as our representation of HD maps. Specifically, the map data is represented as a set of lanes and their connectivity. Each lane contains a centerline, i.e., a sequence of 2D BEV points, which are arranged following the lane direction (see Fig. 3, top). For any two lanes which are directly reachable, 4 types of connections are given: predecessor, successor, left neighbour and right neighbour.\nLane Graph Construction:\nfirst define a lane node as the straight line segment formed by any two consecutive points (grey circles in Fig. 3) of the centerline. The location of a lane node is the averaged coordinates of its two end points. Following the connections between lane centerlines, we also derive 4 connectivity types for the lane nodes, i.e., predecessor, successor, left neighbour and right neighbour.\nWe denote the lane nodes with $V ∈ \\mathbb R^{N ×2}$ , where $N$ is the number of lane nodes and the $i$-th row of $V$ is the BEV coordinates of the $i$-th node. We represent the connectivity with 4 adjacency matrices ${\\lbrace A_i \\rbrace}_{i \\in {pre,suc,left,right}}$ , with $A_i \\in \\mathbb R^{N ×N}$.\nWe denote $A_{i,jk}$, as the element in the $j$-th row and $k$-th column of $A_i$. Then $A_{i,jk} = 1$ if node $k$ is an $i$-type neighbor of node $j$.\nLaneConv Operator:\nNode Feature:\nEach lane node corresponds to a straight line segment of a centerline. To encode all the lane node information, we need to take into account both the shape (size and orientation) and the location (the coordinates of the center) of the corresponding line segment. We parameterize the node feature as follows,\n$$x_i = MLP_{shape} (v_{i}^{end} - v_{i}^{start}) + MLP_{loc}(v_i) $$\nwhere $MLP$ indicates a multi-layer perceptron and the two subscripts refer to shape and location, respectively. $v_i$ is the location of the i-th lane node, i.e., the center between two end points, $v_i^{start}$ and $v_i^{end}$ are the BEV coordinates of the node $i’s$ starting and ending points, and $x_i$ is the $i$-th row of the node feature matrix $X$, denoting the input feature of the $i$-th lane node.\nLaneConv: To aggregate the topology information of the lane graph at a larger scale, we design the following LaneConv operator:\n$$Y = XW_0 + \\sum_{i\\in{pre, suc, left, right}}A_iXW_i,\\tag{2}$$\nwhere $A_i$ and $W_i$ are the adjacency and the weight matrices corresponding to the $i$-th connection type respectively. Since we order the lane nodes from the start to the end of the lane, $A_{suc}$ and $A_{pre}$ are matrices obtained by shifting the identity matrix (diagnal 1) one step towards upper right (non-zero superdiagonal) and lower left (non-zero subdiagonal). $A_{suc}$ and $A_{pre}$ can propagate information from the forward and backward neighbours whereas $A_{left}$ and $A_{right}$ allow information to flow from the cross-lane neighbours. It is not hard to see that our LaneConv builds on top of the general graph convolution and encodes more geometric (e.g., connection type/direction) information. As shown in our experiments this improves over the vanilla graph convolution.\nDilated LaneConv:\nFunctionality: The model needs to capture the long range dependency along the lane direction for accurate prediction.\nthe k-dilation LaneConv operator is defined as follows:\n$$Y = XW_0 + A_{pre}^k XW_{pre,k} + A_{suc}^k X W_{suc,k} \\tag{3}$$\nwhere $A_{pre}^k$ is the $k$-th matrix power of $A_{pre}$. This allows us to directly propagate information along the lane for $k$ steps, with $k$ a hyperparameter. Since $A_{pre}^k$ is highly sparse, one can efficiently compute it using sparse matrix multiplication. Note that the dilated LaneConv is only used for predecessor and successor, as the long range dependency is mostly along the lane direction.\nLaneGCN:\nWith Eq.(2) and Eq.(3), we get a multi-scale LaneConv operator with C dilation size as follows:\n$$Y = XW_0 + \\sum_{i\\in \\lbrace left, right \\rbrace} A_i X W_i + \\sum_{c=1}^C (A_{pre}^{k_c}XW_{pre, k_c} + A_{suc}^{k_c}XW_{suc, k_c})， \\tag{4}$$\nwhere $k_c$ is the $c$-th dilation size. We denote $LaneConv(k_1 , · · · , k_C)$ this multi-scale layer.\nFusion Net Four types fusion modules:\nA2L: introduces real-time traffic information to lane nodes, such as blockage or usage of the lanes. L2L: updates lane node features by propagating the traffic information over the lane graph. -\u003e LaneGCN L2A: fuses updated map features with real-time traffic information back to the actors. A2A: handles the interactions between actors and produces the output actor features, which are then used by the prediction header for motion forecasting. We implement L2L using another LaneGCN, which has the same architecture as the one used in our MapNet (see Section 3.2). In the following we describe the other three modules in detail. We exploit a spatial attention layer for A2L, L2A and A2A. The attention layer applies to each of the three modules in the same way. Taking A2L as an example, given an actor node i, we aggregate the features from its context lane nodes j as follows:\n$$y_i = x_i W_0 + \\sum_j \\phi (concat(x_i, \\Delta_{i,j}, x_j)W_1)W_2, \\tag{5}$$\nwith $x_i$ the feature of the $i$-th node, $W$ a weight matrix, $\\phi$ the compositon of layer notmalization and RelU, and $\\Delta_{ij} = MLP(v_j - v_i)$, where $v$ denotes the node location.\nPrediction Header Take after-fusion actor features as input, a multi-modal prediction header outputs the final motion forecasting. For each actor, it predicts $K$ possible future trajectories and their confidence scores.\nThe header has two branches, a regression branch to predict the trajectory of each mode and a classification branch to predict the confidence score of each mode.\nFor the m-th actor, we apply a residual block and a linear layer in the regression branch to regress the K sequences of BEV coordinates:\n$$O_{m,reg} = \\lbrace (p_{m,1}^k, p_{m,2}^k, …, p_{m,T}^k) \\rbrace _{k\\in[0,K-1]}$$\nwhere $p_{m,i}^k$ is the predicted $m$-th actor’s BEV coordinates of the $k$-th mode at the $i$-th time step. For the classification branch, we apply an MLP to $p^k_{m,T} − p_{m,0}$ to get $K$ distance embeddings. We then concatenate each distance embedding with the actor feature, apply a residual block and a linear layer to output $K$ confidence scores, $O_{m,cls} = (c_{m,0}, c_{m,1}, …, c_{m,K−1})$.\nLearning use the sum of classification and regreesion losses to train the model:\n$$ L = L_{cls} + \\alpha L_{reg},$$\nwhere $\\alpha = 1.0$.\nFor classification, we use the max-margin loss:\n$$L_{cls} = \\frac{1}{M(K-1)}\\sum_{m=1}^M \\sum_{k \\neq \\hat{k}} \\max(0, c_{m,k} + \\epsilon - c_{m, \\hat{k}}) \\tag{6}$$\nwhere $\\epsilon$ is the margin and $M$ is the total number of actors. For regression, we apply the smooth $l1$ loss on all predicted time steps:\n$$L_{reg} = \\frac{1}{MT} \\sum_{m=1}^M \\sum_{t=1}^T reg(p_{m,y}^{\\hat{k}} - p_{m,t}^*) \\tag{7}$$\nwhere $p_t^*$ is the ground truth BEV coordinates at time step $t$, $reg(x) = \\sum\\limits_i d(x_i)$, $x_i$ is the $i$-th element of $x$, and $d(x_i)$ is the smooth $\\ell1$ loss defined as:\n$$d(x_i) = \\begin{cases} 0.5x_i^2 \u0026\\text{if} ||x|| \u003c 1, \\ ||x_i|| - 0.5 \u0026 \\text{otherwise,} \\end{cases} \\tag{8}$$\nwhere $||x_i||$ denotes the $\\ell1$ norm of $x_i$.\nNeural Network Layout Data Process And Network Construction 以官方的2645.csv数据集为例子\nagent node:\ndata['city']:城市名称 data['trajs'] = [agt_traj] + ctx_trajs:轨迹点，(agent + context vehicles) data['steps'] = [agt_step] + ctx_steps:在原始数据中的位置 data['feats'] = feats: (13 X 20 X 3) 前20预测轨迹 + 一维是否存在点 data['ctrs'] = ctrs: (13 X 2) 中心点 data['orig'] = orig: AGENT 当前点坐标 data['theta'] = theta: AGENT 偏转角 data['rot'] = rot: (2 X 2) 旋转矩阵 data['gt_preds'] = gt_preds:(13 X 30 X 2) 后30帧真实轨迹 data['has_preds'] = has_preds: (13 X 30) 标识后30帧轨迹是否存在 lane node:\ngraph['ctrs'] = np.concatenate(ctrs, 0): lane node的中心点坐标 graph['num_nodes'] = num_nodes: lane node的数量 graph['feats'] = np.concatenate(feats, 0): lane node 方向向量 graph['turn'] = np.concatenate(turn, 0): lane node 转向标识 graph['control'] = np.concatenate(control, 0): lane node 的 has_traffic_control 标识 graph['intersect'] = np.concatenate(intersect, 0): lane node 的 is_intersection 标识 graph['pre'] = [pre]: pre[‘u’] 和 pre[‘v’], v 是 u 的pre， 这里表述的是lane node之间的关系 graph['suc'] = [suc]: suc[‘u’] 和 suc[‘v’], v 是 u 的suc， 这里表述的是lane node之间的关系 graph['lane_idcs'] = lane_idcs: lane node index 1 2 3 4 0 0 0 ... 0 1 1 1 ... 1 ... 83 83 83 ... 83 graph['pre_pairs'] = pre_pairs: pair 表述的是lane之间的关系 graph['suc_pairs'] = suc_pairs: pair 表述的是lane之间的关系 graph['left_pairs'] = left_pairs: pair 表述的是lane之间的关系 graph['right_pairs'] = right_pairs: pair 表述的是lane之间的关系 对于pre['u']和pre['v'], v 是 u 的 pre 对于suc['u']和suc['v'], v 是 u 的 suc 对于left['u']和left['v'], v 是 u 的 left 对于right['u']和right['v'], v 是 u 的 right Net结构\nActorNet input: M x 3 x 20 output: M x 128 x 20 解释:\nMapNet: 把 v 按照 u 加到center上 input: N x 4 output: N x 128\nA2M input: N x 128 output: N x 128\nM2M input: N x 128 output: N x 128\nM2A input: N x 128 output: M x 128\nA2A input: N x 128 output: N x 128\nPrediction Header: input M x 128\nMLP Regression MLP Classification ref link: https://zhuanlan.zhihu.com/p/447129428\n",
  "wordCount" : "2206",
  "inLanguage": "en",
  "datePublished": "2022-06-13T16:01:19+08:00",
  "dateModified": "2022-06-13T16:01:19+08:00",
  "author":[{
    "@type": "Person",
    "name": "Jian"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jianye0428.github.io/en/posts/tech/2022-06-13_lanegcn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jian's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jianye0428.github.io/favicon/jian_icon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jianye0428.github.io/en/" accesskey="h" title="Jian&#39;s Blog (Alt + H)">
                <img src="https://jianye0428.github.io/favicon/jian_icon.png" alt="logo" aria-label="logo"
                    height="30">Jian&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://jianye0428.github.io/cn/" title="Chinese"
                            aria-label="Chinese">Chinese</a>
                    </li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jianye0428.github.io/en/myresume/" title="My Resume">
                    <span>My Resume</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/tags/" title="🔖Tags">
                    <span>🔖Tags</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/archives" title="🙋🏻‍♂️Archive">
                    <span>🙋🏻‍♂️Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jianye0428.github.io/en/search/" title="🔍Search (Alt &#43; /)" accesskey=/>
                    <span>🔍Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jianye0428.github.io/en/">Home</a>&nbsp;»&nbsp;<a href="https://jianye0428.github.io/en/posts/">Posts</a></div>
    <h1 class="post-title">
      LaneGCN: Learning Lane Graph Representations for Motion Forecasting
    </h1>
    <div class="post-meta"><span title='2022-06-13 16:01:19 +0800 CST'>2022-06-13</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Jian&nbsp;|&nbsp;<a href="https://github.com/jianye0428/myblog/tree/main/content/posts/tech/2022-06-13_LaneGCN.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#architechture" aria-label="Architechture">Architechture</a></li>
                    <li>
                        <a href="#lane-graph-representations-for-motion-forecasting" aria-label="Lane Graph Representations for Motion Forecasting">Lane Graph Representations for Motion Forecasting</a><ul>
                            
                    <li>
                        <a href="#font-colorredactornetfont-extracting-traffic-participant-representations" aria-label="ActorNet: Extracting Traffic Participant Representations"><font color=red>ActorNet</font>: Extracting Traffic Participant Representations</a></li>
                    <li>
                        <a href="#font-colorredmapnetfont-extracting-structured-map-representation" aria-label="MapNet: Extracting Structured Map Representation"><font color=red>MapNet</font>: Extracting Structured Map Representation</a></li>
                    <li>
                        <a href="#font-colorredfusion-netfont" aria-label="Fusion Net"><font color=red>Fusion Net</font></a></li>
                    <li>
                        <a href="#font-colorredprediction-headerfont" aria-label="Prediction Header"><font color=red>Prediction Header</font></a></li>
                    <li>
                        <a href="#font-colorredlearningfont" aria-label="Learning"><font color=red>Learning</font></a></li>
                    <li>
                        <a href="#font-colorred-neural-network-layoutfont" aria-label=" Neural Network Layout"><font color=red> Neural Network Layout</font></a></li>
                    <li>
                        <a href="#font-colorreddata-process-and-network-constructionfont" aria-label="Data Process And Network Construction"><font color=red>Data Process And Network Construction</font></a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p><code>paper link:</code> <a href="https://arxiv.org/abs/2007.13732">https://arxiv.org/abs/2007.13732</a><br>
<code>PPT:</code> <a href="https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf">https://www.cs.toronto.edu/~byang/slides/LaneGCN.pdf</a></p>
<h2 id="architechture">Architechture<a hidden class="anchor" aria-hidden="true" href="#architechture">#</a></h2>
<p><strong><font color=red>Lane Graph + Actor Map:</font></strong></p>
<ul>
<li>
<p>construct lane graph from vectorized map data to preserve the map structure and can avoid information loss 构建矢量化地图信息，避免地图信息丢失</p>
</li>
<li>
<p>LaneGCN:</p>
<ul>
<li>
<p>extends <strong>graph convolutions with multiple adjacency matrices</strong> and along-lane dilation</p>
<ul>
<li>to capture complex topology and long range dependencies of the lane graph.</li>
</ul>
</li>
<li>
<p>exploit a <strong>fusion network</strong> consisting of four types of interactions: <code>actor-to-lane</code>, <code>lane-to-actor</code>, <code>actor-to-actor</code>, <code>lane-to-lane</code>.</p>
<ul>
<li>present both actors and lanes as nodes in the graph and use a 1D CNN and LaneGCN to extract the features for the actor and lane nodes respectively, and then exploit spatial attention and another LaneGCN to model four types of interactions.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/tech/2022-06-13_LaneGCN/NN_Architecture.png#pic_center" alt="NN Architecture"  />
</p>
<p><strong><font color=red>Difference between VectorNet and LaneGCN:</font></strong></p>
<ul>
<li><u>VecotrNet</u> uses vanilla graph networks with undirected full connections; <u>LaneGCN</u> uses connected lane graph folllowing the map topology and propose task specific multi-type and dilated graph operators.</li>
<li>VectorNet uses polyline-level nodes for interactions; LaneGCN uses polyline segments as map nodes to capture higher resolution.</li>
</ul>
<h2 id="lane-graph-representations-for-motion-forecasting">Lane Graph Representations for Motion Forecasting<a hidden class="anchor" aria-hidden="true" href="#lane-graph-representations-for-motion-forecasting">#</a></h2>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/tech/2022-06-13_LaneGCN/Model_Layout.png#pic_center" alt="Model_Layout"  />
</p>
<h3 id="font-colorredactornetfont-extracting-traffic-participant-representations"><font color=red>ActorNet</font>: Extracting Traffic Participant Representations<a hidden class="anchor" aria-hidden="true" href="#font-colorredactornetfont-extracting-traffic-participant-representations">#</a></h3>
<p>Each Trajctory is represented as a sequence of displacement ${ \bigtriangleup{p_{-(T-1)},&hellip;,\bigtriangleup{p_{-1}}, \bigtriangleup{p_0}}}$, where $\bigtriangleup{p_t}$ is the 2D displacement from time step $t-1$ to t, and T is the trajectory size.</p>
<p>For trajectories with sizes smaller than $T$ , we pad them with zeros. We add a binary $1 × T$ mask to indicate if the element at each step is padded or not and concatenate it with the trajectory tensor, resulting in an input tensor of size $3 × T$.</p>
<p>1D CNN is used to process the trajectory input for its effectiveness in extracting multi-scale features
and efficiency in parallel computing. The output of ActorNet is a temporal feature map, whose element at $t = 0$ is used as the actor feature. The network has 3 groups/scales of 1D convolutions.</p>
<p>Each group consists of 2 residual blocks, with the stride of the first block as 2. We then use a Feature Pyramid Network (FPN) to fuse the
multi-scale features, and apply another residual block to obtain the output tensor. For all layers, the convolution kernel size is 3 and the number of output channels is 128. Layer normalization and the Rectified Linear Unit (ReLU) are used after each convolution.</p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/tech/2022-06-13_LaneGCN/ActorNet.png#pic_center" alt="ActorNet"  />
</p>
<h3 id="font-colorredmapnetfont-extracting-structured-map-representation"><font color=red>MapNet</font>: Extracting Structured Map Representation<a hidden class="anchor" aria-hidden="true" href="#font-colorredmapnetfont-extracting-structured-map-representation">#</a></h3>
<p>General Architecture:</p>
<ul>
<li>part 1: building a lane graph from vectorized map data;</li>
<li>part 2: applying our novel LaneGCN to the lane graph to output the map features.</li>
</ul>
<p><strong>Map Data:</strong></p>
<p>In this paper, we adopt a simple form of vectorized map data as our representation of HD maps. Specifically, the map data is represented as a set of lanes and their connectivity. Each lane contains a centerline, i.e., a sequence of 2D BEV points, which are arranged following the lane direction (see Fig. 3, top). For any two lanes which are directly reachable, 4 types of connections are given: <code>predecessor</code>, <code>successor</code>, <code>left neighbour</code> and <code>right neighbour</code>.</p>
<p><strong>Lane Graph Construction:</strong></p>
<p>first define a lane node as the straight line segment formed by any two consecutive points (grey circles in Fig. 3) of the centerline. The location of a lane node is the averaged coordinates of its two end points. Following the connections between lane centerlines, we also derive 4 connectivity types for the lane nodes, i.e., <code>predecessor</code>, <code>successor</code>, <code>left neighbour</code> and <code>right neighbour</code>.</p>
<p>We denote the lane nodes with $V ∈ \mathbb R^{N ×2}$ , where $N$ is the number of lane nodes and the $i$-th row of $V$ is the BEV coordinates of the $i$-th node. We represent the connectivity with 4 adjacency matrices ${\lbrace A_i \rbrace}_{i \in {pre,suc,left,right}}$ , with $A_i \in \mathbb R^{N ×N}$.</p>
<p>We denote $A_{i,jk}$, as the element in the $j$-th row and $k$-th column of $A_i$. Then $A_{i,jk} = 1$ if node $k$ is an $i$-type neighbor of node $j$.</p>
<p><strong>LaneConv Operator:</strong></p>
<p><font color=green><em>Node Feature:</em></font><br>
Each lane node corresponds to a straight line segment of a centerline. To encode all the lane node information, we need to take into account both the shape (size and orientation) and the location (the coordinates of the center) of the corresponding line segment. We parameterize the node feature as follows,</p>
<p>$$x_i = MLP_{shape} (v_{i}^{end} - v_{i}^{start}) + MLP_{loc}(v_i) $$</p>
<p>where $MLP$ indicates a multi-layer perceptron and the two subscripts refer to shape and location, respectively. $v_i$ is the location of the i-th lane node, i.e., the center between two end points, $v_i^{start}$ and $v_i^{end}$ are the BEV coordinates of the node $i’s$ starting and ending points, and $x_i$ is the $i$-th row of the node feature matrix $X$, denoting the input feature of the $i$-th lane node.</p>
<p><font color=green><em>LaneConv:</em> </font><br>
To aggregate the topology information of the lane graph at a larger scale, we design the following LaneConv operator:</p>
<p>$$Y = XW_0 + \sum_{i\in{pre, suc, left, right}}A_iXW_i,\tag{2}$$</p>
<p>where $A_i$ and $W_i$ are the adjacency and the weight matrices corresponding to the $i$-th connection type respectively. Since we order the lane nodes from the start to the end of the lane, $A_{suc}$ and $A_{pre}$ are matrices obtained by shifting the identity matrix (diagnal 1) one step towards upper right (non-zero superdiagonal) and lower left (non-zero subdiagonal). $A_{suc}$ and $A_{pre}$ can propagate information from the forward and backward neighbours whereas $A_{left}$ and $A_{right}$ allow information to flow from the cross-lane neighbours. It is not hard to see that our LaneConv builds on top of the general graph convolution and encodes more geometric (e.g., connection type/direction) information. As shown in our experiments this improves over the vanilla graph convolution.</p>
<p><font color=green><em>Dilated LaneConv:</em></font></p>
<p>Functionality: The model needs to capture the long range dependency along the lane direction for accurate prediction.</p>
<p>the k-dilation LaneConv operator is defined as follows:</p>
<p>$$Y = XW_0 + A_{pre}^k XW_{pre,k} + A_{suc}^k X W_{suc,k} \tag{3}$$</p>
<p>where $A_{pre}^k$ is the $k$-th matrix power of $A_{pre}$. This allows us to directly propagate information along the lane for $k$ steps, with $k$ a hyperparameter. Since $A_{pre}^k$ is highly sparse, one can efficiently compute it using sparse matrix multiplication. Note that the dilated LaneConv is only used for predecessor and successor, as the long range dependency is mostly along the lane direction.</p>
<p><font color=green><em>LaneGCN:</em></font></p>
<p>With Eq.(2) and Eq.(3), we get a multi-scale LaneConv operator with C dilation size as follows:</p>
<p>$$Y = XW_0 + \sum_{i\in \lbrace left, right \rbrace} A_i X W_i + \sum_{c=1}^C (A_{pre}^{k_c}XW_{pre, k_c} + A_{suc}^{k_c}XW_{suc, k_c})， \tag{4}$$</p>
<p>where $k_c$ is the $c$-th dilation size. We denote $LaneConv(k_1 , · · · , k_C)$ this multi-scale layer.</p>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/tech/2022-06-13_LaneGCN/LaneGCN_Architecture.png" alt="LaneGCN Architecture"  />
</p>
<h3 id="font-colorredfusion-netfont"><font color=red>Fusion Net</font><a hidden class="anchor" aria-hidden="true" href="#font-colorredfusion-netfont">#</a></h3>
<p>Four types fusion modules:</p>
<ul>
<li>A2L: introduces real-time traffic information to lane nodes, such as blockage or usage of the lanes.</li>
<li>L2L: updates lane node features by propagating the traffic information over the lane graph. -&gt; LaneGCN</li>
<li>L2A: fuses updated map features with real-time traffic information back to the actors.</li>
<li>A2A: handles the interactions between actors and produces the output actor features, which are then used by the prediction header for motion forecasting.</li>
</ul>
<p>We implement L2L using another LaneGCN, which has the same architecture as the one used in our MapNet (see Section 3.2). In the following we describe the other three modules in detail. We exploit a spatial attention layer for A2L, L2A and A2A. The attention layer applies to each of the three modules in the same way. Taking A2L as an example, given an actor node i, we aggregate the features from its context lane nodes j as follows:</p>
<p>$$y_i = x_i W_0 + \sum_j \phi (concat(x_i, \Delta_{i,j}, x_j)W_1)W_2, \tag{5}$$</p>
<p>with $x_i$ the feature of the $i$-th node, $W$ a weight matrix, $\phi$ the compositon of layer notmalization and RelU, and $\Delta_{ij} = MLP(v_j - v_i)$, where $v$ denotes the node location.</p>
<h3 id="font-colorredprediction-headerfont"><font color=red>Prediction Header</font><a hidden class="anchor" aria-hidden="true" href="#font-colorredprediction-headerfont">#</a></h3>
<p>Take after-fusion actor features as input, a multi-modal prediction header outputs the final motion forecasting. For each actor, it predicts $K$ possible future trajectories and their confidence scores.</p>
<p>The header has two branches, a regression branch to predict
the trajectory of each mode and a classification branch to predict the confidence score of each mode.</p>
<p>For the m-th actor, we apply a residual block and a linear layer in the
regression branch to regress the K sequences of BEV coordinates:</p>
<p>$$O_{m,reg} = \lbrace (p_{m,1}^k, p_{m,2}^k, &hellip;, p_{m,T}^k) \rbrace _{k\in[0,K-1]}$$</p>
<p>where $p_{m,i}^k$ is the predicted $m$-th actor&rsquo;s BEV coordinates of the $k$-th mode at the $i$-th time step. For the classification branch, we apply an MLP to $p^k_{m,T} − p_{m,0}$ to get $K$ distance embeddings. We then concatenate each distance embedding with the actor feature, apply a residual block and a linear layer to output $K$ confidence scores, $O_{m,cls} = (c_{m,0}, c_{m,1}, &hellip;, c_{m,K−1})$.</p>
<h3 id="font-colorredlearningfont"><font color=red>Learning</font><a hidden class="anchor" aria-hidden="true" href="#font-colorredlearningfont">#</a></h3>
<p>use the sum of classification and regreesion losses to train the model:</p>
<p>$$ L = L_{cls} + \alpha L_{reg},$$</p>
<p>where $\alpha = 1.0$.</p>
<p>For classification, we use the max-margin loss:</p>
<p>$$L_{cls} = \frac{1}{M(K-1)}\sum_{m=1}^M \sum_{k \neq \hat{k}} \max(0, c_{m,k} + \epsilon - c_{m, \hat{k}}) \tag{6}$$</p>
<p>where $\epsilon$ is the margin and $M$ is the total number of actors. For regression, we apply the smooth $l1$ loss on all predicted time steps:</p>
<p>$$L_{reg} = \frac{1}{MT} \sum_{m=1}^M \sum_{t=1}^T reg(p_{m,y}^{\hat{k}} - p_{m,t}^*) \tag{7}$$</p>
<p>where $p_t^*$ is the ground truth BEV coordinates at time step $t$, $reg(x) = \sum\limits_i d(x_i)$, $x_i$ is the $i$-th element of $x$, and $d(x_i)$ is the smooth $\ell1$ loss defined as:</p>
<p>$$d(x_i) = \begin{cases}
0.5x_i^2 &amp;\text{if} ||x|| &lt; 1, \
||x_i|| - 0.5 &amp; \text{otherwise,}
\end{cases} \tag{8}$$</p>
<p>where $||x_i||$ denotes the $\ell1$ norm of $x_i$.</p>
<h3 id="font-colorred-neural-network-layoutfont"><font color=red> Neural Network Layout</font><a hidden class="anchor" aria-hidden="true" href="#font-colorred-neural-network-layoutfont">#</a></h3>
<p><img loading="lazy" src="https://github.com/jianye0428/hello-hugo/raw/master/img/posts/tech/2022-06-13_LaneGCN/NN_Layout.png" alt="LaneGCN Architecture"  />
</p>
<h3 id="font-colorreddata-process-and-network-constructionfont"><font color=red>Data Process And Network Construction</font><a hidden class="anchor" aria-hidden="true" href="#font-colorreddata-process-and-network-constructionfont">#</a></h3>
<blockquote>
<p>以官方的2645.csv数据集为例子</p>
</blockquote>
<p><strong>agent node:</strong></p>
<ul>
<li><code>data['city']:</code>城市名称</li>
<li><code>data['trajs'] = [agt_traj] + ctx_trajs:</code>轨迹点，(agent + context vehicles)</li>
<li><code>data['steps'] = [agt_step] + ctx_steps:</code>在原始数据中的位置</li>
<li><code>data['feats'] = feats:</code> (13 X 20 X 3) 前20预测轨迹 + 一维是否存在点</li>
<li><code>data['ctrs'] = ctrs:</code> (13 X 2) 中心点</li>
<li><code>data['orig'] = orig:</code> AGENT 当前点坐标</li>
<li><code>data['theta'] = theta:</code> AGENT 偏转角</li>
<li><code>data['rot'] = rot:</code> (2 X 2) 旋转矩阵</li>
<li><code>data['gt_preds'] = gt_preds:</code>(13 X 30 X 2) 后30帧真实轨迹</li>
<li><code>data['has_preds'] = has_preds:</code> (13 X 30) 标识后30帧轨迹是否存在</li>
</ul>
<p><strong>lane node:</strong></p>
<ul>
<li><code>graph['ctrs'] = np.concatenate(ctrs, 0):</code> lane node的中心点坐标</li>
<li><code>graph['num_nodes'] = num_nodes:</code> lane node的数量</li>
<li><code>graph['feats'] = np.concatenate(feats, 0):</code> lane node 方向向量</li>
<li><code>graph['turn'] = np.concatenate(turn, 0):</code> lane node 转向标识</li>
<li><code>graph['control'] = np.concatenate(control, 0):</code> lane node 的 has_traffic_control 标识</li>
<li><code>graph['intersect'] = np.concatenate(intersect, 0):</code> lane node 的 is_intersection 标识</li>
<li><code>graph['pre'] = [pre]:</code> pre[&lsquo;u&rsquo;] 和 pre[&lsquo;v&rsquo;], v 是 u 的pre， 这里表述的是lane node之间的关系</li>
<li><code>graph['suc'] = [suc]:</code> suc[&lsquo;u&rsquo;] 和 suc[&lsquo;v&rsquo;], v 是 u 的suc， 这里表述的是lane node之间的关系</li>
<li><code>graph['lane_idcs'] = lane_idcs:</code> lane node index
<ul>
<li>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="o">...</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="o">...</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span> 
</span></span><span class="line"><span class="cl"><span class="mi">83</span> <span class="mi">83</span> <span class="mi">83</span> <span class="o">...</span> <span class="mi">83</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
<li><code>graph['pre_pairs'] = pre_pairs:</code> pair 表述的是lane之间的关系</li>
<li><code>graph['suc_pairs'] = suc_pairs:</code> pair 表述的是lane之间的关系</li>
<li><code>graph['left_pairs'] = left_pairs:</code> pair 表述的是lane之间的关系</li>
<li><code>graph['right_pairs'] = right_pairs:</code> pair 表述的是lane之间的关系
<ul>
<li>对于<code>pre['u']</code>和<code>pre['v']</code>, v 是 u 的 pre</li>
<li>对于<code>suc['u']</code>和<code>suc['v']</code>, v 是 u 的 suc</li>
<li>对于<code>left['u']</code>和<code>left['v']</code>, v 是 u 的 left</li>
<li>对于<code>right['u']</code>和<code>right['v']</code>, v 是 u 的 right</li>
</ul>
</li>
</ul>
<p><strong>Net结构</strong></p>
<ul>
<li><strong>ActorNet</strong>
<code>input:</code> M x 3 x 20
<code>output:</code> M x 128 x 20</li>
</ul>
<p>解释:</p>
<ul>
<li>
<p><strong>MapNet</strong>: 把 v 按照 u 加到center上
<code>input:</code> N x 4
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>A2M</strong>
<code>input:</code> N x 128
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>M2M</strong>
<code>input:</code> N x 128
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>M2A</strong>
<code>input:</code> N x 128
<code>output:</code> M x 128</p>
</li>
<li>
<p><strong>A2A</strong>
<code>input:</code> N x 128
<code>output:</code> N x 128</p>
</li>
<li>
<p><strong>Prediction Header:</strong>
<code>input</code> M x 128</p>
<ul>
<li>MLP Regression</li>
<li>MLP Classification</li>
</ul>
</li>
</ul>
<p>ref link: <a href="https://zhuanlan.zhihu.com/p/447129428">https://zhuanlan.zhihu.com/p/447129428</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jianye0428.github.io/en/tags/prediction/">prediction</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jianye0428.github.io/en/posts/notes/2022-06-17_pytorch_dataset_dataloader/">
    <span class="title"><i class="fas fa-angle-double-left"></i> Prev Page</span>
    <br>
    <span>The Utilization of Dataset and DataLoader</span>
  </a>
  <a class="next" href="https://jianye0428.github.io/en/posts/tech/2022-06-12_social_nce/">
    <span class="title">Next Page <i class="fas fa-angle-double-right"></i></span>
    <br>
    <span>Social_NCE: Contrastive Learning of Socially-aware Motion Representation</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share LaneGCN: Learning Lane Graph Representations for Motion Forecasting on twitter"
        href="https://twitter.com/intent/tweet/?text=LaneGCN%3a%20Learning%20Lane%20Graph%20Representations%20for%20Motion%20Forecasting&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2ftech%2f2022-06-13_lanegcn%2f&amp;hashtags=prediction">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LaneGCN: Learning Lane Graph Representations for Motion Forecasting on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2ftech%2f2022-06-13_lanegcn%2f&amp;title=LaneGCN%3a%20Learning%20Lane%20Graph%20Representations%20for%20Motion%20Forecasting&amp;summary=LaneGCN%3a%20Learning%20Lane%20Graph%20Representations%20for%20Motion%20Forecasting&amp;source=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2ftech%2f2022-06-13_lanegcn%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LaneGCN: Learning Lane Graph Representations for Motion Forecasting on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2ftech%2f2022-06-13_lanegcn%2f&title=LaneGCN%3a%20Learning%20Lane%20Graph%20Representations%20for%20Motion%20Forecasting">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LaneGCN: Learning Lane Graph Representations for Motion Forecasting on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2ftech%2f2022-06-13_lanegcn%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LaneGCN: Learning Lane Graph Representations for Motion Forecasting on whatsapp"
        href="https://api.whatsapp.com/send?text=LaneGCN%3a%20Learning%20Lane%20Graph%20Representations%20for%20Motion%20Forecasting%20-%20https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2ftech%2f2022-06-13_lanegcn%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LaneGCN: Learning Lane Graph Representations for Motion Forecasting on telegram"
        href="https://telegram.me/share/url?text=LaneGCN%3a%20Learning%20Lane%20Graph%20Representations%20for%20Motion%20Forecasting&amp;url=https%3a%2f%2fjianye0428.github.io%2fen%2fposts%2ftech%2f2022-06-13_lanegcn%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>




<footer class="tc-container" id="comment">
    <div class="tc-title"><p class="c-title">Discussion</p></div>
    <div id="tcomments"></div>
</footer>
<script crossorigin="anonymous" src="/js/twikoo.min.b16100b7cf8a61759eab076a122482054e083087aad37c3be1fe2e293934dc34.js" integrity="sha256-sWEAt8&#43;KYXWeqwdqEiSCBU4IMIeq03w74f4uKTk03DQ="></script>
<script>
    twikoo.init({
        envId: 'https://my-repository-pink.vercel.app/',
        el: '#tcomments',
        region: 'ap-shanghai', 
        
        lang: 'zh-CN', 
    });
</script>

</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
