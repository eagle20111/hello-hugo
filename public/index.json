[{"content":"paper link: https://arxiv.org/abs/2007.13732\nArchitechture Lane Graph + Actor Map:\n  construct lane graph from vectorized map data to preserve the map structure and can avoid information loss\n  LaneGCN:\n  extends graph convolutions with multiple adjacency matrices and along-lane dilation\n to capture complex topology and long range dependencies of the lane graph.    exploit a fusion network consisting of four types of interactions: actor-to-lane, lane-to-actor, actor-to-actor, lane-to-lane.\n present both actors and lanes as nodes in the graph and use a 1D CNN and LaneGCN to extract the features for the actor and lane nodes respectively, and then exploit spatial attention and another LaneGCN to model four types of interactions.      Difference between VectorNet and LaneGCN:\n VecotrNet uses vanilla graph networks with undirected full connections; LaneGCN uses connected lane graph folllowing the map topology and propose task specific multi-type and dilated graph operators. VectorNet uses polyline-level nodes for interactions; LaneGCN uses polyline segments as map nodes to capture higher resolution.  Lane Graph Representations for Motion Forecasting ActorNet: Extracting Traffic Participant Representations Each Trajctory is represented as a sequence of displacement ${ \\bigtriangleup{p_{-(T-1)},\u0026hellip;,\\bigtriangleup{p_{-1}}, \\bigtriangleup{p_0}}}$, where $\\bigtriangleup{p_t}$ is the 2D displacement from time step $t-1$ to t, and T is the trajectory size.\nFor trajectories with sizes smaller than $T$ , we pad them with zeros. We add a binary $1 × T$ mask to indicate if the element at each step is padded or not and concatenate it with the trajectory tensor, resulting in an input tensor of size $3 × T$.\n1D CNN is used to process the trajectory input for its effectiveness in extracting multi-scale features and efficiency in parallel computing. The output of ActorNet is a temporal feature map, whose element at $t = 0$ is used as the actor feature. The network has 3 groups/scales of 1D convolutions.\nEach group consists of 2 residual blocks, with the stride of the first block as 2. We then use a Feature Pyramid Network (FPN) to fuse the multi-scale features, and apply another residual block to obtain the output tensor. For all layers, the convolution kernel size is 3 and the number of output channels is 128. Layer normalization and the Rectified Linear Unit (ReLU) are used after each convolution.\nMapNet: Extracting Structured Map Representation General Architecture:\n part 1: building a lane graph from vectorized map data; part 2: applying our novel LaneGCN to the lane graph to output the map features.  Map Data:\nIn this paper, we adopt a simple form of vectorized map data as our representation of HD maps. Specifically, the map data is represented as a set of lanes and their connectivity. Each lane contains a centerline, i.e., a sequence of 2D BEV points, which are arranged following the lane direction (see Fig. 3, top). For any two lanes which are directly reachable, 4 types of connections are given: predecessor, successor, left neighbour and right neighbour.\nLane Graph Construction:\nfirst define a lane node as the straight line segment formed by any two consecutive points (grey circles in Fig. 3) of the centerline. The location of a lane node is the averaged coordinates of its two end points. Following the connections between lane centerlines, we also derive 4 connectivity types for the lane nodes, i.e., predecessor, successor, left neighbour and right neighbour.\nWe denote the lane nodes with $V ∈ \\mathbb R^{N ×2}$ , where $N$ is the number of lane nodes and the $i$-th row of $V$ is the BEV coordinates of the $i$-th node. We represent the connectivity with 4 adjacency matrices ${\\lbrace A_i \\rbrace}_{i \\in {pre,suc,left,right}}$ , with $A_i \\in \\mathbb R^{N ×N}$.\nWe denote $A_{i,jk}$, as the element in the $j$-th row and $k$-th column of $A_i$. Then $A_{i,jk} = 1$ if node $k$ is an $i$-type neighbor of node $j$.\nLaneConv Operator:\nNode Feature:\nEach lane node corresponds to a straight line segment of a centerline. To encode all the lane node information, we need to take into account both the shape (size and orientation) and the location (the coordinates of the center) of the corresponding line segment. We parameterize the node feature as follows,\n$$x_i = MLP_{shape} (v_{i}^{end} - v_{i}^{start}) + MLP_{loc}(v_i) $$\nwhere $MLP$ indicates a multi-layer perceptron and the two subscripts refer to shape and location, respectively. $v_i$ is the location of the i-th lane node, i.e., the center between two end points, $v_i^{start}$ and $v_i^{end}$ are the BEV coordinates of the node $i’s$ starting and ending points, and $x_i$ is the $i$-th row of the node feature matrix $X$, denoting the input feature of the $i$-th lane node.\nLaneConv: \nTo aggregate the topology information of the lane graph at a larger scale, we design the following LaneConv operator:\n$$Y = XW_0 + \\sum_{i\\in{pre, suc, left, right}}A_iXW_i,\\tag{2}$$\nwhere $A_i$ and $W_i$ are the adjacency and the weight matrices corresponding to the $i$-th connection type respectively. Since we order the lane nodes from the start to the end of the lane, $A_{suc}$ and $A_{pre}$ are matrices obtained by shifting the identity matrix one step towards upper right (non-zero superdiagonal) and lower left (non-zero subdiagonal). $A_{suc}$ and $A_{pre}$ can propagate information from the forward and backward neighbours whereas $A_{left}$ and $A_{right}$ allow information to flow from the cross-lane neighbours. It is not hard to see that our LaneConv builds on top of the general graph convolution and encodes more geometric (e.g., connection type/direction) information. As shown in our experiments this improves over the vanilla graph convolution.\nDilated LaneConv:\nFunctionality: The model needs to capture the long range dependency along the lane direction for accurate prediction.\nthe k-dilation LaneConv operator is defined as follows:\n$$Y = XW_0 + A_{pre}^k XW_{pre,k} + A_{suc}^k X W_{suc,k} \\tag{3}$$\nwhere $A_{pre}^k$ is the $k$-th matrix power of $A_{pre}$. This allows us to directly propagate information along the lane for $k$ steps, with $k$ a hyperparameter. Since $A_{pre}^k$ is highly sparse, one can efficiently compute it using sparse matrix multiplication. Note that the dilated LaneConv is only used for predecessor and successor, as the long range dependency is mostly along the lane direction.\nLaneGCN:\nWith Eq.(2) and Eq.(3), we get a multi-scale LaneConv operator with C dilation size as follows:\n$$Y = XW_0 + \\sum_{i\\in \\lbrace left, right \\rbrace} A_i X W_i + \\sum_{c=1}^C (A_{pre}^{k_c}XW_{pre, k_c} + A_{suc}^{k_c}XW_{suc, k_c})， \\tag{4}$$\nwhere $k_c$ is the $c$-th dilation size. We denote $LaneConv(k_1 , · · · , k_C)$ this multi-scale layer.\nFusion Net Four types fusion modules:\n A2L: introduces real-time traffic information to lane nodes, such as blockage or usage of the lanes. L2L: updates lane node features by propagating the traffic information over the lane graph. -\u0026gt; LaneGCN L2A: fuses updated map features with real-time traffic information back to the actors. A2A: handles the interactions between actors and produces the output actor features, which are then used by the prediction header for motion forecasting.  We implement L2L using another LaneGCN, which has the same architecture as the one used in our MapNet (see Section 3.2). In the following we describe the other three modules in detail. We exploit a spatial attention layer for A2L, L2A and A2A. The attention layer applies to each of the three modules in the same way. Taking A2L as an example, given an actor node i, we aggregate the features from its context lane nodes j as follows:\n$$y_i = x_i W_0 + \\sum_j \\phi (concat(x_i, \\Delta_{i,j}, x_j)W_1)W_2, \\tag{5}$$\nwith $x_i$ the feature of the $i$-th node, $W$ a weight matrix, $\\phi$ the compositon of layer notmalization and RelU, and $\\Delta_{ij} = MLP(v_j - v_i)$, where $v$ denotes the node location.\nPrediction Header Take after-fusion actor features as input, a multi-modal prediction header outputs the final motion forecasting. For each actor, it predicts $K$ possible future trajectories and their confidence scores.\nThe header has two branches, a regression branch to predict the trajectory of each mode and a classification branch to predict the confidence score of each mode.\nFor the m-th actor, we apply a residual block and a linear layer in the regression branch to regress the K sequences of BEV coordinates:\n$$O_{m,reg} = \\lbrace (p_{m,1}^k, p_{m,2}^k, \u0026hellip;, p_{m,T}^k) \\rbrace _{k\\in[0,K-1]}$$\nwhere $p_{m,i}^k$ is the predicted $m$-th actor\u0026rsquo;s BEV coordinates of the $k$-th mode at the $i$-th time step. For the classification branch, we apply an MLP to $p^k_{m,T} − p_{m,0}$ to get $K$ distance embeddings. We then concatenate each distance embedding with the actor feature, apply a residual block and a linear layer to output $K$ confidence scores, $O_{m,cls} = (c_{m,0}, c_{m,1}, \u0026hellip;, c_{m,K−1})$.\nLearning use the sum of classification and regreesion losses to train the model:\n$$ L = L_{cls} + \\alpha L_{reg},$$\nwhere $\\alpha = 1.0$.\nFor classification, we use the max-margin loss:\n$$L_{cls} = \\frac{1}{M(K-1)}\\sum_{m=1}^M \\sum_{k \\neq \\hat{k}} \\max(0, c_{m,k} + \\epsilon - c_{m, \\hat{k}}) \\tag{6}$$\nwhere $\\epsilon$ is the margin and $M$ is the total number of actors. For regression, we apply the smooth $l1$ loss on all predicted time steps:\n$$L_{reg} = \\frac{1}{MT} \\sum_{m=1}^M \\sum_{t=1}^T reg(p_{m,y}^{\\hat{k}} - p_{m,t}^*) \\tag{7}$$\nwhere $p_t^*$ is the ground truth BEV coordinates at time step $t$, $reg(x) = \\sum\\limits_i d(x_i)$, $x_i$ is the $i$-th element of $x$, and $d(x_i)$ is the smooth $\\ell1$ loss defined as:\n$$d(x_i) = \\begin{cases} 0.5x_i^2 \u0026amp;\\text{if} ||x|| \u0026lt; 1, \\ ||x_i|| - 0.5 \u0026amp; \\text{otherwise,} \\end{cases} \\tag{8}$$\nwhere $||x_i||$ denotes the $\\ell1$ norm of $x_i$.\n","permalink":"https://jianye0428.github.io/posts/tech/2022-06-13_lanegcn/","summary":"paper link: https://arxiv.org/abs/2007.13732\nArchitechture Lane Graph + Actor Map:\n  construct lane graph from vectorized map data to preserve the map structure and can avoid information loss\n  LaneGCN:\n  extends graph convolutions with multiple adjacency matrices and along-lane dilation\n to capture complex topology and long range dependencies of the lane graph.    exploit a fusion network consisting of four types of interactions: actor-to-lane, lane-to-actor, actor-to-actor, lane-to-lane.","title":"LaneGCN: Learning Lane Graph Representations for Motion Forecasting"},{"content":"paper link: https://arxiv.org/abs/2012.11717\n论文解读参考:\n[1] https://zhuanlan.zhihu.com/p/434650863\n[2] https://www.gushiciku.cn/pl/amod\nIssue to solve and its Solution Due to the ill-distributed training Data, it\u0026rsquo;s difficult to capture the notion of the \u0026ldquo;negative\u0026rdquo; examples like collision.\nSolution:\nModeling the negative samples through self-supervision:\n a social contrastive loss: regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; Construct negative samples based on prior knowledge of rare but dangerous circumstances.  a social sampling strategy (informed): construct the positive enven from the ground-truth location of the primary agent and the negative events from the regions of other neighbors. given that one location cannot be occupied by multiple agents at the same time.\n   Method: Contrastive Learning + Social NCE Contrastive Representation Learning   Functionality:\n  Representation Learning: to learn a parametric function that maps the raw data into a feature space to extract abstract and useful information for downstream tasks.\n  NCE(Noise Contrastive Estimation): to train encoder\n  $$\\mathcal{L_{NCE}} = -\\log \\frac{\\exp(sim(q,k^+)/\\tau)}{\\sum_{n=0}^N \\exp(sim(q,k_n)/ \\tau)}$$\nwhere the encoded query $q$ is brought close to one positive key $k_0 = k^+$ and pushed apart from $N$ negative keys ${ k_1, k_2, \u0026hellip; , k_N}$, $\\tau$ is a temperature hyperparameter, and $sim(u,v) = u^{\\mathsf{T}}v/(||u||||v||)$ is the cosine similarity between two feature vectors.\n  Social NCE Social NCE Description:\n智能体 $i$ 在时刻 $t$ 上的位置记为 $s^i_t=(x^i_t,y^i_t)$ 。那么 $M$ 个智能体的联合状态记为 $s_t = { s_t^1, \u0026hellip;, s^M_t}$ 。给定一个历史观测序列 ${s_1, s_2, \u0026hellip;, s_t}$ ，任务是预测所有智能体未来直至 $T$ 时刻的轨迹 ${s_{t+1}, \u0026hellip;, s_T}$，许多最近的预测模型被设计为编码器 - 解码器神经网络，其中运动编码器 $f(\\cdot)$ 首先提取与 $i$ 相关的紧密表示 $h_t^i$ ，然后解码器 $g(\\cdot)$ 随后推测出其未来的轨迹 $\\hat{s}^i_{t+1,T}$ :\n$$h^i_t = f(s_{1:t}, i), $$ $$\\hat{s}^i_{t+1:T} = g(h^i_t)$$\n为了多智能体之间的社交互动，$f(\\cdot)$通常包含两个子模块：一个序列建模模块 $f_S(\\cdot)$ 用于编码每个单独的序列，以及一个交互模块 $f_I(\\cdot)$ 用于在多智能体之间共享信息：\n$$z^i_t = f_S(h^i_{t-1}, s^i_t),$$ $$h^i_t = f_I(z_t, i)$$\n其中， $z^i_t$ 是给定智能体 $i$ 在时间 $t$ 观察其自身状态的潜在表示， $z_t = {z^1_t,\u0026hellip;,z^M_t}$ 。很多方法已经探索了各种架构，并验证了其准确性。尽管如此，它们的鲁棒性仍然是一个悬而未决的问题。 最近的几项工作表明，现有模型预测的轨迹通常会输出社会不可接受的解决方案（例如，碰撞），表明缺乏关于社会准则的常识。\n  query: embedding of history observations $q = \\psi(h^i_t)$, where $\\psi(\\cdot)$ is an MLP projection head;\n  key: embedding of a future event $k = \\phi(s^i_{s+\\delta t}, \\delta t)$, where $\\phi(\\cdot)$ is an event encoder modeled by an MLP, $s_{t+\\delta t}^i$ is a sampled spatial location and $\\delta_t \u0026gt; 0$ is the sampling horizon.\n tuning $\\delta_t \\in \\Lambda$, e.g. $\\Lambda = {1,\u0026hellip;,4}$, then future events in the next few step can be taken in account simultaneously. Nevertheless, when $\\delta_t$ is a fixed value, then $\\phi(\\cdot)$ can be simplified as a location encoder, i.e., $\\phi(s^i_{t+\\delta t})$.\n   给定一个场景，包括感兴趣的主智体（蓝色）和附近多个相邻智体（灰色），Social-NCE 损失鼓励在嵌入空间中提取的运动表示，接近未来的正样本事件，并远离可能导致碰撞或不适的合成负样本事件. Social NCE的损失函数如下:\n$$\\mathcal{L_{SocialNCE}} = -\\log\\frac{\\exp(\\psi(h^i_t)\\cdot\\phi(s^{i,+}{t+\\delta t}, \\delta t)/\\tau)}{\\sum{\\delta t\\in\\Lambda}\\sum_{n=0}^{N}\\exp(\\psi(h^i_t)\\cdot\\phi(s^{i,n}_{t+\\delta t}, \\delta t)/\\tau))}$$\n最终的训练损失函数为Social-NCE和传统任务损失项之和，即轨迹预测的mean squared error (MSE) 或者negative log-likelihood (NLL)：\n$$\\mathcal{L}(f,g,\\psi, \\phi) = \\mathcal{L}{task}(f,g) + \\lambda \\mathcal{L}{SocialNCE}(f, \\psi, \\phi)$$\n其中，$\\lambda$ 为超参数，控制SocialNCE损失函数的重要程度。\nsampling strategy in multi-agent context 采样策略 在其他智能体附近寻求更多信息的负样本:\n$$s^{i,n-}{t+\\delta t} = s^{j}{t+\\delta t} + \\bigtriangleup{s_p} + \\epsilon$$\n其中， $j\\in{1,2,\u0026hellip;,M} \\backslash i$ 是其他agent的index, $\\bigtriangleup{s_p}$ 是适合社交距离的局部位移。\n对于positive sample, 对该agent周围直接采样获得:\n$$s^{i,n-}{t+\\delta t} = s^{i}{t+\\delta t} + \\epsilon$$\n","permalink":"https://jianye0428.github.io/posts/tech/2022-06-12_social_nce/","summary":"paper link: https://arxiv.org/abs/2012.11717\n论文解读参考:\n[1] https://zhuanlan.zhihu.com/p/434650863\n[2] https://www.gushiciku.cn/pl/amod\nIssue to solve and its Solution Due to the ill-distributed training Data, it\u0026rsquo;s difficult to capture the notion of the \u0026ldquo;negative\u0026rdquo; examples like collision.\nSolution:\nModeling the negative samples through self-supervision:\n a social contrastive loss: regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; Construct negative samples based on prior knowledge of rare but dangerous circumstances.","title":"Social_NCE: Contrastive Learning of Socially-aware Motion Representation"},{"content":"1. torch.einsum() torch.einsum(equation, *operands)-\u0026gt;Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187\n三条基本规则:\n 规则一: equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， \u0026ldquo;ik,kj-\u0026gt;ij\u0026rdquo;，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作； 规则二: 只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引； 规则三: equation 箭头右边的索引顺序可以是任意的，比如上面的 \u0026ldquo;ik,kj-\u0026gt;ij\u0026rdquo; 如果写成 \u0026ldquo;ik,kj-\u0026gt;ji\u0026rdquo;，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成  特殊规则:\n equation 可以不写包括箭头在内的右边部分，那么在这种情况下，输出张量的维度会根据默认规则推导。就是把输入中只出现一次的索引取出来，然后按字母表顺序排列，比如上面的矩阵乘法 \u0026ldquo;ik,kj-\u0026gt;ij\u0026rdquo; 也可以简化为 \u0026ldquo;ik,kj\u0026rdquo;，根据默认规则，输出就是 \u0026ldquo;ij\u0026rdquo; 与原来一样； equation 中支持 \u0026ldquo;\u0026hellip;\u0026rdquo; 省略号，用于表示用户并不关心的索引。比如只对一个高维张量的最后两维做转置可以这么写： 1 2 3  a = torch.randn(2,3,5,7,9) # i = 7, j = 9 b = torch.einsum(\u0026#39;...ij-\u0026gt;...ji\u0026#39;, [a])     2. torch.permute()/torch.transpose() torch.permute(dim0, dim1, dim2):用于调换不同维度的顺序 torch.transpose(input, dim0, dim1):交换矩阵的两个维度\n3. torch.rand() torch.rand(dim0, dim1):生成dim0 x dim1的tensor\n4. torch.size()/torch.shape torch.size():返回tensor的size torch.shape:返回tensor的size\n5. torch.tensordot() ref: tensordot()和einsum()的区别: https://blog.csdn.net/Eric_1993/article/details/105670381 torch.tensordot(tensor1， tensor2， axes=([dim1,dim2],[dim0, dim1])): 将axes指定的子数组进行点乘, axes 指定具体的维度.\n6. torch.transpose() torch.transpose(tensor, dim0, dim2) —\u0026gt; Tensor:在dim0和dim1方向上转置\n","permalink":"https://jianye0428.github.io/posts/notes/2022-06-09_pytorch/","summary":"1. torch.einsum() torch.einsum(equation, *operands)-\u0026gt;Tensor:爱因斯坦求和 ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252 ref2: 例子: https://zhuanlan.zhihu.com/p/361209187\n三条基本规则:\n 规则一: equation 箭头左边，在不同输入之间重复出现的索引表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， \u0026ldquo;ik,kj-\u0026gt;ij\u0026rdquo;，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作； 规则二: 只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引； 规则三: equation 箭头右边的索引顺序可以是任意的，比如上面的 \u0026ldquo;ik,kj-\u0026gt;ij\u0026rdquo; 如果写成 \u0026ldquo;ik,kj-\u0026gt;ji\u0026rdquo;，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成  特殊规则:\n equation 可以不写包括箭头在内的右边部分，那么在这种情况下，输出张量的维度会根据默认规则推导。就是把输入中只出现一次的索引取出来，然后按字母表顺序排列，比如上面的矩阵乘法 \u0026ldquo;ik,kj-\u0026gt;ij\u0026rdquo; 也可以简化为 \u0026ldquo;ik,kj\u0026rdquo;，根据默认规则，输出就是 \u0026ldquo;ij\u0026rdquo; 与原来一样； equation 中支持 \u0026ldquo;\u0026hellip;\u0026rdquo; 省略号，用于表示用户并不关心的索引。比如只对一个高维张量的最后两维做转置可以这么写： 1 2 3  a = torch.randn(2,3,5,7,9) # i = 7, j = 9 b = torch.einsum(\u0026#39;...ij-\u0026gt;...ji\u0026#39;, [a])     2.","title":"Pytorch Notes"},{"content":"paper link: https://arxiv.org/abs/2002.11927?from=leiphonecolumn_paperreview0323\n网络结构 特点: Social STGCNN不同于其他方法只是聚合各种学习的行人状态，而是对行人交互做图建模。其中提出一种kernel function把行人社交交互嵌入一个adjacency matrix。\n 代码显示，图建模一般在数据前处理完成。\n Model Description 两部分：时空图卷积神经网络ST-GCNN、时间外推器TXP-CNN。\nST-GCNN对行人轨迹的图表示进行时空卷积操作以提取特征。这些特征是观察到的行人轨迹历史的紧凑表示。 TXP-CNN将这些特征作为输入，并预测所有行人作为一个整体的未来轨迹。我们使用时间外推器的名字是因为TXP-CNN期望通过卷积运算外推未来的轨迹。\n给定T帧，构造表示 $G=(V,A)$ 的时空图. 然后，$G$ 通过时空图卷积神经网络(ST-GCNNs)转发，创建一个时空嵌入。 之后，TXP-CNNs 预测了未来的轨迹。 $P$ 是行人位置的维数，$N$ 是行人的数目，$T$ 是时间步长, $\\hat{P}$是来自ST-GCNN的嵌入的维数.\n(1) Graph Representation of Pedestrian Trajectories\n我们首先构造一组空间图 $G_t$，表示每个时间步长 $t$ 在场景中行人的相对位置，$G_t = (V_t, E_t)$ 。 $V_t$是图 $G_t$ 的顶点集，观察到的位置 $(x^i_t，y^i_t)$ 是顶点 $v^i_t$ 的属性; $E_t$ 是边集，如果顶点 $v^i_t$ 和顶点 $v^j_t$ 相连 $e^{ij}_t = 1$ ，否则 $=0$。\n为了建模两个节点之间相互影响的强度，我们附加了一个值$a^{ij}_t$, 它是由每个$ e^{ij}_t$ 的某种核函数计算得到。$a^{ij}_t$ 被组织为带权邻接矩阵$A_t$。\n$a^{ij}_{sim,t}$是要在邻接矩阵$A_t$中使用的内核函数。 定义为:\n$$\\begin{equation} a^{ij}_{sim,t}= \\left { \\begin{aligned} 1/||v^i_t - v^j_t||_2 , ||v^i_t - v^j_t||_1\\neq0 \\ 0, Otherwise \\end{aligned} \\right. \\end{equation}$$\n(2) Graph Convolution Neural Network\n对于在二维网格地图或特征地图上定义的卷积运算，定义如下:\n$$z^{(l+1)} = \\sigma(\\sum_{h=1}^{k}\\sum_{\\omega=1}^{k}(p(z^{(l)},h, \\omega) \\cdot \\boldsymbol{W}^{(l)}(h, \\omega))$$\n其中，$k$是内核大小，$p(.)$ 是采样函数，其聚集以$z$为中心的邻居的信息， $\\sigma$ 是激活函数。${l}$表示神经网络层。\n图卷积定义如下:\n$$v^{i(l+1)} =\\sigma (\\frac{1}{\\Omega}\\sum_{v^{j(l)}\\in B(v^{j(l)})}p(v^{i(l)}, v^{j(l)}) \\cdot \\boldsymbol{W}(v^{i(l)}, v^{j(l)}))$$\n其中$\\frac{1}{\\Omega}$ 是正则化项，$B(v^i) = { v^j|d(v^i,v^j)≤D }$是顶点的邻居集，而$d(v^i,v^j)$表示连接$v^i$和$v^j$的最短距离， $\\Omega$是邻居集的基数。\n(3) Spatio-Temporal Graph Convolution Neural Network(ST-GCNNs)\n通过定义一个新的图G，其属性是$G_t$属性的集合，ST-GCNN将空间图卷积扩展到时空图卷积。 $G$结合了行人轨迹的时空信息。值得注意的是，$G_1，…，G_T$的拓扑结构是相同的，而当t变化时，不同的属性被分配给$v^i_t$。\n因此，我们将$G$定义为$(V,E)$，其中$V={v_i|i\\in { 1，…，N }}$ 和 $E={e_{ij}|i，j，{1，…，N}}$。 顶点$v_i$在G中的属性是$v^i_t$的集合，$∀t∈{0，…，T}$。 另外， 加权邻接矩阵A对应于$G$ 是${ A_1，…，A_T}$的集合。 我们将ST-GCNN产生的嵌入表示为 $\\overline{V}$.\n(4) Time-Extrapolator Convolution Neural Network (TXP-CNN)\nST-GCNN的功能是从输入图中提取时空节点嵌入。然而，我们的目标是预测行人未来的进一步位置。 TXP-CNN直接作用于图嵌入 $\\overline{V}$ 的时间维度，并将其扩展为预测的必要条件。 由于TXP-CNN依赖于特征空间的卷积运算，因此与递归单元相比，它的参数较小。需要注意的一个特性是， TXP-CNN层不是置换不变的，因为在TXP-CNN之前，图嵌入的变化会导致不同的结果。Other than this, if the order of pedestrians is permutated starting from the input to Social-STGCNN then the predictions are invariant.\nmodel(Social STGCNN) Implementation  Adjacency Matrix Normalization  $$ A_t = \\Lambda_t^{-\\frac{1}{2}}\\hat{A}\\Lambda_t^{-\\frac{1}{2}}$$\nwhere $\\hat{A_t} = A_t + I$ and $\\Lambda_t$ is the diagonal node degree matric of $\\hat{A_t}$. We use $\\hat{A}$ and $\\Lambda$ to denote the stack of $\\hat{A_t}$ and $\\Lambda_t$ repectively.\nThe normalization of adjacency is essential for the graph CNN to work properly.\nSTGCNN Network Mechanism  $$f(V^{l}, A) = \\sigma(\\Lambda_t^{-\\frac{1}{2}}\\hat{A}\\Lambda_t^{-\\frac{1}{2}}V^{(l)}W^{(l)})$$\nwhere, $V^{(l)}$ denotes the stack of $V^{(l)}_t$, and $W^{(l)}$ denotes the trainable parameters.\nData Processing 数据处理以及图构建 obs_traj - 前8帧观察轨迹(绝对坐标)\npred_traj_gt - 后12帧预测轨迹(ground truth)(绝对坐标)\nobs_traj_rel - 前8帧观察轨迹(相对坐标)\npred_traj_gt_rel - 后12帧预测轨迹(ground truth)(相对坐标)\nnon_linear_ped - 非线性轨迹 (剔除)\nloss_mask V_obs - graph nodes\nA_obs - graph Adjacency Matrix\nV_tr - 预测轨迹 graph nodes\nA_tr - 预测轨迹 graph Adjacency Matrix\n","permalink":"https://jianye0428.github.io/posts/tech/2022-06-08_social_stgcnn/","summary":"paper link: https://arxiv.org/abs/2002.11927?from=leiphonecolumn_paperreview0323\n网络结构 特点: Social STGCNN不同于其他方法只是聚合各种学习的行人状态，而是对行人交互做图建模。其中提出一种kernel function把行人社交交互嵌入一个adjacency matrix。\n 代码显示，图建模一般在数据前处理完成。\n Model Description 两部分：时空图卷积神经网络ST-GCNN、时间外推器TXP-CNN。\nST-GCNN对行人轨迹的图表示进行时空卷积操作以提取特征。这些特征是观察到的行人轨迹历史的紧凑表示。 TXP-CNN将这些特征作为输入，并预测所有行人作为一个整体的未来轨迹。我们使用时间外推器的名字是因为TXP-CNN期望通过卷积运算外推未来的轨迹。\n给定T帧，构造表示 $G=(V,A)$ 的时空图. 然后，$G$ 通过时空图卷积神经网络(ST-GCNNs)转发，创建一个时空嵌入。 之后，TXP-CNNs 预测了未来的轨迹。 $P$ 是行人位置的维数，$N$ 是行人的数目，$T$ 是时间步长, $\\hat{P}$是来自ST-GCNN的嵌入的维数.\n(1) Graph Representation of Pedestrian Trajectories\n我们首先构造一组空间图 $G_t$，表示每个时间步长 $t$ 在场景中行人的相对位置，$G_t = (V_t, E_t)$ 。 $V_t$是图 $G_t$ 的顶点集，观察到的位置 $(x^i_t，y^i_t)$ 是顶点 $v^i_t$ 的属性; $E_t$ 是边集，如果顶点 $v^i_t$ 和顶点 $v^j_t$ 相连 $e^{ij}_t = 1$ ，否则 $=0$。\n为了建模两个节点之间相互影响的强度，我们附加了一个值$a^{ij}_t$, 它是由每个$ e^{ij}_t$ 的某种核函数计算得到。$a^{ij}_t$ 被组织为带权邻接矩阵$A_t$。\n$a^{ij}_{sim,t}$是要在邻接矩阵$A_t$中使用的内核函数。 定义为:\n$$\\begin{equation} a^{ij}_{sim,t}= \\left { \\begin{aligned} 1/||v^i_t - v^j_t||_2 , ||v^i_t - v^j_t||_1\\neq0 \\ 0, Otherwise \\end{aligned} \\right.","title":"Social_STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction"},{"content":"最大似然估计（Maximum likelihood estimation, 简称MLE）和最大后验概率估计（Maximum a posteriori estimation, 简称MAP）是很常用的两种参数估计方法，如果不理解这两种方法的思路，很容易弄混它们。下文将详细说明MLE和MAP的思路与区别。\n但别急，我们先从概率和统计的区别讲起。\n概率和统计是一个东西吗？ 概率(probabilty)和统计(statistics)看似两个相近的概念，其实研究的问题刚好相反。\n概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性(例如均值，方差，协方差等等)。 举个例子，我想研究怎么养猪(模型是猪)，我选好了想养的品种、喂养方式、猪棚的设计等等(选择参数)，我想知道我养出来的猪大概能有多肥，肉质怎么样(预测结果)。\n统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉(这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等)，然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等(推测模型参数)。\n一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。\n显然，本文解释的MLE和MAP都是统计领域的问题。它们都是用来推测参数的方法。为什么会存在着两种不同方法呢？ 这需要理解贝叶斯思想。我们来看看贝叶斯公式。\n贝叶斯公式到底在说什么？ 学习机器学习和模式识别的人一定都听过贝叶斯公式(Bayes’ Theorem)： 式[1] $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n贝叶斯公式看起来很简单，无非是倒了倒条件概率和联合概率的公式。\n把B展开，可以写成: 式[2] $P(A|B)=\\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\sim A)P(\\sim A)}$\n这个式子就很有意思了。\n想想这个情况。一辆汽车(或者电瓶车)的警报响了，你通常是什么反应？有小偷？撞车了？ 不。。 你通常什么反应都没有。因为汽车警报响一响实在是太正常了！每天都要发生好多次。本来，汽车警报设置的功能是，出现了异常情况，需要人关注。然而，由于虚警实在是太多，人们渐渐不相信警报的功能了。\n贝叶斯公式就是在描述，你有多大把握能相信一件证据？（how much you can trust the evidence）\n我们假设响警报的目的就是想说汽车被砸了。把$A$计作“汽车被砸了”，$B$计作“警报响了”，带进贝叶斯公式里看。我们想求等式左边发生$A∣B$的概率，这是在说警报响了，汽车也确实被砸了。汽车被砸**引起(trigger)**警报响，即B∣A。但是，也有可能是汽车被小孩子皮球踢了一下、被行人碰了一下等其他原因(统统计作$\\sim A$)，其他原因引起汽车警报响了，即 $B|\\sim A$。那么，现在突然听见警报响了，这时汽车已经被砸了的概率是多少呢(这即是说，警报响这个证据有了，多大把握能相信它确实是在报警说汽车被砸了)想一想，应当这样来计算。用警报响起、汽车也被砸了这事件的数量，除以响警报事件的数量(这即[式1])。进一步展开，即警报响起、汽车也被砸了的事件的数量，除以警报响起、汽车被砸了的事件数量加上警报响起、汽车没被砸的事件数量(这即[式2])。\n再思考[式2]。想让$P(A∣B)=1$，即警报响了，汽车一定被砸了，该怎么做呢？让$P(B|\\sim A)P(\\sim A) = 0$即 可 。很容易想清楚，假若让$P(\\sim A)=0$,即杜绝了汽车被球踢、被行人碰到等等其他所有情况，那自然，警报响了，只剩下一种可能——汽车被砸了。这即是提高了响警报这个证据的说服力。\n**从这个角度总结贝叶斯公式：做判断的时候，要考虑所有的因素。**老板骂你，不一定是你把什么工作搞砸了，可能只是他今天出门前和太太吵了一架。\n再思考[式2]。观察【式2】右边的分子，$P(B∣A)$为汽车被砸后响警报的概率。姑且认为这是1吧。但是，若$P(A)$很小，即汽车被砸的概率本身就很小，则$P(B∣A)P(A)$仍然很小，即【式2】右边分子仍然很小，$P(A|B)$还是大不起来。 这里，$​P(A)$ 即是常说的先验概率，如果A的先验概率很小，就算$P(B∣A)$较大，可能A的后验概率$P(A∣B)$还是不会大(假设$P(B∣\\sim A)P(\\sim A)$不变的情况下)。\n从这个角度思考贝叶斯公式：一个本来就难以发生的事情，就算出现某个证据和他强烈相关，也要谨慎。证据很可能来自别的虽然不是很相关，但发生概率较高的事情。\n似然函数 似然(likelihood)这个词其实和概率(probability)是差不多的意思，Colins字典这么解释:The likelihood of something happening is how likely it is to happen. 你把likelihood换成probability，这解释也读得通。但是在统计里面，似然函数和概率函数却是两个不同的概念(其实也很相近就是了)。\n对于这个函数:\n$$P(x|\\theta)$$\n输入有两个: $x$表示某一个具体的数据；$\\theta$表示模型的参数。\n如果$\\theta$是已知确定的，$x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。\n如果$x$是已知确定的，$\\theta$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现$x$这个样本点的概率是多少。\n最大似然估计(MLE) 假设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正反面出现的概率（记为$\\theta$）各是多少？\n这是一个统计问题，回想一下，解决统计问题需要什么？ 数据！\n于是我们拿这枚硬币抛了10次，得到的数据($x_0$)是：反正正正正反正正正反。我们想求的正面概率$\\theta$是模型参数，而抛硬币模型我们可以假设是二项分布。\n那么，出现实验结果$x_0$(即反正正正正反正正正反)的似然函数是多少呢？\n$$f(x_0 ,\\theta) = (1-\\theta)\\times\\theta\\times\\theta\\times\\theta\\times\\theta\\times(1-\\theta)\\times\\theta\\times\\theta\\times\\theta\\times(1-\\theta) = \\theta ^ 7(1 - \\theta)^3 = f(\\theta)$$ ​ 注意，这是个只关于$\\theta$的函数。而最大似然估计，顾名思义，就是要最大化这个函数。我们可以画出$f(\\theta)$的图像：\n可以看出，在$\\theta = 0.7$时，似然函数取得最大值。\n这样，我们已经完成了对$\\theta$的最大似然估计。即，抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是0.7。（ummm…这非常直观合理，对吧？）\n且慢，一些人可能会说，硬币一般都是均匀的啊！ 就算你做实验发现结果是“反正正正正反正正正反”，我也不信$\\theta = 0.7$。\n这里就包含了贝叶斯学派的思想了——要考虑先验概率。 为此，引入了最大后验概率估计。\n最大后验概率估计(MAP) 最大似然估计是求参数$\\theta$, 使似然函数$P(x_0 | \\theta)$最 大 。 最大后验概率估计则是想求$\\theta$使$P(x_0|\\theta)$ 最大。求得的$\\theta$不单单让似然函数大，不单单让似然函数大，$\\theta$自己出现的先验概率也得大。(这有点像正则化里加惩罚项的思想，不过正则化里是利用加法，而MAP里是利用乘法).\nMAP其实是在最大化$P(\\theta|x_0) = \\frac{P(x_0|\\theta)P(\\theta)}{P(x_0)}$，不过因为$x_0$是确定的(即投出的“反正正正正反正正正反”)，$P(x_0)$是一个已知值，所以去掉了分母$P(x_0)$(假设“投10次硬币”是一次实验，实验做了1000次，“反正正正正反正正正反”出现了n次，则$P(x_0) = n/1000$)。总之，这是一个可以由数据集得到的值）。最大化$P(\\theta | x_0)$的意义也很明确，$x_0$已经出现了，要求$\\theta$取什么值使$P(\\theta | x_0)$最大。顺带一提，$P(\\theta | x_0)$, ​即后验概率，这就是“最大后验概率估计”名字的由来。\n对于投硬币的例子来看，我们认为（”先验地知道“$\\theta$取0.5的概率很大，取其他值的概率小一些。我们用一个高斯分布来具体描述我们掌握的这个先验知识，例如假设$P(\\theta)$为均值0.5，方差0.1的高斯函数，如下图：\n则$P(x_0 | \\theta)$的函数图像为：\n注意，此时函数取最大值时，θ \\thetaθ取值已向左偏移，不再是0.7。实际上，在$\\theta = 0.558$时函数取得了最大值。即，用最大后验概率估计，得到$\\theta = 0.558$\n最后，那要怎样才能说服一个贝叶斯派相信$\\theta = 0.7$呢？你得多做点实验。。\n如果做了1000次实验，其中700次都是正面向上，这时似然函数为:\n如果仍然假设$P(\\theta)$为均值0.5，方差0.1的高斯函数，$P(x_0 | \\theta) P(\\theta)$的函数图像为:\n在$\\theta = 0.696$处，$P(x_0 | \\theta) P(\\theta)$取得最大值。\n这样，就算一个考虑了先验概率的贝叶斯派，也不得不承认得把θ \\thetaθ估计在0.7附近了。\nPS. 要是遇上了顽固的贝叶斯派，认为$P(\\theta = 0.5) = 1$，那就没得玩了。。 无论怎么做实验，使用MAP估计出来都是$\\theta = 0.5$。这也说明，一个合理的先验概率假设是很重要的。（通常，先验概率能从数据中直接分析得到）\n最大似然估计和最大后验概率估计的区别 相信读完上文，MLE和MAP的区别应该是很清楚的了。MAP就是多个作为因子的先验概率$P(\\theta)$。或者，也可以反过来，认为MLE是把先验概率$P(\\theta)$认为等于1，即认为$\\theta$是均匀分布。\nref：https://blog.csdn.net/u011508640/article/details/72815981\n","permalink":"https://jianye0428.github.io/posts/tech/2022-05-30_mle/","summary":"最大似然估计（Maximum likelihood estimation, 简称MLE）和最大后验概率估计（Maximum a posteriori estimation, 简称MAP）是很常用的两种参数估计方法，如果不理解这两种方法的思路，很容易弄混它们。下文将详细说明MLE和MAP的思路与区别。\n但别急，我们先从概率和统计的区别讲起。\n概率和统计是一个东西吗？ 概率(probabilty)和统计(statistics)看似两个相近的概念，其实研究的问题刚好相反。\n概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性(例如均值，方差，协方差等等)。 举个例子，我想研究怎么养猪(模型是猪)，我选好了想养的品种、喂养方式、猪棚的设计等等(选择参数)，我想知道我养出来的猪大概能有多肥，肉质怎么样(预测结果)。\n统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉(这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等)，然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等(推测模型参数)。\n一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。\n显然，本文解释的MLE和MAP都是统计领域的问题。它们都是用来推测参数的方法。为什么会存在着两种不同方法呢？ 这需要理解贝叶斯思想。我们来看看贝叶斯公式。\n贝叶斯公式到底在说什么？ 学习机器学习和模式识别的人一定都听过贝叶斯公式(Bayes’ Theorem)： 式[1] $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n贝叶斯公式看起来很简单，无非是倒了倒条件概率和联合概率的公式。\n把B展开，可以写成: 式[2] $P(A|B)=\\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\\sim A)P(\\sim A)}$\n这个式子就很有意思了。\n想想这个情况。一辆汽车(或者电瓶车)的警报响了，你通常是什么反应？有小偷？撞车了？ 不。。 你通常什么反应都没有。因为汽车警报响一响实在是太正常了！每天都要发生好多次。本来，汽车警报设置的功能是，出现了异常情况，需要人关注。然而，由于虚警实在是太多，人们渐渐不相信警报的功能了。\n贝叶斯公式就是在描述，你有多大把握能相信一件证据？（how much you can trust the evidence）\n我们假设响警报的目的就是想说汽车被砸了。把$A$计作“汽车被砸了”，$B$计作“警报响了”，带进贝叶斯公式里看。我们想求等式左边发生$A∣B$的概率，这是在说警报响了，汽车也确实被砸了。汽车被砸**引起(trigger)**警报响，即B∣A。但是，也有可能是汽车被小孩子皮球踢了一下、被行人碰了一下等其他原因(统统计作$\\sim A$)，其他原因引起汽车警报响了，即 $B|\\sim A$。那么，现在突然听见警报响了，这时汽车已经被砸了的概率是多少呢(这即是说，警报响这个证据有了，多大把握能相信它确实是在报警说汽车被砸了)想一想，应当这样来计算。用警报响起、汽车也被砸了这事件的数量，除以响警报事件的数量(这即[式1])。进一步展开，即警报响起、汽车也被砸了的事件的数量，除以警报响起、汽车被砸了的事件数量加上警报响起、汽车没被砸的事件数量(这即[式2])。\n再思考[式2]。想让$P(A∣B)=1$，即警报响了，汽车一定被砸了，该怎么做呢？让$P(B|\\sim A)P(\\sim A) = 0$即 可 。很容易想清楚，假若让$P(\\sim A)=0$,即杜绝了汽车被球踢、被行人碰到等等其他所有情况，那自然，警报响了，只剩下一种可能——汽车被砸了。这即是提高了响警报这个证据的说服力。\n**从这个角度总结贝叶斯公式：做判断的时候，要考虑所有的因素。**老板骂你，不一定是你把什么工作搞砸了，可能只是他今天出门前和太太吵了一架。\n再思考[式2]。观察【式2】右边的分子，$P(B∣A)$为汽车被砸后响警报的概率。姑且认为这是1吧。但是，若$P(A)$很小，即汽车被砸的概率本身就很小，则$P(B∣A)P(A)$仍然很小，即【式2】右边分子仍然很小，$P(A|B)$还是大不起来。 这里，$​P(A)$ 即是常说的先验概率，如果A的先验概率很小，就算$P(B∣A)$较大，可能A的后验概率$P(A∣B)$还是不会大(假设$P(B∣\\sim A)P(\\sim A)$不变的情况下)。\n从这个角度思考贝叶斯公式：一个本来就难以发生的事情，就算出现某个证据和他强烈相关，也要谨慎。证据很可能来自别的虽然不是很相关，但发生概率较高的事情。\n似然函数 似然(likelihood)这个词其实和概率(probability)是差不多的意思，Colins字典这么解释:The likelihood of something happening is how likely it is to happen.","title":"详解最大似然估计(MLE)、最大后验概率估计(MAP)，以及贝叶斯公式的理解"},{"content":"用pickle保存和加载模型  保存模型 1 2 3 4 5 6 7  import pickle from sklearn.svm import SVC model_dir = \u0026#39;./model.pkl\u0026#39; model = SVC() with open(model_dir, \u0026#39;wb\u0026#39;) as f: pickle.dump(model, f) f.close() # 注意:保存完模型之后要关闭文件    加载模型 1 2 3 4 5  import pickle model_dir = \u0026#39;./model.pkl\u0026#39; with open(model_dir, \u0026#39;rb\u0026#39;) as f: model = pickel.load(f) print(mode.predict(x))     逻辑回归 Logistic Regression  LR Implementation code snippets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np import matplotlib.pyplot as plt import pickle from tqdm import tqdm data_path = \u0026#39;./data/merged_data/data.npy\u0026#39; data = np.load(data_path, allow_pickle=True) model_l1_path=\u0026#39;./model/logistic_reg_l1.pickle\u0026#39; model_l2_path=\u0026#39;./model/logictic_reg_l2.pickle\u0026#39; X = data[:,0:35] y = data[:, -1] X_train, x_test, Y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # lr_l1 = LogisticRegression(penalty=\u0026#34;l1\u0026#34;, C=0.5, solver=\u0026#39;sag\u0026#39;, multi_class=\u0026#34;auto\u0026#34;) # lr_l2 = LogisticRegression(penalty=\u0026#34;l2\u0026#34;, C=0.5, solver=\u0026#39;sag\u0026#39;, multi_class=\u0026#34;auto\u0026#34;) # # train model # lr_l1.fit(X_train, Y_train) # lr_l2.fit(X_train, Y_train) # model performence on train set l1_train_predict = [] l2_train_predict = [] # model performence on test set l1_test_predict = [] l2_test_predict = [] for c in tqdm(np.linspace(0.01, 2, 50)): # lr_l1 = LogisticRegression(penalty=\u0026#34;l1\u0026#34;, C=c, solver=\u0026#39;liblinear\u0026#39;, max_iter=1000) # lr_l2 = LogisticRegression(penalty=\u0026#39;l2\u0026#39;, C=c, solver=\u0026#39;liblinear\u0026#39;, max_iter=1000) lr_l1 = LogisticRegression(penalty=\u0026#34;l1\u0026#34;, C=c, solver=\u0026#39;liblinear\u0026#39;, max_iter=1000, multi_class=\u0026#39;auto\u0026#39;) lr_l2 = LogisticRegression(penalty=\u0026#39;l2\u0026#39;, C=c, solver=\u0026#39;liblinear\u0026#39;, max_iter=1000, multi_class=\u0026#39;auto\u0026#39;) # 训练模型，记录L1正则化模型在训练集测试集上的表现 lr_l1.fit(X_train, Y_train) l1_train_predict.append(accuracy_score(lr_l1.predict(X_train), Y_train)) l1_test_predict.append(accuracy_score(lr_l1.predict(x_test), y_test)) # 记录L2正则化模型的表现 lr_l2.fit(X_train, Y_train) l2_train_predict.append(accuracy_score(lr_l2.predict(X_train), Y_train)) l2_test_predict.append(accuracy_score(lr_l2.predict(x_test), y_test)) if c == 2: pred_y_test = lr_l2.predict(x_test) mask = abs(pred_y_test-y_test) \u0026lt; 5 neg_test = pred_y_test[mask] res = (len(neg_test)/len(pred_y_test)) print(res) with open(model_l1_path, \u0026#39;wb\u0026#39;) as f1: pickle.dump(lr_l1, f1) with open(model_l2_path, \u0026#39;wb\u0026#39;) as f2: pickle.dump(lr_l2, f2) data = [l1_train_predict, l2_train_predict, l1_test_predict, l2_test_predict] label = [\u0026#39;l1_train\u0026#39;, \u0026#39;l2_train\u0026#39;, \u0026#39;l1_test\u0026#39;, \u0026#34;l2_test\u0026#34;] color = [\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;blue\u0026#39;] plt.figure(figsize=(12, 6)) for i in range(4) : plt.plot(np.linspace(0.01, 2, 50), data[i], label=label[i], color=color[i]) plt.legend(loc=\u0026#34;best\u0026#34;) plt.show()     支持向量机 Support Vector Machine  Using GridSearch to find the best parameters [code snippets] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71  import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Perceptron, LogisticRegression from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn import datasets from sklearn import metrics import pickle merged_data_dir = \u0026#39;../data/merged_data/merged_data.npy\u0026#39; model_dir=\u0026#39;./svm.pkl\u0026#39; data = np.load(merged_data_dir, allow_pickle=True) #labeling for ele in data: if ele[-1] \u0026lt; 20: ele[-1] = 0 elif ele[-1] \u0026gt;=20 and ele[-1] \u0026lt; 40: ele[-1] = 1 else: ele[-1] = 2 X = data[:,0:34] y = data[:,-1] print(y) # Create training and test split  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y) # feature scaling # sc = StandardScaler() # sc.fit(X_train) # X_train_std = sc.transform(X_train) # X_test_std = sc.transform(X_test) ################################## # # Instantiate the Support Vector Classifier (SVC) # svc = SVC(C=10, random_state=1, kernel=\u0026#39;rbf\u0026#39;, gamma=0.3) # # Fit the model # svc.fit(X_train, y_train) # # Make the predictions # y_predict = svc.predict(X_test) # # Measure the performance # print(\u0026#34;Accuracy score %.3f\u0026#34; %metrics.accuracy_score(y_test, y_predict)) ############################################# def svm_cross_validation(train_x, train_y): from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC model = SVC(kernel=\u0026#39;rbf\u0026#39;, probability=True) param_grid = {\u0026#39;C\u0026#39;: [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], \u0026#39;gamma\u0026#39;: [0.001, 0.0001]} grid_search = GridSearchCV(model, param_grid, n_jobs = 8, verbose=1, scoring=\u0026#39;accuracy\u0026#39;) grid_search.fit(train_x, train_y) best_parameters = grid_search.best_estimator_.get_params() for para, val in list(best_parameters.items()): print(para, val) model = SVC(kernel=\u0026#39;rbf\u0026#39;, C=best_parameters[\u0026#39;C\u0026#39;], gamma=best_parameters[\u0026#39;gamma\u0026#39;], probability=True) model.fit(train_x, train_y) return model svm_model = svm_cross_validation(X_train, y_train) with open(model_dir, \u0026#39;wb\u0026#39;) as f1: pickle.dump(svm_model, f1) f1.close() print(svm_model.score(X_test, y_test)) y_predict = svm_model.predict(X_test) print(y_predict)     ","permalink":"https://jianye0428.github.io/posts/notes/2022-05-28_ml/","summary":"用pickle保存和加载模型  保存模型 1 2 3 4 5 6 7  import pickle from sklearn.svm import SVC model_dir = \u0026#39;./model.pkl\u0026#39; model = SVC() with open(model_dir, \u0026#39;wb\u0026#39;) as f: pickle.dump(model, f) f.close() # 注意:保存完模型之后要关闭文件    加载模型 1 2 3 4 5  import pickle model_dir = \u0026#39;./model.pkl\u0026#39; with open(model_dir, \u0026#39;rb\u0026#39;) as f: model = pickel.load(f) print(mode.predict(x))     逻辑回归 Logistic Regression  LR Implementation code snippets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  from sklearn.","title":"Machine Learning Algo"},{"content":"numpy function   np.stack();np.vstack();np.hstack();np.concatenate() 区别\n  np.concatenate()函数根据指定的维度，对一个元组、列表中的list或者ndarray进行连接\n1 2 3  # np.concatenate() numpy.concatenate((a1, a2, ...), axis=0)#在0维进行拼接 numpy.concatenate((a1, a2, ...), axis=1)#在1维进行拼接     np.stack()函数的原型是numpy.stack(arrays, axis=0)，即将一堆数组的数据按照指定的维度进行堆叠。\n1 2 3  # np.stack() numpy.stack([a1, a2], axis=0)#在0维进行拼接 numpy.stack([a1, a2], axis=1)#在1维进行拼接    注意:进行stack的两个数组必须有相同的形状，同时，输出的结果的维度是比输入的数组都要多一维。\n   np.vstack()的函数原型：vstack(tup) ，参数tup可以是元组，列表，或者numpy数组，返回结果为numpy的数组。它是垂直（按照行顺序）的把数组给堆叠起来。\n  np.hstack()的函数原型：hstack(tup) ，参数tup可以是元组，列表，或者numpy数组，返回结果为numpy的数组。它其实就是水平(按列顺序)把数组给堆叠起来，与vstack()函数正好相反。\n   ref: https://cloud.tencent.com/developer/article/1378491\n    ","permalink":"https://jianye0428.github.io/posts/notes/2022-05-24_numpy/","summary":"numpy function   np.stack();np.vstack();np.hstack();np.concatenate() 区别\n  np.concatenate()函数根据指定的维度，对一个元组、列表中的list或者ndarray进行连接\n1 2 3  # np.concatenate() numpy.concatenate((a1, a2, ...), axis=0)#在0维进行拼接 numpy.concatenate((a1, a2, ...), axis=1)#在1维进行拼接     np.stack()函数的原型是numpy.stack(arrays, axis=0)，即将一堆数组的数据按照指定的维度进行堆叠。\n1 2 3  # np.stack() numpy.stack([a1, a2], axis=0)#在0维进行拼接 numpy.stack([a1, a2], axis=1)#在1维进行拼接    注意:进行stack的两个数组必须有相同的形状，同时，输出的结果的维度是比输入的数组都要多一维。\n   np.vstack()的函数原型：vstack(tup) ，参数tup可以是元组，列表，或者numpy数组，返回结果为numpy的数组。它是垂直（按照行顺序）的把数组给堆叠起来。\n  np.hstack()的函数原型：hstack(tup) ，参数tup可以是元组，列表，或者numpy数组，返回结果为numpy的数组。它其实就是水平(按列顺序)把数组给堆叠起来，与vstack()函数正好相反。\n   ref: https://cloud.tencent.com/developer/article/1378491\n    ","title":"Numpy Notes"},{"content":"python文件相关 os.path模块   os.path.exists(): 判断当前目录以及文件是否存在 os.path.mkdir(): 若目录或文件不存在，则创建\n1 2 3 4 5 6 7 8 9  import os # 目录 dirs = \u0026#39;/Users/joseph/work/python/\u0026#39; if not os.path.exists(dirs): os.makedirs(dirs) # 文件 filename = \u0026#39;/Users/joseph/work/python/poem.txt\u0026#39; if not os.path.exists(filename): os.system(r\u0026#34;touch {}\u0026#34;.format(path))#调用系统命令行来创建文件     os.listdir()： 用于返回指定的文件夹包含的文件或文件夹的名字的列表\n1 2 3 4 5 6 7 8  # 打开文件 path = \u0026#34;/var/www/html/\u0026#34; # 如果目录名字为中文 需要转码处理 path = unicode(path,\u0026#39;utf-8\u0026#39;) dirs = os.listdir(path) # 输出所有文件和文件夹 for file in dirs: print(file)     os.path.join(): 路径拼接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  import os path = \u0026#34;/home\u0026#34; # Join various path components  print(os.path.join(path, \u0026#34;User/Desktop\u0026#34;, \u0026#34;file.txt\u0026#34;)) # /home/User/Desktop/file.txt path = \u0026#34;User/Documents\u0026#34; # Join various path components  print(os.path.join(path, \u0026#34;/home\u0026#34;, \u0026#34;file.txt\u0026#34;)) # /home/file.txt # In above example \u0026#39;/home\u0026#39;  # represents an absolute path  # so all previous components i.e User / Documents  # are thrown away and joining continues  # from the absolute path component i.e / home.  print(os.path.join(path, \u0026#34;Downloads\u0026#34;, \u0026#34;file.txt\u0026#34;, \u0026#34;/home\u0026#34;)) # /home # In above example \u0026#39;/User\u0026#39; and \u0026#39;/home\u0026#39;  # both represents an absolute path  # but \u0026#39;/home\u0026#39; is the last value  # so all previous components before \u0026#39;/home\u0026#39;  # will be discarded and joining will  # continue from \u0026#39;/home\u0026#39;     os.path.abspath(path): 返回绝对路径\n  os.path.basename(path): 返回文件名\n  os.path.commonprefix(list): 返回list(多个路径)中，所有path共有的最长的路径\n  os.path.dirname(path): 返回文件路径\n  os.path.expanduser(path): 把path中包含的\u0026quot;~\u0026ldquo;和\u0026rdquo;~user\u0026quot;转换成用户目录\n  os.path.expandvars(path): 根据环境变量的值替换path中包含的 \u0026ldquo;$name\u0026rdquo; 和 \u0026ldquo;${name}\u0026rdquo;\n  os.path.getatime(path): 返回最近访问时间(浮点型秒数)\n  os.path.getmtime(path): 返回最近文件修改时间\n  os.path.getctime(path): 返回文件 path 创建时间\n  os.path.getsize(path): 返回文件大小，如果文件不存在就返回错误\n  os.path.isfile(path): 判断路径是否为文件\n  os.path.isdir(path): 判断路径是否为目录\n  os.path.islink(path): 判断路径是否为链接\n  os.path.ismount(path): 判断路径是否为挂载点\n  os.path.normcase(path): 转换path的大小写和斜杠\n  os.path.normpath(path): 规范path字符串形式\n  os.path.realpath(path): 返回path的真实路径\n  os.path.relpath(path[, start]): 从start开始计算相对路径\n  os.path.samefile(path1, path2): 判断目录或文件是否相同\n  os.path.sameopenfile(fp1, fp2): 判断fp1和fp2是否指向同一文件\n  os.path.samestat(stat1, stat2): 判断stat tuple stat1和stat2是否指向同一个文件\n  os.path.split(path): 把路径分割成 dirname 和 basename，返回一个元组\n  os.path.splitdrive(path): 一般用在 windows 下，返回驱动器名和路径组成的元组\n  os.path.splitext(path): 分割路径，返回路径名和文件扩展名的元组\n  os.path.splitunc(path): 把路径分割为加载点与文件\n  os.path.walk(path, visit, arg): 遍历path，进入每个目录都调用visit函数，visit函数必须有3个参数(arg, dirname, names)，dirname表示当前目录的目录名，names代表当前目录下的所有文件名，args则为walk的第三个参数\nos.walk(path,topdown=True,onerror=None): 函数返回一个元组，含有三个元素。这三个元素分别是：每次遍历的路径名、路径下子目录列表、目录下文件列表。\n1 2 3 4 5  path = \u0026#39;xxx/xxx\u0026#39; for root, dirs, files in os.walk(path): print(root) # path以及path下的目录 print(dirs) # path下的文件夹 print(files) # path下每个文件夹中的文件    区别：os.path.walk()与os.walk()产生的文件名列表并不相同.os.walk()产生目录树下的目录路径和文件路径，而os.path.walk()只产生文件路径（是子目录与文件的混合列表）。 ref: https://www.cnblogs.com/zmlctt/p/4222621.html\n   os.path.supports_unicode_filenames: 设置是否支持unicode路径名\n  ","permalink":"https://jianye0428.github.io/posts/notes/2022-05-23_python/","summary":"python文件相关 os.path模块   os.path.exists(): 判断当前目录以及文件是否存在 os.path.mkdir(): 若目录或文件不存在，则创建\n1 2 3 4 5 6 7 8 9  import os # 目录 dirs = \u0026#39;/Users/joseph/work/python/\u0026#39; if not os.path.exists(dirs): os.makedirs(dirs) # 文件 filename = \u0026#39;/Users/joseph/work/python/poem.txt\u0026#39; if not os.path.exists(filename): os.system(r\u0026#34;touch {}\u0026#34;.format(path))#调用系统命令行来创建文件     os.listdir()： 用于返回指定的文件夹包含的文件或文件夹的名字的列表\n1 2 3 4 5 6 7 8  # 打开文件 path = \u0026#34;/var/www/html/\u0026#34; # 如果目录名字为中文 需要转码处理 path = unicode(path,\u0026#39;utf-8\u0026#39;) dirs = os.listdir(path) # 输出所有文件和文件夹 for file in dirs: print(file)     os.","title":"Python Notes"},{"content":"pandas notes Input/Output   pd.read_csv(filepath): 读取csv文件 ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv\n  pd.read_pickle():读取pickle数据\n1 2  import pandas pandas.read_pickle(filepath_or_buffer, compression=\u0026#39;infer\u0026#39;, storage_options=None)   ref: https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html Parameters:\n filepath_or_buffer: 文件名或者文件路径 字符串、路径对象(实现 os.PathLike[str] )或 file-like 对象实现二进制 readlines() 函数。 compression: str or dict, default ‘infer’ 用于on-disk 数据的即时解压缩。如果 ‘infer’ 和 ‘filepath_or_buffer’ 是 path-like，则从以下扩展名检测压缩：“.gz”、“.bz2”、“.zip”、“.xz”或“.zst”(否则不压缩)。如果使用‘zip’，ZIP 文件必须只包含一个要读入的数据文件。设置为None 不解压缩。也可以是键 \u0026lsquo;method\u0026rsquo; 设置为 {'zip' , 'gzip' , 'bz2' , 'zstd' } 之一的字典，其他键值对分别转发到 zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File 或 zstandard.ZstdDecompressor 。例如，可以使用自定义压缩字典为 Zstandard 解压缩传递以下内容：compression={\u0026lsquo;method\u0026rsquo;: \u0026lsquo;zstd\u0026rsquo;, \u0026lsquo;dict_data\u0026rsquo;: my_compression_dict}。 storage_options: dict, optional 对特定存储连接有意义的额外选项，例如主机、端口、用户名、密码等。对于 HTTP(S) URL，键值对作为标头选项转发到 urllib。对于其他 URL(例如以 “s3://” 和 “gcs://” 开头)，键值对被转发到fsspec 。有关详细信息，请参阅fsspec和urllib。    General functions 通用函数 Series DataFrame DataFrame是一个【表格型】的数据结构，可以看做是【由Series组成的字典】（共用同一个索引）。DataFrame由按一定顺序排列的多列数据组成。设计初衷是将Series的使用场景从一维拓展到多维。\nConstructor  DataFrame[data, index, columns, dtype, copy]: 构造一个DataFrame对象  Attributes and underlying data  DataFrame.index: 行标签(行信息) DataFrame.columns: 列标签(列信息) DataFrame.dtypes: 返回DataFrame的数据类型 DataFrame.info([verbose, buf, max_cols, ...]): 返回df的信息 DataFrame.select_dtypes([include, exclude]): 返回DataFrame中根据columns筛选的部分数据 DataFrame.values: 以numpy数组的形式返回数据 DataFrame.axes: 返回一个list，其中是df的axes DataFrame.ndim: 返回int，代表axes/array的数量 DataFrame.shape: 返回tuple, 代表df维度 DataFrame.memory_usage([index, deep]): 返回数据内存使用情况 DataFrame.empty: 判断df是否为空 DataFrame.set_flags(*[, copy, ...]): 返回带有更新标记的df DataFrame.set_flags(*, copy=False, allows_duplicate_labels=None)  参数：allows_duplicate_labels：布尔型，可选。返回的对象是否允许重复标签。 返回：Series或DataFrame, 与调用者相同的类型。 注意：此方法返回一个新对象，该对象是与输入数据相同的视图。改变输入或输出值将反映在另一个中。此方法旨在用于方法链中。“Flags” 与 “metadata” 不同。标志反映了 pandas 对象(Series 或 DataFrame)的属性。元数据是 index 据集的属性，应存储在 DataFrame.attrs 中。 demo: 1 2 3 4 5 6  \u0026gt;\u0026gt;\u0026gt; df = pd.DataFrame({\u0026#34;A\u0026#34;:[1, 2]}) \u0026gt;\u0026gt;\u0026gt; df.flags.allows_duplicate_labels True \u0026gt;\u0026gt;\u0026gt; df2 = df.set_flags(allows_duplicate_labels=False) \u0026gt;\u0026gt;\u0026gt; df2.flags.allows_duplicate_labels False       Conversion  DataFrame.astype(dtype[,copy, errors]):数据类型转换 DataFrame.convert_dtypes([infer_objects, ...]):根据现存数据推断pd.NA数据类型 DataFrame.infer_objects():根据现有数据大部分数据推断类型 DataFrame.copy([deep]):深度拷贝  demo  1 2 3  s = pd.Series([1,2], index=[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;]) deep = s.copy()# 深拷贝 shallow = s.copy(deep=False) # 浅拷贝    DataFrame.bool():判断数据是ture还是false，只针对单个元素对象  Indexing，iteration  DataFrame.head([n]): return the first n rows DataFrame.at[4,'B']: 用标签取值(行名为4，列名为B的值) DataFrame.iat[1,2]: 用行列的整数取值(第1行,第二列的值) DataFrame.loc['cobra':'viper', 'max_speed']: 取行名为\u0026rsquo;cobra\u0026rsquo;至\u0026rsquo;viper\u0026rsquo;, 列名为\u0026rsquo;max_speed\u0026rsquo;的值 DataFrame.iloc: 通过行列的值取值  df.iloc[0]:取第0行，所有列的值，返回series类型 df.iloc[[0]]:取得第0行，所有列的值，返回df类型 df.iloc[[0,1]]:取得第0行和第1行的所有列的值 df.iloc[:3]:取得第0，1，2行的值 df.iloc[[True, False, True]]: 用True/False标记要取的行 df.iloc[lambda x:x.index % 2 == 0]: 用lambda标记要取的行 df.iloc[0,1]:取得第0行，第1列的值 df.iloc[[0,2],[1,3]]: 取得第0行，第2行，第1列，第3列的值 df.iloc[1:3, 0:3]: 取得第1行，第2行，第0列，第1列，第2列的值 df.iloc[:, [True,False,True,False]]:取所有的行，用True/False取相应的列 df.iloc[:,lambda df:[0,2]]: 取所有的行，取第0列，第2列   df.insert(loc, column, value, allow_duplicates=False):插入相应的列  loc:(int), 列的位置 column: 列的名字，一般类型为string value: 列数据的值   df.drop():删除固定的行或者列 df.drop_duplicates(subset, keep, inplace=False,ignore_index=False):删除重复的行或者列       subset: 根据某一列的值，删除行数据 keep: 设置保留第一次出现的数据或者最后一次出现的数据      ","permalink":"https://jianye0428.github.io/posts/notes/2022-05-23_pandas/","summary":"pandas notes Input/Output   pd.read_csv(filepath): 读取csv文件 ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv\n  pd.read_pickle():读取pickle数据\n1 2  import pandas pandas.read_pickle(filepath_or_buffer, compression=\u0026#39;infer\u0026#39;, storage_options=None)   ref: https://pandas.pydata.org/docs/reference/api/pandas.read_pickle.html Parameters:\n filepath_or_buffer: 文件名或者文件路径 字符串、路径对象(实现 os.PathLike[str] )或 file-like 对象实现二进制 readlines() 函数。 compression: str or dict, default ‘infer’ 用于on-disk 数据的即时解压缩。如果 ‘infer’ 和 ‘filepath_or_buffer’ 是 path-like，则从以下扩展名检测压缩：“.gz”、“.bz2”、“.zip”、“.xz”或“.zst”(否则不压缩)。如果使用‘zip’，ZIP 文件必须只包含一个要读入的数据文件。设置为None 不解压缩。也可以是键 \u0026lsquo;method\u0026rsquo; 设置为 {'zip' , 'gzip' , 'bz2' , 'zstd' } 之一的字典，其他键值对分别转发到 zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File 或 zstandard.ZstdDecompressor 。例如，可以使用自定义压缩字典为 Zstandard 解压缩传递以下内容：compression={\u0026lsquo;method\u0026rsquo;: \u0026lsquo;zstd\u0026rsquo;, \u0026lsquo;dict_data\u0026rsquo;: my_compression_dict}。 storage_options: dict, optional 对特定存储连接有意义的额外选项，例如主机、端口、用户名、密码等。对于 HTTP(S) URL，键值对作为标头选项转发到 urllib。对于其他 URL(例如以 “s3://” 和 “gcs://” 开头)，键值对被转发到fsspec 。有关详细信息，请参阅fsspec和urllib。    General functions 通用函数 Series DataFrame DataFrame是一个【表格型】的数据结构，可以看做是【由Series组成的字典】（共用同一个索引）。DataFrame由按一定顺序排列的多列数据组成。设计初衷是将Series的使用场景从一维拓展到多维。","title":"Pandas Notes"},{"content":"Linux系统各系统文件夹下的区别 首先，usr 指 Unix System Resource，而不是User。\n通常，\n  /usr/bin下面的都是系统预装的可执行程序，会随着系统升级而改变。\n  /usr/local/bin目录是给用户放置自己的可执行程序的地方，推荐放在这里，不会被系统升级而覆盖同名文件。\n  如果两个目录下有相同的可执行程序，谁优先执行受到PATH环境变量的影响，比如我的一台服务器的PATH变量为。\n1  echo $PATH   这里/usr/local/bin优先于/usr/bin, 一般都是如此。\n/lib是内核级的, /usr/lib是系统级的, /usr/local/lib是用户级的.\n  / - 对你的电脑来说, 有且只有一个根目录。所有的东西都是从这里开始。举个例子: 当你在终端里输入\u0026quot;/home\u0026quot;，你其实是在告诉电脑，先从/(根目录)开始，再进入到home目录。\n  /lib/ — 包含许多被/bin/和/sbin/中的程序使用的库文件。目录/usr/lib/中含有更多用于用户程序的库文件。/lib目录下放置的是/bin和/sbin目录下程序所需的库文件。/lib目录下的文件的名称遵循下面的格式：\n  libc.so.* ld* 仅仅被/usr目录下的程序所使用的共享库不必放到/lib目录下。只有/bin和/sbin下的程序所需要的库有必要放到/lib目录下。实际上，libm.so.*类型的库文件如果被是/bin和/sbin所需要的，也可以放到/usr/lib下。     /bin/ — 用来贮存用户命令。目录 /usr/bin 也被用来贮存用户命令。\n  /sbin/ — 许多系统命令(例如 shutdown)的贮存位置。目录/usr/sbin中也包括了许多系统命令。\n  /root/ — 根用户(超级用户)的主目录。\n  /mnt/ — 该目录中通常包括系统引导后被挂载的文件系统的挂载点。譬如，默认的光盘挂载点是/mnt/cdrom/.\n  /boot/ — 包括内核和其它系统启动期间使用的文件。\n  /lost+found/ — 被fsck用来放置零散文件(没有名称的文件)。\n  /lib/ — 包含许多被/bin/和/sbin/中的程序使用的库文件。目录/usr/lib/中含有更多用于用户程序的库文件。\n  /dev/ — 贮存设备文件。\n  /etc/ — 包含许多配置文件和目录。\n  /var/ — 用于贮存variable(或不断改变的)文件，例如日志文件和打印机假脱机文件。\n  /usr/ — 包括与系统用户直接有关的文件和目录，例如应用程序及支持它们的库文件。在这个目录下，你可以找到那些不适合放在/bin或/etc目录下的额外的工具。比如像游戏阿，一些打印工具拉等等。/usr目录包含了许多子目录： /usr/bin目录用于存放程序; /usr/share用于存放一些共享的数据，比如音乐文件或者图标等等;/usr/lib目录用于存放那些不能直接运行的，但却是许多程序运行所必需的一些函数库文件。\n  /proc/ — 一个虚拟的文件系统(不是实际贮存在磁盘上的)，它包括被某些程序使用的系统信息。\n  /initrd/ — 用来在计算机启动时挂载 initrd.img 映像文件的目录以及载入所需设备模块的目录。\n 警告:  不要删除/initrd/目录。如果你删除了该目录后再重新引导Red Hat Linux时，你将无法引导你的计算机。\n     /tmp/ — 用户和程序的临时目录。/tmp给予所有系统用户读写权。\n  /home/ — 用户主目录的默认位置。\n  /opt/ — 可选文件和程序的贮存目录。该目录主要被第三方开发者用来简易地安装和卸装他们的软件包。这里主要存放那些可选的程序。你想尝试最新的firefox测试版吗?那就装到/opt目录下吧，这样，当你尝试完，想删掉firefox的时候，你就可 以直接删除它，而不影响系统其他任何设置。安装到/opt目录下的程序，它所有的数据、库文件等等都是放在同个目录下面。\n  /usr/local/ - 这里主要存放那些手动安装的软件，即apt或者apt-get安装的软件。它和/usr目录具有相类似的目录结构。让软件包管理器来管理/usr目录，而把自定义的脚本(scripts)放到/usr/local目录下面，我想这应该是个不错的主意。\n  /media/ - 有些linux的发行版使用这个目录来挂载那些usb接口的移动硬盘(包括U盘)、CD/DVD驱动器等等。\n  /usr/local/ 和 /usr/share/ 区别   /usr/local - 这个目录一般是用来存放用户自编译安装软件的存放目录; 一般是通过源码包安装的软件，如果没有特别指定安装目录的话，一般是安装在这个目录中。这个目录下面有子目录。\n  /usr/share - 系统共用的东西存放地，比如/usr/share/fonts是字体目录，/usr/share/doc和/usr/share/man帮助文件。\n  /var/log - 系统日志存放，分析日志要看这个目录的东西;\n  /var/spool - 打印机、邮件、代理服务器等脱机目录。\n  Linux Command Notes 查找文件的命令:find/locate/whereis/which/type/grep find find命令准确，但速度非常慢，它可以查找任何类型的文件\n  使用格式\n1  find [指定目录] [指定条件] [指定动作]   参数说明:\n [指定目录]： 所要搜索的目录及其所有子目录。默认为当前目录 [指定条件]： 所要搜索的文件的特征  -name：按文件名来查找文件 -user：按照文件的属主来查找文件 -group：按照文件所属的组来查找文件 -perm：按照文件权限来查找文件 -prune：不在当前指定目录中查找   [指定动作]： 对搜索结果进行特定的处理  -print：将匹配的文件输出到标准输出 -exec：对匹配的文件执行该参数所给出的shell命令 -ok：和-exec的作用相同，在执行每一个命令之前，让用户来确定是否执行     find命令不加任何参数时，表示搜索路径为当前目录及其子目录，默认的动作为-print，即不过滤任何结果，也就是说输出所有的文件\n 使用示例: - 递归搜索当前目录中，所有以file开头的文件 shell find . -name 'file*'  - 递归搜索当前目录中，所有以file开头的文件，并显示它们的详细信息 shell find . -name 'file*' -ls \n  locate locate命令可以说是find -name的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库/var/lib/locatedb，这个数据库中含有本地所有文件信息.\nLinux自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库.\n  使用格式:\n1  locate [参数] \u0026lt;文件名\u0026gt;     使用示例:\n 搜索etc目录下所有以file开头的文件 1  locate /etc/file    搜索用户主目录下，所有以f开头的文件，并且忽略大小写 1  locate -i ~/f       whereis whereis命令只能搜索特定格式的文件\n 使用格式 1  whereis [参数] \u0026lt;文件名\u0026gt;    可搜索文集类型  二进制文件(-b) 源代码文件(-s) 说明文件(-m)     如果省略参数，则返回所有信息\n  使用示例:  找出名为find的文件位置 1  whereis find       which which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果, 也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n 使用格式 1  which \u0026lt;命令\u0026gt;    使用实例:  查找find命令的位置 1  which find       type type命令其实不能算查找命令，它是用来区分某个命令到底是由shell自带的，还是由shell外部的独立二进制文件提供的; 如果一个命令是外部命令，那么使用-p参数，会显示该命令的路径，相当于which命令。\n 使用格式 1  type \u0026lt;命令\u0026gt;    使用实例:  查看cd命令是否为shell自带的命令 1  type cd    查看grep是否为外部命令 1  type grep       grep grep命令用于查找拥有特殊字段的文件。\n  语法\n1  grep [-abcEFGhHilLnqrsvVwxy][-A\u0026lt;显示行数\u0026gt;][-B\u0026lt;显示列数\u0026gt;][-C\u0026lt;显示列数\u0026gt;][-d\u0026lt;进行动作\u0026gt;][-e\u0026lt;范本样式\u0026gt;][-f\u0026lt;范本文件\u0026gt;][--help][范本样式][文件或目录...]   参数:\n -a 或 \u0026ndash;text : 不要忽略二进制的数据。 -A\u0026lt;显示行数\u0026gt; 或 \u0026ndash;after-context=\u0026lt;显示行数\u0026gt; : 除了显示符合范本样式的那一列之外，并显示该行之后的内容。 -b 或 \u0026ndash;byte-offset : 在显示符合样式的那一行之前，标示出该行第一个字符的编号。 -B\u0026lt;显示行数\u0026gt; 或 \u0026ndash;before-context=\u0026lt;显示行数\u0026gt; : 除了显示符合样式的那一行之外，并显示该行之前的内容。 -c 或 \u0026ndash;count : 计算符合样式的列数。 -C\u0026lt;显示行数\u0026gt; 或 \u0026ndash;context=\u0026lt;显示行数\u0026gt;或-\u0026lt;显示行数\u0026gt; : 除了显示符合样式的那一行之外，并显示该行之前后的内容。 -d \u0026lt;动作\u0026gt; 或 \u0026ndash;directories=\u0026lt;动作\u0026gt; : 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。 -e\u0026lt;范本样式\u0026gt; 或 \u0026ndash;regexp=\u0026lt;范本样式\u0026gt; : 指定字符串做为查找文件内容的样式。 -E 或 \u0026ndash;extended-regexp : 将样式为延伸的正则表达式来使用。 -f\u0026lt;规则文件\u0026gt; 或 \u0026ndash;file=\u0026lt;规则文件\u0026gt; : 指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。 -F 或 \u0026ndash;fixed-regexp : 将样式视为固定字符串的列表。 -G 或 \u0026ndash;basic-regexp : 将样式视为普通的表示法来使用。 -h 或 \u0026ndash;no-filename : 在显示符合样式的那一行之前，不标示该行所属的文件名称。 -H 或 \u0026ndash;with-filename : 在显示符合样式的那一行之前，表示该行所属的文件名称。 -i 或 \u0026ndash;ignore-case : 忽略字符大小写的差别。 -l 或 \u0026ndash;file-with-matches : 列出文件内容符合指定的样式的文件名称。 -L 或 \u0026ndash;files-without-match : 列出文件内容不符合指定的样式的文件名称。 -n 或 \u0026ndash;line-number : 在显示符合样式的那一行之前，标示出该行的列数编号。 -o 或 \u0026ndash;only-matching : 只显示匹配PATTERN 部分。 -q 或 \u0026ndash;quiet或\u0026ndash;silent : 不显示任何信息。 -r 或 \u0026ndash;recursive : 此参数的效果和指定\u0026quot;-d recurse\u0026quot;参数相同。 -s 或 \u0026ndash;no-messages : 不显示错误信息。 -v 或 \u0026ndash;invert-match : 显示不包含匹配文本的所有行。 -V 或 \u0026ndash;version : 显示版本信息。 -w 或 \u0026ndash;word-regexp : 只显示全字符合的列。 -x \u0026ndash;line-regexp : 只显示全列符合的列。 -y : 此参数的效果和指定\u0026quot;-i\u0026quot;参数相同。    示例:\n1 2  # 查找指定目录/etc/acpi 及其子目录（如果存在子目录的话）下所有文件中包含字符串\u0026#34;update\u0026#34;的文件，并打印出该字符串所在行的内容 grep -r update /etc/acpi   1 2  # 查看符合条件的日志条目。 grep -n \u0026#39;2019-10-24 00:01:11\u0026#39; *.log   1 2  # 只匹配文本文件，不匹配二进制文件的命令 grep -srn \u0026#34;parameter\u0026#34; . --binary-files=without-match     ","permalink":"https://jianye0428.github.io/posts/tech/2022-05-13_linux_filesystem/","summary":"Linux系统各系统文件夹下的区别 首先，usr 指 Unix System Resource，而不是User。\n通常，\n  /usr/bin下面的都是系统预装的可执行程序，会随着系统升级而改变。\n  /usr/local/bin目录是给用户放置自己的可执行程序的地方，推荐放在这里，不会被系统升级而覆盖同名文件。\n  如果两个目录下有相同的可执行程序，谁优先执行受到PATH环境变量的影响，比如我的一台服务器的PATH变量为。\n1  echo $PATH   这里/usr/local/bin优先于/usr/bin, 一般都是如此。\n/lib是内核级的, /usr/lib是系统级的, /usr/local/lib是用户级的.\n  / - 对你的电脑来说, 有且只有一个根目录。所有的东西都是从这里开始。举个例子: 当你在终端里输入\u0026quot;/home\u0026quot;，你其实是在告诉电脑，先从/(根目录)开始，再进入到home目录。\n  /lib/ — 包含许多被/bin/和/sbin/中的程序使用的库文件。目录/usr/lib/中含有更多用于用户程序的库文件。/lib目录下放置的是/bin和/sbin目录下程序所需的库文件。/lib目录下的文件的名称遵循下面的格式：\n  libc.so.* ld* 仅仅被/usr目录下的程序所使用的共享库不必放到/lib目录下。只有/bin和/sbin下的程序所需要的库有必要放到/lib目录下。实际上，libm.so.*类型的库文件如果被是/bin和/sbin所需要的，也可以放到/usr/lib下。     /bin/ — 用来贮存用户命令。目录 /usr/bin 也被用来贮存用户命令。\n  /sbin/ — 许多系统命令(例如 shutdown)的贮存位置。目录/usr/sbin中也包括了许多系统命令。\n  /root/ — 根用户(超级用户)的主目录。\n  /mnt/ — 该目录中通常包括系统引导后被挂载的文件系统的挂载点。譬如，默认的光盘挂载点是/mnt/cdrom/.\n  /boot/ — 包括内核和其它系统启动期间使用的文件。","title":"Linux filesystem"},{"content":"VIM 8.2 安装 1. Install Python3.9 from source   Update the packages list and install the packages necessary to build Python\n1  sudo apt update \u0026amp;\u0026amp; sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev     Download the latest release’s source code from the Python download page using wget\n1  wegt https://www.python.org/ftp/python/3.9.0/Python-3.9.1.tgz     Switch to the Python source directory and execute the configure script which performs a number of checks to make sure all of the dependencies on your system are present\n1 2 3  cd Python-3.9.1 ./configure --enable-optimizations --with-lto --enable-shared --prefix=/usr/local/python39 make -j8     When the build process is complete, install the Python binaries by typing\n1  sudo make altinstall    Do not use the standard make install as it will overwrite the default system python3 binary.\n   copy the dynamic library to usr/lib/x86_64-linux-gnu/libpython3.9.so.1.0\n1  sudo cp /usr/local/python39/lib/libpython3.9.so.1.0 /usr/lib/x86_64-linux-gnu/    the command can slove the error: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory\n   make the soft link to set python39 as default python3\n1 2  sudo ln -sf /usr/local/python39/bin/python3.9 /usr/bin/python3 sudo ln -s /usr/local/python39/bin/python3.9 /usr/bin/python3.9     using update-alternatives to switch different python version\n  list all the python versions\n1  sudo update-alternatives --list python3     display python3\n1  sudo update-alternatives --display python3     set different number for different version\n1 2  sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2     show different mode and select number to switch another mode\n1  sudo update-alternatives --config python3       2. 源码安装cmake 2.1 download the cmake source code  download source code 1  wget https://github.com/Kitware/CMake/releases/download/v3.23.1/cmake-3.23.1.tar.gz     2.2 extract the source code directory and run the command to install  extraction and configuration 1 2 3  cd cmake-2.23.0 ./bootstrap //需要的话也可以指定安装目录，例如--prefix=/usr/local/cmake make \u0026amp;\u0026amp; sudo make install     2.3 create soft link and set cmake as default  set cmake as default 1  sudo ln -s /usr/local/bin/cmake /usr/bin/cmake     3. 首先从github下载源码vim 8.2 3.1 源码安装vim8.2   run the following command to downlaod source code of VIM from github\n1 2 3 4 5  git clone git clone https://github.com/vim/vim.git cd vim git pull cd src/ sudo make distclean # 如果您以前构建国vim     cofigure the installation file\n1 2 3  ./configure --with-features=huge --enable-multibyte --enable-python3interp=dynamic --with-python3-config-dir=/usr/lib/python3.10/config-3.10-x86_64-linux-gnu/ --enable-cscope --enable-gui=auto --enable-gtk2-check --enable-fontset --enable-largefile --disable-netbeans --with-compiledby=\u0026#34;18817571704@163.com\u0026#34; --enable-fail-if-missing --prefix=/usr/local/vim82 sudo make sudo make install     enable clipboard\n then you can copy the content from system clipboard to vim 1  sudo apt-get install vim-gtk3       卸载vim\n  使用以下命令重置编译操作\n1  sudo make distclean     使用以下命令，可以卸载命令\n1  sudo make uninstall       3.2 安装vim-plug以及插件   安装vim-plug:\n1 2  curl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim     安装主题gruvbox\nto fix error: Cannot find color scheme \u0026lsquo;gruvbox\u0026rsquo;\n1 2  mkdir ~/.vim/colors/ cp ~/.vim/plugged/gruvbox/gruvbox.vim ~/.vim/colors/     安装YCM(YouCompleteMe) 根据~/.vimrc按装YCM\n1 2  cd ~/.vim/plugged/YouCompleteMe/ ./install.py --clang-completer     安装ctags\n1  sudo apt-get install exuberant-ctags     其他主题直接编辑:PlugInstall进行安装\n  3.2 reference  参考链接:\n[1] https://github.com/ycm-core/YouCompleteMe/wiki/Building-Vim-from-source [2] https://wizardforcel.gitbooks.io/use-vim-as-ide/content/0.html  ","permalink":"https://jianye0428.github.io/posts/tech/2022-05-11_vim_installation/","summary":"VIM 8.2 安装 1. Install Python3.9 from source   Update the packages list and install the packages necessary to build Python\n1  sudo apt update \u0026amp;\u0026amp; sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev     Download the latest release’s source code from the Python download page using wget\n1  wegt https://www.python.org/ftp/python/3.9.0/Python-3.9.1.tgz     Switch to the Python source directory and execute the configure script which performs a number of checks to make sure all of the dependencies on your system are present","title":"Build VIM 8.2 from source"},{"content":"git command record as my cheatsheet 1. git rebase ref: https://git-scm.com/docs/git-rebase\n  用法一:git rebase \u0026lt;branch-name\u0026gt; 将topic分支的base由E改为master\n1 2 3  A---B---C topic / D---E---F---G master   运行:\n1 2  git rebase master git rebase master topic   结果:\n1 2 3  A\u0026#39;--B\u0026#39;--C\u0026#39; topic / D---E---F---G master    if upstream branch already has a change like below:\n 1 2 3  A---B---C topic / D---E---A\u0026#39;---F master   then run the command git rebase master, you will get following result:\n1 2 3  B\u0026#39;---C\u0026#39; topic / D---E---A\u0026#39;---F master     用法二:git rebase --onto assume topic is based on next, and next is based on master\n1 2 3 4 5  o---o---o---o---o master \\  o---o---o---o---o next \\  o---o---o topic   run the command below:\n1  git rebase --onto master next topic   then we get the result below:\n1 2 3 4 5  o---o---o---o---o master | \\  | o\u0026#39;--o\u0026#39;--o\u0026#39; topic \\  o---o---o---o---o next   Another example: A range of commits could also be removed with rebase. If we have the following situation:\n1  E---F---G---H---I---J topicA   then the command\n1  git rebase --onto topicA~5 topicA~3 topicA   would result in the removal of commits F and G:\n1  E---H\u0026#39;---I\u0026#39;---J\u0026#39; topicA     用法三:git rebase -i \u0026lt;commit_id\u0026gt; \u0026lt;commit_id\u0026gt; $\\mathbb{\\rightarrow}$ 将多个commit合并为一个。\n1 2 3 4 5  # 执行git log，得到以下commit_ids \u0026gt;\u0026gt;\u0026gt;21fd585 \u0026gt;\u0026gt;\u0026gt;45j3483 \u0026gt;\u0026gt;\u0026gt;9i8975d \u0026gt;\u0026gt;\u0026gt;73c20ec   目标: 将21fd585、45j3483、9i8975d rebase 到 73c20ec\n1  git rebase -i 73c20ec 21fd585   得到:\n1 2 3 4  pick pick pick pick   改为\n1 2 3 4  pick squash squash squash   最后，编辑commit内容， 得到\n1 2  \u0026gt;\u0026gt;\u0026gt;b8bec33 # 此处为新的commit \u0026gt;\u0026gt;\u0026gt;73c20ec   推送到remote:\n1  git push -f origin master   ref:\n https://www.bilibili.com/video/BV15h411f74h/ https://blog.csdn.net/weixin_45953517/article/details/114362752 https://blog.csdn.net/weixin_44691608/article/details/118740059#t7   遇到detached HEAD的解决办法\n 1 2 3 4 5  git branch b1 git checkout master git merge b1 git push origin master git branch -d b1     2. git cherrypick  将指定的提交用于其他分支 例如: 1 2 3  a - b - c - d Master \\  e - f - g Feature   run the command below and apply commit(f) to master 1 2  git checkout master git cherry-pick f   get the result 1 2 3  a - b - c - d - f Master \\  e - f - g Feature    转移多个提交 1 2  # 将 A 和 B 两个提交应用到当前分支 git cherry-pick \u0026lt;HashA\u0026gt; \u0026lt;HashB\u0026gt;   或者 1 2  # 该命令可以转移从 A 到 B 的所有提交,它们必须按照正确的顺序放置：提交 A 必须早于提交 B，否则命令将失败，但不会报错。 git cherry-pick A..B   1 2  # 使用上面的命令，提交 A 将不会包含在 Cherry pick 中， 如果要包含提交 A，可以使用下面的语法。 git cherry-pick A^..B   ref:https://www.ruanyifeng.com/blog/2020/04/git-cherry-pick.html  3. git submodule  将一个repo添加为submodule 1  git submodule add https://github.com/chaconinc/DbConnector    克隆含有子模块的项目 1 2 3 4 5 6  git clone https://github.com/chaconinc/MainProject #此时包含子模块目录，但是其中没有任何文件 cd MainProject cd DbConnector/ # 此时有DbConnector目录，但是文件夹是空的 git submodule init # 用来初始化本地配置文件 git submodule update # 从该项目中抓取并检出父项目中列出的合适的提交   或者 1  git clone --recurse-submodules https://github.com/chaconinc/MainProject   或者已经克隆了项目，但是忘记--recurse-submodule, 则使用 1  git submodule update --init --recursive     4. 拉取远程分支到本地   拉取某一个远程的分支，并在创建相应的本地分支名称\n1 2  git fetch origin remote-branch-name git checkout -b local-branch-name origin/remote-branch-name     5. git tag  用git tag打标签 1 2  git tag -a v1.0 git tag -a v0 85fc7e7 #追加标签    git clone 按照tag拉取代码 1 2  # git clone --branch [tags标签] [git地址] git clone -b v5.2.0 --depth=1 http://gitlab地址     6. git stash  git stash:隐藏修改 1 2 3 4 5 6 7 8 9 10  git stash # 隐藏修改 git stash save \u0026#34;stash-name\u0026#34; #给每一个stash取名字 git stash pop # 恢复隐藏的修改 git stash list # 列出所有的隐藏 git stash apply [number] # 指定恢复使用哪一个隐藏修改 git stash drop # 移除某一项修改 git stash clear # 删除所有隐藏的修改 git stash show # 查看隐藏的修改 git stash show -p git stash show --patch # 查看特定的stash的diff     7. 代码回退: git reset/git revert   ref:https://blog.csdn.net/weixin_35082950/article/details/113629326\n  本地分支版本回退的方法\n1 2  git reflog # 找回要回退的版本的commit_id git reset --hard \u0026lt;commit_id\u0026gt;     自己的远程分支版本回退的方法\n1 2 3 4 5 6  # 如果你的错误提交已经推送到自己的远程分支了，那么就需要回滚远程分支了。  # 1. 首先要回退本地分支： git reflog git reset --hard \u0026lt;commit_id\u0026gt; # 2. 强制推送到远程分支 git push -f     公共远程分支版本回退的问题\n 一个显而易见的问题：如果你回退公共远程分支，把别人的提交给丢掉了怎么办？\n 假设你的远程master分支情况是这样的:\n1  A1–A2–B1 #   其中A、B分别代表两个人，A1、A2、B1代表各自的提交。并且所有人的本地分支都已经更新到最新版本，和远程分支一致。\n这个时候你发现A2这次提交有错误，你用reset回滚远程分支master到A1，那么理想状态是你的队友一拉代码git pull，他们的master分支也回滚了，然而现实却是，你的队友会看到下面的提示：\n1 2 3 4 5  $ git status On branch master Your branch is ahead of \u0026#39;origin/master\u0026#39; by 2 commits. (use \u0026#34;git push\u0026#34; to publish your local commits) nothing to commit, working directory clean    也就是说，你的队友的分支并没有主动回退，而是比远程分支超前了两次提交，因为远程分支回退了嘛。\n 1 2 3  git revert HEAD #撤销最近一次提交 git revert HEAD~1 #撤销上上次的提交，注意：数字从0开始 git revert 0ffaacc #撤销0ffaacc这次提交    git revert 命令意思是撤销某次提交。它会产生一个新的提交，虽然代码回退了，但是版本依然是向前的，所以，当你用revert回退之后，所有人pull之后，他们的代码也自动的回退了。 但是，要注意以下几点：\n 1、revert 是撤销一次提交，所以后面的commit id是你需要回滚到的版本的前一次提交。\n2、使用revert HEAD是撤销最近的一次提交，如果你最近一次提交是用revert命令产生的，那么你再执行一次，就相当于撤销了上次的撤销操作，换句话说，你连续执行两次revert HEAD命令，就跟没执行是一样的。\n3、使用revert HEAD~1 表示撤销最近2次提交，这个数字是从0开始的，如果你之前撤销过产生了commi id，那么也会计算在内的。\n4、如果使用 revert 撤销的不是最近一次提交，那么一定会有代码冲突，需要你合并代码，合并代码只需要把当前的代码全部去掉，保留之前版本的代码就可以了。\n     git revert 命令的好处就是不会丢掉别人的提交，即使你撤销后覆盖了别人的提交，他更新代码后，可以在本地用 reset 向前回滚，找到自己的代码，然后拉一下分支，再回来合并上去就可以找回被你覆盖的提交了。\n revert 合并代码，解决冲突 使用revert命令，如果不是撤销的最近一次提交，那么一定会有冲突，如下所示：\n1 2 3 4 5 6  \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 全部清空 第一次提交 ======= 全部清空 \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; parent of c24cde7... 全部清空   解决冲突很简单，因为我们只想回到某次提交，因此需要把当前最新的代码去掉即可，也就是HEAD标记的代码:\n1 2 3 4  \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 全部清空 第一次提交 =======   把上面部分代码去掉就可以了，然后再提交一次代码就可以解决冲突了。\n8. git branch  将本地分支与远程分支关联: 1  git branch --set-upstream=origin/remote_branch your_branch     ","permalink":"https://jianye0428.github.io/posts/tech/2022-05-11_git_command/","summary":"git command record as my cheatsheet 1. git rebase ref: https://git-scm.com/docs/git-rebase\n  用法一:git rebase \u0026lt;branch-name\u0026gt; 将topic分支的base由E改为master\n1 2 3  A---B---C topic / D---E---F---G master   运行:\n1 2  git rebase master git rebase master topic   结果:\n1 2 3  A\u0026#39;--B\u0026#39;--C\u0026#39; topic / D---E---F---G master    if upstream branch already has a change like below:\n 1 2 3  A---B---C topic / D---E---A\u0026#39;---F master   then run the command git rebase master, you will get following result:","title":"Git Command Notes"},{"content":"zsh说明   zsh是一个Linux下强大的shell, 由于大多数Linux产品安装以及默认使用bash shell, 但是丝毫不影响极客们对zsh的热衷, 几乎每一款Linux产品都包含有zsh，通常可以用apt-get、urpmi或yum等包管理器进行安装.\n  zsh是bash的增强版，其实zsh和bash是两个不同的概念，zsh更加强大。\n  通常zsh配置起来非常麻烦，且相当的复杂，所以oh-my-zsh是为了简化zsh的配置而开发的，因此oh-my-zsh算是zsh的配置.\n  准备   查看当前系统用shell版本\n1  echo $SHELL     查看系统自带哪些shell\n1  cat /etc/shells     安装zsh  通过命令行安装zsh 1  sudo apt install zsh     zsh配置   将zsh设置为默认的shell\n1  chsh -s /bin/zsh     然后重启电脑\n1  reboot     安装oh-my-zsh及其个性化配置 安装oh-my-zsh  执行以下命令安装oh-my-zsh 1  sh -c \u0026#34;$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\u0026#34;   或者 1  sh -c \u0026#34;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026#34;     主题配置   打开配置文件~/.zshrc 输入:\n1  ZSH_THEME=\u0026#34;xxf\u0026#34;   xxf.zsh-theme文件下载地址: xxf.zsh-theme文件下载\n下载之后将文件拷贝到以下路径: /home/username/.oh-my-zsh/themes/\n  插件 安装自动补全插件incr  首先，下载incr插件到本地 1 2 3  cd ~/.oh-my-zsh/plugins/ mkdir incr \u0026amp;\u0026amp; cd incr wget http://mimosa-pudica.net/src/incr-0.2.zsh    编辑~/.zshrc文件，添加以下内容: 1  source ~/.oh-my-zsh/plugins/incr/incr*.zsh    然后，source一下: 1  source ~/.zshrc     直接使用默认插件   在~/.zshrc文件中添加插件:\n1  plugins=(git extract z)     安装autojump插件  通过命令行安装autojump 1  sudo apt install autojump    在~/.zshrc文件中编辑: 1  . /usr/share/autojump/autojump.sh    然后，source一下: 1  source ~/.zshrc     安装zsh-syntax-highlighting语法高亮插件   从gihub下载源码并放在~/.oh-my-zsh/plugins/文件夹下:\n1 2  cd ~/.oh-my-zsh/plugins/ git clone https://github.com/zsh-users/zsh-syntax-highlighting.git     在~/.zshrc文件中编辑:\n1  source ~/.oh-my-zsh/plugins/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh     然后，source一下:\n1  source ~/.zshrc     安装zsh-autosuggestions语法历史记录插件   从gihub下载源码并放在~/.oh-my-zsh/plugins/文件夹下:\n1 2  cd ~/.oh-my-zsh/plugins/ git clone git@github.com:zsh-users/zsh-autosuggestions.git     在~/.zshrc文件中编辑:\n1  source ~/.oh-my-zsh/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh     然后，source一下:\n1  source ~/.zshrc     其他  设置更新日期 在~/.zshrc文件中编： 1  exprot UPDATE_ZSH_DAYS=13    禁止自动更新 1  DISABLE_AUTO_UPDATE=\u0026#34;true\u0026#34;    手动更新oh-my-zsh 1  upgrade_oh_my_zsh    卸载oh-my-zsh 1  uninstall_on_my_zsh zsh     从bash到zsh的切换  直接执行zsh和oh-my-zsh的安装以及配置，并且在~/.zshrc文件中添加: 1  source ~/.bashrc     zsh 快捷键  快捷键 ⌃ + u: 清空当前行 ⌃ + a: 移动到行首 ⌃ + e: 移动到行尾 ⌃ + f: 向前移动 ⌃ + b: 向后移动 ⌃ + p: 上一条命令 ⌃ + n: 下一条命令 ⌃ + r: 搜索历史命令 ⌃ + y: 召回最近用命令删除的文字 ⌃ + h: 删除光标之前的字符 ⌃ + d: 删除光标所指的字符 ⌃ + w: 删除光标之前的单词 ⌃ + k: 删除从光标到行尾的内容 ⌃ + t: 交换光标和之前的字符  ","permalink":"https://jianye0428.github.io/posts/tech/2022-05-09_zsh_installation/","summary":"zsh说明   zsh是一个Linux下强大的shell, 由于大多数Linux产品安装以及默认使用bash shell, 但是丝毫不影响极客们对zsh的热衷, 几乎每一款Linux产品都包含有zsh，通常可以用apt-get、urpmi或yum等包管理器进行安装.\n  zsh是bash的增强版，其实zsh和bash是两个不同的概念，zsh更加强大。\n  通常zsh配置起来非常麻烦，且相当的复杂，所以oh-my-zsh是为了简化zsh的配置而开发的，因此oh-my-zsh算是zsh的配置.\n  准备   查看当前系统用shell版本\n1  echo $SHELL     查看系统自带哪些shell\n1  cat /etc/shells     安装zsh  通过命令行安装zsh 1  sudo apt install zsh     zsh配置   将zsh设置为默认的shell\n1  chsh -s /bin/zsh     然后重启电脑\n1  reboot     安装oh-my-zsh及其个性化配置 安装oh-my-zsh  执行以下命令安装oh-my-zsh 1  sh -c \u0026#34;$(wget https://raw.","title":"Ubuntu 22.04 | zsh 以及 oh-my-zsh的安装和配置"},{"content":"PPO Architechture ","permalink":"https://jianye0428.github.io/posts/tech/2022-05-06_ppo/","summary":"PPO Architechture ","title":"PPO -- Proximal Policy Optimization"},{"content":"[DQN]paper link: https://arxiv.org/pdf/1312.5602v1.pdf\nDQN: Playing Atari with Deep Reinforcement Learning General Architecture Here is Network listed:\n play Atari games using RL and perform better than human CNN + Q Learning: CNN for frame-skiped images features extraction; and Q Learning for policy generation     Network Channel Kernel Size Stride Activation Output Size     Input NA NA NA NA 84x84x4   First Conv 16 8x8 4 Relu 20x20x6   Second Conv 32 4x4 2 Relu 9x9x32   Hidden NA NA NA Relu 256   Output NA NA NA None 4 to 18     在当时，普遍的做法是为每一个action学习一个函数，而不是一个网络结构直接输出所有q的value.\n Key 1: Input Info Process  图像处理部分\n  Grayscale, Downsampling and Cropping  RGB channels to gray scale channel (将RGB取均值为灰度图): 216 x 163 x 3 =\u0026gt;(grayscale) 216 x 163 x 1 =\u0026gt;(downsampling) 110 x 84 x 1 =\u0026gt;(cropping) 84 x 84 x 1     游戏部分\n  Key Frame and Action Repeat  select skipped frames (每个4帧选取关键帧)，假设智能体看不见中间过程; 而且agent在每k帧选择一个action，可以加速训练 作用:  加速游戏进行: 计算Q-Value是最耗时的步骤; 减少噪声: 过分紧密的frame重复信息过多，之前的action容易被否决; 缩短reward signal到具体aciton之间的时间间隔。     History as Input  continuous history key frames as input (连续四个关键帧作为输入) 作用:  可以帮助智能体获得更多有效信息进行训练     Reward Clipping  将多有的reward简化为+1, -1和0 缺点: 有可能对训练效果有影响 作用: 损失了部分信息，但是可以保证不同游戏的reward scale相同，可以用相同的参数进行训练(因为在论文中，作者在多个游戏上对DQN进行了验证)。    Key 2: Replay Buffer   原理:\n DQN中对神经网络的训练本质依然是SGD，SGD要求多次利用样本，并且样本独立，但相邻的transition都是高度相关的，所以要记住过去的transition一起抽样; Replay Buffer通过记忆一段时间内的trainsition，可以让训练数据分布更平稳; Replay Buffer通过忘记很久之前的trainsition，可以保证记住的分布大致模拟当前policy的分布，从而进行policy update; 可以多次重复采样，提升data efficiency.    Replay Buffer生效的一个重要条件: 存储transition数量合适\n 太多: 可能使reward signal太过稀疏，影响训练 太少: 可能会导致训练数据的分布迅速变化    Key 3: Semi-Gradient Method 在Eauation3中，\n$$y_i = r + \\gamma \\max_{a\u0026rsquo;}Q(s\u0026rsquo;, a\u0026rsquo;; \\theta_{t-1})$$\n不和之后的Q函数共享参数;\n但是在实际的训练过程中，采用\n$$ y_i = r + \\gamma \\max_{a\u0026rsquo;}Q(s\u0026rsquo;, a\u0026rsquo;; \\theta_{t})$$\n和之后的Q函数共享参数，但是实际上不参与导数计算，这种方法称为Semi-Gradient Method。\n 作用: 使训练更新更稳定。  ","permalink":"https://jianye0428.github.io/posts/tech/2022-05-05_dqn/","summary":"[DQN]paper link: https://arxiv.org/pdf/1312.5602v1.pdf\nDQN: Playing Atari with Deep Reinforcement Learning General Architecture Here is Network listed:\n play Atari games using RL and perform better than human CNN + Q Learning: CNN for frame-skiped images features extraction; and Q Learning for policy generation     Network Channel Kernel Size Stride Activation Output Size     Input NA NA NA NA 84x84x4   First Conv 16 8x8 4 Relu 20x20x6   Second Conv 32 4x4 2 Relu 9x9x32   Hidden NA NA NA Relu 256   Output NA NA NA None 4 to 18     在当时，普遍的做法是为每一个action学习一个函数，而不是一个网络结构直接输出所有q的value.","title":"DQN -- Deep Q Network"},{"content":"TensorRT Installation Custom Operator ScatterElements ","permalink":"https://jianye0428.github.io/posts/tech/2022-04-24_tensorrt_custom_operator/","summary":"TensorRT Installation Custom Operator ScatterElements ","title":"TensorRT custom operator development -- ScatterElements"}]