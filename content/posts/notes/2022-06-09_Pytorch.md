---
title: "PyTorch Notes"
slug: ""
date: 2022-06-09T19:14:27+08:00
summary: ""
author: ["Jian"]
cover:
    image: ""
    alt: ""
tags: ["PyTorch"]
katex: true
mermaid: false
draft: false
---


## Torch 基本函数
### 1. **```torch.einsum()```**
```torch.einsum(equation, *operands)->Tensor```:爱因斯坦求和
ref1: 算子部署: https://blog.csdn.net/HW140701/article/details/120654252
ref2: 例子: https://zhuanlan.zhihu.com/p/361209187

**三条基本规则:**
- **规则一:** equation 箭头左边，在不同输入之间<font color=red>重复出现的索引</font>表示，把输入张量沿着该维度做乘法操作，比如还是以上面矩阵乘法为例， "ik,kj->ij"，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作；
- **规则二:** 只出现在 equation 箭头左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面提到的求和索引；
- **规则三:** equation 箭头右边的索引顺序可以是任意的，比如上面的 "ik,kj->ij" 如果写成 "ik,kj->ji"，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成
  
**特殊规则:**
   - equation 可以不写包括箭头在内的右边部分，那么在这种情况下，输出张量的维度会根据默认规则推导。就是把输入中只出现一次的索引取出来，然后按字母表顺序排列，比如上面的矩阵乘法 "ik,kj->ij" 也可以简化为 "ik,kj"，根据默认规则，输出就是 "ij" 与原来一样；
   - equation 中支持 "..." 省略号，用于表示用户并不关心的索引。比如只对一个高维张量的最后两维做转置可以这么写：
     ```shell
     a = torch.randn(2,3,5,7,9)
     # i = 7, j = 9
     b = torch.einsum('...ij->...ji', [a])
     ```

### 2. **```torch.permute()/torch.transpose()```**
```torch.permute(dim0, dim1, dim2)```:用于调换不同维度的顺序
```torch.transpose(input, dim0, dim1)```:交换矩阵的两个维度


### 3. **```torch.rand()```**
```torch.rand(dim0, dim1)```:生成dim0 x dim1的tensor

### 4. **```torch.size()/torch.shape```**
```torch.size()```:返回tensor的size
```torch.shape```:返回tensor的size


### 5. **```torch.tensordot()```**
ref: tensordot()和einsum()的区别: https://blog.csdn.net/Eric_1993/article/details/105670381
```torch.tensordot(tensor1， tensor2， axes=([dim1,dim2],[dim0, dim1]))```: 将axes指定的子数组进行点乘, axes 指定具体的维度.

### 6. **```torch.transpose()```**
```torch.transpose(tensor, dim0, dim2) —> Tensor```:在dim0和dim1方向上转置

###7. **`torch.index_add_()`**

` Tensor.index_add_(dim, index, source, *, alpha=1) → Tensor`

demo:
```python 
>>> x = torch.ones(5, 3)
>>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)
>>> index = torch.tensor([0, 4, 2])
>>> x.index_add_(0, index, t)
tensor([[  2.,   3.,   4.],
        [  1.,   1.,   1.],
        [  8.,   9.,  10.],
        [  1.,   1.,   1.],
        [  5.,   6.,   7.]])
>>> x.index_add_(0, index, t, alpha=-1)
tensor([[  1.,   1.,   1.],
        [  1.,   1.,   1.],
        [  1.,   1.,   1.],
        [  1.,   1.,   1.],
        [  1.,   1.,   1.]])
```


## Torch NN Module

```python
import torch
from torch import nn
from torch import functional as F
```

### 1. **`nn.Conv1d()`**

  `torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)`

  **Shape:**
    - Input: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$
    - Output: &(N, C_{in}, L_{in})& or $(C_{in}, L_{in})$, where
    $$L_{out} = \frac{L_{in} + 2 \cdot \text{padding} - \text{dilation} \cdot (\text{kernel\_size} - 1) - 1}{stride}$$
  **demo：**
  ```python
  m = nn.Conv1d(16, 33, 3, stride=2)
  input = torch.randn(20, 16, 50) # B x C x H or N x C x L
  output = m(input)
  print(output.shape) # torch.Size([20, 33, 24])
  ```
### 2. **`nn.Conv2d()`**

### 3. **`nn.functional.interpolate()`**
  `torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False)`


### 4. **`nn.functional.ReLU()`**